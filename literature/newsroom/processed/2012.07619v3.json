{
    "paper_id": "2012",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2025-02-14T13:47:15.450050Z"
    },
    "title": "What Makes a Good and Useful Summary? Incorporating Users in Automatic Summarization Research",
    "authors": [
        {
            "first": "Maartje",
            "middle": [],
            "last": "Ter Hoeve",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Amsterdam",
                "location": {}
            },
            "email": "m.a.terhoeve@uva.nl"
        },
        {
            "first": "Julia",
            "middle": [],
            "last": "Kiseleva",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Amsterdam",
                "location": {}
            },
            "email": "julia.kiseleva@microsoft.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Automatic text summarization has enjoyed great progress over the years and is used in numerous applications, impacting the lives of many. Despite this development, there is little research that meaningfully investigates how the current research focus in automatic summarization aligns with users' needs. To bridge this gap, we propose a survey methodology that can be used to investigate the needs of users of automatically generated summaries. Importantly, these needs are dependent on the target group. Hence, we design our survey in such a way that it can be easily adjusted to investigate different user groups. In this work we focus on university students, who make extensive use of summaries during their studies. We find that the current research directions of the automatic summarization community do not fully align with students' needs. Motivated by our findings, we present ways to mitigate this mismatch in future research on automatic summarization: we propose research directions that impact the design, the development and the evaluation of automatically generated summaries.",
    "pdf_parse": {
        "paper_id": "2012",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Automatic text summarization has enjoyed great progress over the years and is used in numerous applications, impacting the lives of many. Despite this development, there is little research that meaningfully investigates how the current research focus in automatic summarization aligns with users' needs. To bridge this gap, we propose a survey methodology that can be used to investigate the needs of users of automatically generated summaries. Importantly, these needs are dependent on the target group. Hence, we design our survey in such a way that it can be easily adjusted to investigate different user groups. In this work we focus on university students, who make extensive use of summaries during their studies. We find that the current research directions of the automatic summarization community do not fully align with students' needs. Motivated by our findings, we present ways to mitigate this mismatch in future research on automatic summarization: we propose research directions that impact the design, the development and the evaluation of automatically generated summaries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The field of automatic text summarization has experienced great progress over the last years, especially since the rise of neural sequence to sequence models (e.g., Cheng and Lapata, 2016; See et al., 2017; Vaswani et al., 2017) . The introduction of self-supervised transformer language models like BERT (Devlin et al., 2019) has given the field an additional boost (e.g., Liu et al., 2018; Liu and Lapata, 2019; Lewis et al., 2020; Xu et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 165,
                        "end": 188,
                        "text": "Cheng and Lapata, 2016;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 189,
                        "end": 206,
                        "text": "See et al., 2017;",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 207,
                        "end": 228,
                        "text": "Vaswani et al., 2017)",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 305,
                        "end": 326,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 374,
                        "end": 391,
                        "text": "Liu et al., 2018;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 392,
                        "end": 413,
                        "text": "Liu and Lapata, 2019;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 414,
                        "end": 433,
                        "text": "Lewis et al., 2020;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 434,
                        "end": 450,
                        "text": "Xu et al., 2020)",
                        "ref_id": "BIBREF56"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The-often implicit-goal of automatic text summarization is to generate a condensed textual version of the input document(s), whilst preserving the main message. This is reflected in today's most common evaluation metrics for the task; they focus on aspects such as informativeness, fluency, succinctness and factuality (e.g., Lin, 2004; Nenkova ",
                "cite_spans": [
                    {
                        "start": 326,
                        "end": 336,
                        "text": "Lin, 2004;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 337,
                        "end": 344,
                        "text": "Nenkova",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This figure shows the classical approach for textual summarization. Unstructured textual input is transformed into shorter textual output.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input factors",
                "sec_num": null
            },
            {
                "text": "The usual approach for textual summarization uses unstructured textual input and output. and Passonneau, 2004; Paulus et al., 2018; Narayan et al., 2018b; Goodrich et al., 2019; Wang et al., 2020; Xie et al., 2021) . The needs of the users of the summaries are often not explicitly addressed, despite their importance in explicit definitions of the goal of automatic summarization (Sp\u00e4rck Jones, 1998; Mani, 2001a) . Mani defines this goal as: \"to take an information source, extract content from it, and present the most important content to the user in a condensed form and in a manner sensitive to the user's or application's needs.\"",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 110,
                        "text": "Passonneau, 2004;",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 111,
                        "end": 131,
                        "text": "Paulus et al., 2018;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 132,
                        "end": 154,
                        "text": "Narayan et al., 2018b;",
                        "ref_id": null
                    },
                    {
                        "start": 155,
                        "end": 177,
                        "text": "Goodrich et al., 2019;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 178,
                        "end": 196,
                        "text": "Wang et al., 2020;",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 197,
                        "end": 214,
                        "text": "Xie et al., 2021)",
                        "ref_id": "BIBREF55"
                    },
                    {
                        "start": 381,
                        "end": 401,
                        "text": "(Sp\u00e4rck Jones, 1998;",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 402,
                        "end": 414,
                        "text": "Mani, 2001a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input factors",
                "sec_num": null
            },
            {
                "text": "Different user groups have different needs. Investigating these needs explicitly is critical, given the impact of adequate information transfer (Bennett et al., 2012) . We propose a survey methodology to investigate these needs. In designing the survey, we take stock of past work by Sp\u00e4rck Jones (1998) who argues that in order to generate useful summaries, one should take the context of a summary into account-a statement that has been echoed by others (e.g., Mani, 2001a; Aries et al., 2019) . To do this in a structured manner, Sp\u00e4rck Jones introduces three context factor classes: input factors, purpose factors and output factors, which respectively describe the input material, the purpose of the summary, and what the summary should look like. We structure our survey and its implications around these factors. In Figure 1 we give an example of incorporating the context factors in the design of automatic summarization methods.",
                "cite_spans": [
                    {
                        "start": 144,
                        "end": 166,
                        "text": "(Bennett et al., 2012)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 281,
                        "end": 303,
                        "text": "by Sp\u00e4rck Jones (1998)",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 463,
                        "end": 475,
                        "text": "Mani, 2001a;",
                        "ref_id": null
                    },
                    {
                        "start": 476,
                        "end": 495,
                        "text": "Aries et al., 2019)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 830,
                        "end": 831,
                        "text": "1",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Input factors",
                "sec_num": null
            },
            {
                "text": "Our proposed survey can be flexibly adjusted to different user groups. Here we turn our focus to university students as a first stakeholder group. University students are a particularly relevant group to focus on first, as they benefit from using pre-made summaries in a range of study activities (Reder and Anderson, 1980) , but the desired characteristics of these pre-made summaries have not been extensively investigated. We use the word premade to differentiate such summaries from the ones that users write themselves. Automatically generated summaries fall in the pre-made category, and should thus have the characteristics that users wish for pre-made summaries.",
                "cite_spans": [
                    {
                        "start": 297,
                        "end": 323,
                        "text": "(Reder and Anderson, 1980)",
                        "ref_id": "BIBREF42"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input factors",
                "sec_num": null
            },
            {
                "text": "Motivated by our findings, we propose important future research directions that directly impact the design, development, and evaluation of automatically generated summaries. We contribute the following: C1 We design a survey that can be easily adapted and reused to investigate and understand the needs of the wide variety of users of automatically generated summaries; C2 We develop a thorough understanding of how automatic summarization can optimally benefit users in the educational domain, which leads us to unravel important and currently underexposed research directions for automatic summarization; C3 We propose a new, feasible and comprehensive evaluation methodology to explicitly evaluate the usefulness of a generated summary for its intended purpose.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input factors",
                "sec_num": null
            },
            {
                "text": "In Section 1 we introduced the context factors as proposed by Sp\u00e4rck Jones (1998) . Each context factor class can be divided into more fine-grained subclasses. To ensure the flow of the paper, we list an overview in Appendix A. Below, we explain and use the context factors and their fine-grained subclasses to structure the related work. As our findings have implications for the evaluation of automatic summarization, we also discuss evaluation methods. Lastly, we discuss the use-cases of automatic summaries in the educational domain.",
                "cite_spans": [
                    {
                        "start": 59,
                        "end": 81,
                        "text": "by Sp\u00e4rck Jones (1998)",
                        "ref_id": "BIBREF47"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Input factors. We start with the fine-grained input factor unit, which describes how many sources are to be summarized at once, and the factor scale, which describes the length of the input data. These factors are related to the difference between single and multi-document summarization (e.g., Chopra et al., 2016; Cheng and Lapata, 2016; Wang et al., 2016; Yasunaga et al., 2017; Nallapati et al., 2017; Narayan et al., 2018b; Liu and Lapata, 2019) . Scale plays an important role when material shorter than a single document is summarized, such as sentence summarization (e.g., Rush et al., 2015) . Regarding the genre of the input material, most current work focuses on the news domain or Wikipedia (e.g., Sandhaus, 2008; Hermann et al., 2015; Koupaee and Wang, 2018; Liu et al., 2018; Narayan et al., 2018a) . A smaller body of work addresses different input genres, such as scientific articles (e.g., Cohan et al., 2018) , forum data (e.g., V\u00f6lske et al., 2017) , opinions (e.g., Amplayo and Lapata, 2020) or dialogues (e.g., Liu et al., 2021) . These differences are also closely related to the input factor subject type, which describes the difficulty level of the input material. The factor medium refers to the input language. Most automatic summarization work is concerned with English as language input, although there are exceptions, such as Chinese (e.g., Hu et al., 2015) or multilingual input (Ladhak et al., 2020) . The last input factor is structure. Especially in recent neural approaches, explicit structure of the input text is often ignored. Exceptions include graph-based approaches, where implicit document structure is used to summarize a document (e.g., Tan et al., 2017; Yasunaga et al., 2017) , and summarization of tabular data (e.g., Zhang et al., 2020a) or screenplays (e.g., Papalampidi et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 295,
                        "end": 315,
                        "text": "Chopra et al., 2016;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 316,
                        "end": 339,
                        "text": "Cheng and Lapata, 2016;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 340,
                        "end": 358,
                        "text": "Wang et al., 2016;",
                        "ref_id": "BIBREF53"
                    },
                    {
                        "start": 359,
                        "end": 381,
                        "text": "Yasunaga et al., 2017;",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 382,
                        "end": 405,
                        "text": "Nallapati et al., 2017;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 406,
                        "end": 428,
                        "text": "Narayan et al., 2018b;",
                        "ref_id": null
                    },
                    {
                        "start": 429,
                        "end": 450,
                        "text": "Liu and Lapata, 2019)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 581,
                        "end": 599,
                        "text": "Rush et al., 2015)",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 710,
                        "end": 725,
                        "text": "Sandhaus, 2008;",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 726,
                        "end": 747,
                        "text": "Hermann et al., 2015;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 748,
                        "end": 771,
                        "text": "Koupaee and Wang, 2018;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 772,
                        "end": 789,
                        "text": "Liu et al., 2018;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 790,
                        "end": 812,
                        "text": "Narayan et al., 2018a)",
                        "ref_id": null
                    },
                    {
                        "start": 907,
                        "end": 926,
                        "text": "Cohan et al., 2018)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 947,
                        "end": 967,
                        "text": "V\u00f6lske et al., 2017)",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 1032,
                        "end": 1049,
                        "text": "Liu et al., 2021)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 1370,
                        "end": 1386,
                        "text": "Hu et al., 2015)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 1409,
                        "end": 1430,
                        "text": "(Ladhak et al., 2020)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 1680,
                        "end": 1697,
                        "text": "Tan et al., 2017;",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 1698,
                        "end": 1720,
                        "text": "Yasunaga et al., 2017)",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 1764,
                        "end": 1784,
                        "text": "Zhang et al., 2020a)",
                        "ref_id": null
                    },
                    {
                        "start": 1807,
                        "end": 1832,
                        "text": "Papalampidi et al., 2020)",
                        "ref_id": "BIBREF40"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Automatic text summarization",
                "sec_num": "2.1"
            },
            {
                "text": "Purpose factors. Although identified as the most important context factor class by Sp\u00e4rck Jones (1998)-and followed by, for example, Mani (2001a)-purpose factors do not receive a substantial amount of attention. There are some exceptions, e.g., query-based summarization (e.g., Nema et al., 2017; Litvak and Vanetik, 2017) , question-driven summarization (e.g., Deng et al., 2020) , personalized summarization (e.g., M\u00f3ro and Bielikov\u00e1, 2012) and interactive summarization (e.g., Hirsch et al., 2021) . They take the situation and the audience into account. The use-cases of the generated summaries are also clearer in these approaches.",
                "cite_spans": [
                    {
                        "start": 278,
                        "end": 296,
                        "text": "Nema et al., 2017;",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 297,
                        "end": 322,
                        "text": "Litvak and Vanetik, 2017)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 362,
                        "end": 380,
                        "text": "Deng et al., 2020)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 417,
                        "end": 442,
                        "text": "M\u00f3ro and Bielikov\u00e1, 2012)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 480,
                        "end": 500,
                        "text": "Hirsch et al., 2021)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Automatic text summarization",
                "sec_num": "2.1"
            },
            {
                "text": "Output factors. We start with the output factors style and material. The latter is concerned with the degree of coverage of the summary. Most generated summaries have an informative style and cover most of the input material. 2020) make knowledge graphs. The difference between abstractive and extractive summarization is likely the best known distinction in output type (e.g., Nallapati et al., 2017; See et al., 2017; Narayan et al., 2018b; Gehrmann et al., 2018; Liu and Lapata, 2019) , although it is not entirely clear which output factor best describes the difference.",
                "cite_spans": [
                    {
                        "start": 378,
                        "end": 401,
                        "text": "Nallapati et al., 2017;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 402,
                        "end": 419,
                        "text": "See et al., 2017;",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 420,
                        "end": 442,
                        "text": "Narayan et al., 2018b;",
                        "ref_id": null
                    },
                    {
                        "start": 443,
                        "end": 465,
                        "text": "Gehrmann et al., 2018;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 466,
                        "end": 487,
                        "text": "Liu and Lapata, 2019)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Automatic text summarization",
                "sec_num": "2.1"
            },
            {
                "text": "In Section 5 we use the context factors to identify future research directions, based on the difference between our findings and the related work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Automatic text summarization",
                "sec_num": "2.1"
            },
            {
                "text": "Evaluation methods for automatic summarization can be grouped in intrinsic vs. extrinsic methods (Mani, 2001b) . Intrinsic methods evaluate the model itself, e.g., on informativeness or fluency (Paulus et al., 2018; Liu and Lapata, 2019) . Extrinsic methods target how a summary performs when used for a task (Dorr et al., 2005; Wang et al., 2020) . Extrinsic methods are resource intensive, explaining the popularity of intrinsic methods.",
                "cite_spans": [
                    {
                        "start": 97,
                        "end": 110,
                        "text": "(Mani, 2001b)",
                        "ref_id": null
                    },
                    {
                        "start": 194,
                        "end": 215,
                        "text": "(Paulus et al., 2018;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 216,
                        "end": 237,
                        "text": "Liu and Lapata, 2019)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 309,
                        "end": 328,
                        "text": "(Dorr et al., 2005;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 329,
                        "end": 347,
                        "text": "Wang et al., 2020)",
                        "ref_id": "BIBREF52"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "2.2"
            },
            {
                "text": "Evaluation methods can also be grouped in automatic vs. human evaluation methods. Different automatic metrics have been proposed, such as Rouge (Lin, 2004) and BERTScore (Zhang et al., 2020b) which respectively evaluate lexical and semantic similarity. Other methods use an au-tomatic question-answering evaluation methodology (Wang et al., 2020; Durmus et al., 2020) . Most human evaluation approaches evaluate intrinsic factors such as informativeness, readability and conciseness (DUC, 2003; Nallapati et al., 2017; Paulus et al., 2018; Liu and Lapata, 2019) -factors that are difficult to evaluate automatically. There are also some extrinsic human evaluation methods, where judges are asked to perform a certain task based on the summary (e.g., Narayan et al., 2018b) . So far, usefulnessfoot_0 has not been evaluated in a feasible and comprehensive manner, whereas it is an important metric to evaluate whether summaries fulfil users' needs. Therefore, we bridge the gap by introducing a feasible and comprehensive evaluation methodology to evaluate usefulness.",
                "cite_spans": [
                    {
                        "start": 144,
                        "end": 155,
                        "text": "(Lin, 2004)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 170,
                        "end": 191,
                        "text": "(Zhang et al., 2020b)",
                        "ref_id": null
                    },
                    {
                        "start": 327,
                        "end": 346,
                        "text": "(Wang et al., 2020;",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 347,
                        "end": 367,
                        "text": "Durmus et al., 2020)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 483,
                        "end": 494,
                        "text": "(DUC, 2003;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 495,
                        "end": 518,
                        "text": "Nallapati et al., 2017;",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 519,
                        "end": 539,
                        "text": "Paulus et al., 2018;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 540,
                        "end": 561,
                        "text": "Liu and Lapata, 2019)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 750,
                        "end": 772,
                        "text": "Narayan et al., 2018b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "2.2"
            },
            {
                "text": "Summaries play a prominent role in education. Reder and Anderson (1980) find that students who use a pre-made summary score better on a range of study activities than students who do not use such a summary. As the quality of automatically generated summaries increases (e.g., Lewis et al., 2020; Xu et al., 2020) , so does the potential to use them in the educational domain, especially given the increasing importance of digital tools and devices for education (Luckin et al., 2012; Hashim, 2018) . With these developments in mind, it is critical that educators are aware of the pedagogical implications; they need to understand how to best make use of all new possibilities (Hashim, 2018; Amhag et al., 2019) . The outcomes of our survey result in concrete suggestions for developing methods for automatic summarization in the educational domain, whilst taking students' needs into account.",
                "cite_spans": [
                    {
                        "start": 46,
                        "end": 71,
                        "text": "Reder and Anderson (1980)",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 276,
                        "end": 295,
                        "text": "Lewis et al., 2020;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 296,
                        "end": 312,
                        "text": "Xu et al., 2020)",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 462,
                        "end": 483,
                        "text": "(Luckin et al., 2012;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 484,
                        "end": 497,
                        "text": "Hashim, 2018)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 676,
                        "end": 690,
                        "text": "(Hashim, 2018;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 691,
                        "end": 710,
                        "text": "Amhag et al., 2019)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Automatic summarization for education",
                "sec_num": "2.3"
            },
            {
                "text": "Here we detail our survey procedure. For concreteness, we present the details with our intended target group in mind. The context factors form the backbone of our survey and the setup can be easily adjusted to investigate the needs of different target groups. For example, we ask participants about a pre-made summary for a recent study activity, but it is straightforward to adapt this to a different use-case that is more suitable for other user groups. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Survey Procedure and Participants",
                "sec_num": "3"
            },
            {
                "text": "We recruited participants among students at universities across the Netherlands by contacting ongoing courses and student associations, and by advertisements on internal student websites. As incentive, we offered a ten euro shopping voucher to ten randomly selected participants. A total of 118 participants started the survey and 82 completed the full survey, resulting in a 69.5% completion rate. We only include participants who completed the study in our analysis. Participants spent 10 minutes on average on the survey. In the final part of our survey we ask participants to indicate their current level of education and main field of study. The details are given in Figure 2 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 679,
                        "end": 680,
                        "text": "2",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Participants",
                "sec_num": "3.1"
            },
            {
                "text": "Figure 3 shows a brief overview of our survey procedure. A detailed account is given in Appendix B. We arrived at the final survey version after a number of pilot runs where we ensured participants understood their task and all questions. We ran the survey with SurveyMonkey (surveymonkey.com). A verbatim copy is included in Appendix C and released under CC BY license.foot_1 ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "3",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Survey procedure",
                "sec_num": "3.2"
            },
            {
                "text": "Introduction. The survey starts with an introduction where we explain what to expect, how we process the data and that participation is voluntary. After participants agree with this, an explanation of the term pre-made summary follows. As we do not want to bias participants by stating that the summary was automatically generated, we explain that the summary can be made by anyone, e.g., a teacher, a good performing fellow student, the authors of the original material, or a computer. Recall that an automatically generated summary is a pre-made summary. Hence, our survey identifies the characteristics an automatically generated summary should have. We also give examples of types of pre-made summaries; based on the pilot experiments we noticed that people missed this information. We explicitly state that these are just examples and that participants can come up with any example of a helpful pre-made summary. Context factors. In the main part of our survey we focus on the context factors. First, we ask participants whether they have made use of a pre-made summary in one of their recent study activities. If so, we ask them to choose the study activity where a summary was most useful. We call this group the Remembered group, as they describe an existing summary from memory. If people indicate that they have not used a pre-made summary in one of their recent study activities, we ask them whether they can imagine a situation where a pre-made summary would have been helpful. If not, we ask them why not and lead them to the final background questions and closing page. If yes, we ask them to keep this imaginary situation in mind for the rest of the survey. We call this group the Imagined group.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Survey procedure",
                "sec_num": "3.2"
            },
            {
                "text": "Now we ask the Remembered and Imagined groups about the input, purpose and output factors of the summary they have in mind. We ask questions for each of the context factor subclasses that we discussed in Section 2. At this point, the two groups are in different branches of the survey. The difference is mainly linguistically motivated: in the Imagined group we use verbs of probability instead of asking to describe an existing situation. Some questions can only be asked in the Remembered group, e.g., how helpful the summary was.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Survey procedure",
                "sec_num": "3.2"
            },
            {
                "text": "In the first context factor question we ask what the study material consisted of. We give a number of options, as well as an 'other' checkbox. To avoid position bias, all answer options for multiple choice and multiple response questions in the survey are randomized, with the 'other' checkbox always as the last option. If participants do not choose the 'mainly text' option, we tell them that we focus on textual input in the current studyfoot_2 and ask whether they can think of a situation where the input did consist of text. If not, we lead them to the background questions and closing page. If yes, they proceed to the questions that give us a full overview of the input, purpose and output factors of the situation participants have in mind. Finally, we ask the Remembered group to suggest how their described summary could be turned into their ideal summary. We then ask both groups for any final remarks about the summary or input material. Trustworthiness and future features questions. So far we have included the possibility that the summary was machine-generated, but also explicitly included other options so as not to bias participants. At this point we acknowledge that machinegenerated summaries could give rise to additional challenges and opportunities. Hence, we include some exploratory questions to get an understanding of the trust users would have in machine-generated summaries and to get ideas for the interpretation of the context factors in exploratory settings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Survey procedure",
                "sec_num": "3.2"
            },
            {
                "text": "For the first questions we tell participants to imagine that the summary was made by a computer, but contained all needs identified in the first part of the survey. We then ask them about trust in computer-and human-generated summaries. Next, we ask them to imagine that they could interact with the computer program that made the summary in the form of a digital assistant. We tell them not to feel restricted by the capabilities of today's digital assistants. The verbatim text is given in Appendix C. We ask participants to select the three most and the three least useful features for the digital assistant, similar to ter Hoeve et al. (2020).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Survey procedure",
                "sec_num": "3.2"
            },
            {
                "text": "For each question we examine the outcomes of all respondents together and of different subgroups (Table 1 ). For space and clarity reasons, we present the results of all respondents together, unless interesting differences between groups are found. We use the question formulations as used for the Remembered group and abbreviate answer options. Answers to multiple choice and multiple response questions are presented in an aggregated manner and we ensure that none of the open answers can be used to identify individual participants.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 104,
                        "end": 105,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4"
            },
            {
                "text": "Of our participants, 78.0% were led to the Remembered branch and of the remaining 22.0%, 78.2% were led to the Imagined branch. We asked the few remaining participants why they could not think of a case where a pre-made summary could be useful for them. People answered that they would not 1 All respondents together 2 Remembered branch vs Imagined branch 3 Different study fields 4 Different study levels 5 Different levels of how helpful the summary was according to participants, rated on a 5-point Likert scale (note that only the remembered group answered this question)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Identifying branches",
                "sec_num": "4.1"
            },
            {
                "text": "Table 1 : Levels of investigation. We did not find significant differences for each, but add all for completeness.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Identifying branches",
                "sec_num": "4.1"
            },
            {
                "text": "trust such a summary and that making a summary themselves helped with their study activities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Identifying branches",
                "sec_num": "4.1"
            },
            {
                "text": "Figure 4 shows the input factor results. We highlight some here. Textual input is significantly more popular than other input types (Figure 4a ),foot_3 stressing the relevance of automatic text summarization. People described a diverse input for scale and unit (Figure 4b ), much more than the classical focus of automatic summarization suggests. Most input had a considerable amount of structure (Figure 4e ). Structure is often discarded in automatic summarization, although it can be very informative.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "4",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 140,
                        "end": 142,
                        "text": "4a",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 269,
                        "end": 271,
                        "text": "4b",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 405,
                        "end": 407,
                        "text": "4e",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Input factors",
                "sec_num": "4.2"
            },
            {
                "text": "Figure 5 shows the purpose factor results. Participants indicated that the summary was helpful or very helpful (Figure 5f ), which allows us to draw valid conclusions from the survey. 5 We now highlight some results from the other questions in this category. For the intended audience of the summaries, students selected level (4) and (5) (\"a lot (4) or full (5) domain knowledge is expected from the users of the summary\") significantly more often than the other options (Figure 5d ). Although perhaps unsurprising given our target group, it is an important outcome as this requires a different level of detail than, for example, a brief overview of a news article. People used the summaries for many different use-cases (Figure 5e ), whereas current research on automatic summarization mainly focuses on giving an overview of the input. We show the results for the Remembered vs. Imagined splits, (a) Medium: The study material consisted of (MC) as the Imagined group chose refresh memory and overview more often than the Remembered group (Fisher's exact test, p < 0.05). Although not significant after a Bonferroni correction, this can still be insightful for future research directions. Lastly, participants in the Imagined group ticked more boxes than participants in the Remembered group: 3.33 vs. 2.57 per participant on average, stressing the importance of considering many different use-cases for automatically generated summaries.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "5",
                        "ref_id": "FIGREF7"
                    },
                    {
                        "start": 119,
                        "end": 121,
                        "text": "5f",
                        "ref_id": "FIGREF7"
                    },
                    {
                        "start": 480,
                        "end": 482,
                        "text": "5d",
                        "ref_id": "FIGREF7"
                    },
                    {
                        "start": 730,
                        "end": 732,
                        "text": "5e",
                        "ref_id": "FIGREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Purpose factors",
                "sec_num": "4.3"
            },
            {
                "text": "Figure 6 shows the results for the output factor questions. Textual summaries were significantly more popular than other summary types (Figure 6a ), which again stresses the importance of automatic text summarization. Most participants indicated that the summary covered (or should cover) most of the input material (Figure 6c ). For the output factor style we find an interesting difference between the Remembered and Imagined group (Figure 6d ). Whereas the Remembered group described significantly more often an informative summary, the Imagined group opted significantly more often for a critical or aggregative summary. Most research on automatic summarization focusses on informative summaries only. For the output factor structure (Figure 6b ), people described a substantially richer format of the pre-made summaries than adopted in most research on automatic summarization. Instead of simply a running text, the vast majority of people indicated that the summary contained (or should contain) structural elements such as special formatting, diagrams, headings, etc. Moreover, the Imagined group ticked more answer boxes on average than the Remembered group: 4.17 vs. 3.56 per participant, indicating a desire for structure in the generated summaries, which is supported by the open answer questions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "6",
                        "ref_id": "FIGREF8"
                    },
                    {
                        "start": 143,
                        "end": 145,
                        "text": "6a",
                        "ref_id": "FIGREF8"
                    },
                    {
                        "start": 324,
                        "end": 326,
                        "text": "6c",
                        "ref_id": "FIGREF8"
                    },
                    {
                        "start": 442,
                        "end": 444,
                        "text": "6d",
                        "ref_id": "FIGREF8"
                    },
                    {
                        "start": 746,
                        "end": 748,
                        "text": "6b",
                        "ref_id": "FIGREF8"
                    }
                ],
                "eq_spans": [],
                "section": "Output factors",
                "sec_num": "4.4"
            },
            {
                "text": "Open answer questions. We asked participants in the Remembered group how the summary could be transformed into their ideal summary and 86.9% of these participants made suggestions. Many of those include adding additional structural elements to the summary, like figures, tables or structure in the summary text itself. For example, one of the participants wrote: \"An ideal summary is good enough to fully replace the original (often longer) texts contained in articles that need to be read for exams. The main purpose behind this is speed of learning from my experience. More tables, graphs and visual representations of the study material and (b) Situation ( 2 key concepts / links would improve the summary, as I would faster comprehend the study material.\" Another participant wrote: \"-colors and a key for color-coding -different sections, such as definitions on the left maybe and then the rest of the page reflects the structure of the course material with notes on the readings that have many headings and subheadings.\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Output factors",
                "sec_num": "4.4"
            },
            {
                "text": "Another theme is the desire to have more examples in the summary. One participant wrote: \"More examples i think. For me personally i need examples to understand the material. Now i needed to imagine them myself\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Output factors",
                "sec_num": "4.4"
            },
            {
                "text": "Some participants wrote that they would like a more personalized summary, for example: \"I'd highlight some things I find difficult. So I'd personalise the summary more.\" Another participant wrote: \"Make it more personalized may be. These notes were by another student. I might have focussed more on some parts and less on others.\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Output factors",
                "sec_num": "4.4"
            },
            {
                "text": "Of all participants, 48.0% indicated that it would not make a difference to them whether a summary is machine-or human-generated, as long as the quality is as good as a human-generated one. This last point is reflected in which types of summaries participants would trust more. People opted significantly more often for a human-generated one. For the future feature questions, adding more details to the summary and answering questions based on the content of the summary were very popular. We give a full account in Appendix D.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Trustworthiness and future features",
                "sec_num": "4.5"
            },
            {
                "text": "Our findings have important implications for the design and development of future automatic summarization methods. We present these in Table 2 , per context factor. Summarizing, the research developments as summarized in Section 2 are encouraging, yet given that automatic summarization methods increasingly mediate people's lives, we argue that more attention should be devoted to its stakeholders, i.e., to the purpose factors. Here we have shown that students, an important stakeholder group, have different expectations of pre-made (a) Format ( 1 summaries than what most automatic summarization methods offer. These differences include the type of input material that is to be summarized, but also how these summaries are presented. Presum-",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 141,
                        "end": 142,
                        "text": "2",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Future research directions",
                "sec_num": "5.1"
            },
            {
                "text": "Stronger focus on developing methods that can:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input Factors",
                "sec_num": null
            },
            {
                "text": "\u2022 handle a wide variety and a mixture of different types of input documents at once; \u2022 understand the relationships between different input documents; \u2022 use the structure of the input document(s).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Input Factors",
                "sec_num": null
            },
            {
                "text": "\u2022 Explicitly define a standpoint on the purpose factors in each research project; ably, this also holds for other stakeholder groups and thus we hope to see our survey used for different target groups in the future. Datasets. To support these future directions we need to expand efforts on using and collecting a wide variety of datasets. Most recent data collection efforts are facilitating different input factorsthe purpose and output factors need more emphasis. Our findings also impact the evaluation of summarization methods. We discuss this next.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Purpose Factors",
                "sec_num": null
            },
            {
                "text": "Following Sp\u00e4rck Jones (1998) and Mani (2001a) , we argue that a good choice of context factors is crucial in producing useful summaries for users.",
                "cite_spans": [
                    {
                        "start": 34,
                        "end": 46,
                        "text": "Mani (2001a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Usefulness as evaluation methodology",
                "sec_num": "5.2"
            },
            {
                "text": "It is important to explicitly evaluate this. The few existing methods to evaluate usefulness are very resource demanding (e.g., Riccardi et al., 2015) or not comprehensive enough (e.g., DUC, 2003; Dorr et al., 2005) . Thus, we propose a feasible and comprehensive method to evaluate usefulness.",
                "cite_spans": [
                    {
                        "start": 128,
                        "end": 150,
                        "text": "Riccardi et al., 2015)",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 186,
                        "end": 196,
                        "text": "DUC, 2003;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 197,
                        "end": 215,
                        "text": "Dorr et al., 2005)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Usefulness as evaluation methodology",
                "sec_num": "5.2"
            },
            {
                "text": "For the evaluation methodology, we again use the context factors. Before the design and development of the summarization method the intended purpose factors need to be defined. Especially the fine-grained factor use is important here. Next, the output factors need to be evaluated on the use factors. For this, we take inspiration from research on simulated work tasks (Borlund, 2003) . Evaluators should be given a specific task to imagine, e.g., writing a news article, or studying for an exam. This task should be relatable to the evaluators, so that reliable answers can be obtained (Borlund, 2016) . With this task in mind, evaluators should be asked to judge two summaries in a pairwise manner on their usefulness, in the following format: The [output factor] of which of these two summaries is most useful to you to [use factor]? For example: The style of which of these two summaries is most useful to you to substitute a chapter that you need to learn for your exam preparation? It is critical to ensure that judges understand the meaning of each of the evaluation criteria -style and substitute in the example. We provide example questions for each of the use and output factors in Appendix E.",
                "cite_spans": [
                    {
                        "start": 369,
                        "end": 384,
                        "text": "(Borlund, 2003)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 587,
                        "end": 602,
                        "text": "(Borlund, 2016)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Usefulness as evaluation methodology",
                "sec_num": "5.2"
            },
            {
                "text": "In this paper we focused on users of automatically generated summaries and argued for a stronger emphasis on their needs in the design, development and evaluation of automatic summarization methods. We led by example and proposed a survey methodology to identify these needs. Our survey is deeply grounded in past work by Sp\u00e4rck Jones (1998) on context factors for automatic summarization and can be re-used to investigate a wide variety of users. In this work we use our survey to investigate the needs of university students, an important target group of automatically generated summaries. We found that the needs identified by our participants are not fully supported by current automatic summarization methods and we proposed future research directions to accommodate these needs. Finally, we proposed an evaluation methodology to evaluate the usefulness of automatically generated summaries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "With this work we hope to take a step in the right direction to make research into automatic summarization more inclusive, by explicitly taking the needs of users of these summaries into account. As stressed throughout the paper, these needs are different per user group and therefore it is critical that a wide variety of user groups will be investigated. There might also be within group differences. For example, in this work we have focussed on students from universities in one country, but students attending universities in other geographical locations and with different cultures might express different needs. It is important to take these considerations into account, to limit the risk of overfitting on a particular user group and potentially harming other user groups.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ethical Impact",
                "sec_num": "7"
            },
            {
                "text": "Purpose Factors Output Factors",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Overview context factors Input Factors",
                "sec_num": null
            },
            {
                "text": "Structure: How is the input text structured? E.g., subheadings, rhetorical patterns, etc.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Form Situation Material",
                "sec_num": null
            },
            {
                "text": "Tied: It is known who will use the summary, for what purpose and when.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Form Situation Material",
                "sec_num": null
            },
            {
                "text": "Covering: The summary covers all of the important information in the source text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Form Situation Material",
                "sec_num": null
            },
            {
                "text": "Scale: How large is the input data that we are summarizing? E.g., a book, a chapter, a single article, etc.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Form Situation Material",
                "sec_num": null
            },
            {
                "text": "Floating: It is not (exactly) known who will use the summary, for what purpose or when.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Form Situation Material",
                "sec_num": null
            },
            {
                "text": "Partial: The summary (intentionally) covers only parts of the important information in the source text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Form Situation Material",
                "sec_num": null
            },
            {
                "text": "Medium: What is the input language type? E.g., full text, telegraphese style, etc. This also refers to which natural language is used.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Form Situation Material",
                "sec_num": null
            },
            {
                "text": "Genre: What type of literacy does the input text have? E.g., description, narrative, etc.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Audience Format",
                "sec_num": null
            },
            {
                "text": "Targetted: A lot of domain knowledge is expected from the readers of the summary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Audience Format",
                "sec_num": null
            },
            {
                "text": "Running: The summary is formatted as an abstract like text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Audience Format",
                "sec_num": null
            },
            {
                "text": "Untargetted: No domain knowledge is expected from the readers of the summary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Subject Type",
                "sec_num": null
            },
            {
                "text": "Headed: The summary is structured following a certain standardised format with headings and other explicit structure.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Subject Type",
                "sec_num": null
            },
            {
                "text": "Ordinary: Everyone could understand this input type.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Subject Type",
                "sec_num": null
            },
            {
                "text": "Specialized: You need to speak the jargon to understand this input type.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Use Style",
                "sec_num": null
            },
            {
                "text": "Retrieving: Use the summary to retrieve source text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Use Style",
                "sec_num": null
            },
            {
                "text": "Informative: The summary conveys the raw information that is in the source text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Use Style",
                "sec_num": null
            },
            {
                "text": "The input type text is only understandable for people familiar with a certain area, for example because it contains local names.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Restricted:",
                "sec_num": null
            },
            {
                "text": "Previewing: Use the summary to preview a text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Restricted:",
                "sec_num": null
            },
            {
                "text": "Indicative: The summary just states the topic of the source text, nothing more.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Restricted:",
                "sec_num": null
            },
            {
                "text": "Substitutes: Use the summary to substitute the source text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unit",
                "sec_num": null
            },
            {
                "text": "Critical: The summary gives a critical review of the merits of the source text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unit",
                "sec_num": null
            },
            {
                "text": "Single: Only one input source is given.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unit",
                "sec_num": null
            },
            {
                "text": "Refreshing: Use the summary to refresh ones memory of the source text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unit",
                "sec_num": null
            },
            {
                "text": "Aggregative: Different source texts are put in relation to one another to give an overview of a certain topic.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unit",
                "sec_num": null
            },
            {
                "text": "Multi: Multiple input sources are given.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unit",
                "sec_num": null
            },
            {
                "text": "Prompts: Use the summary as action prompt to read the source text. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unit",
                "sec_num": null
            },
            {
                "text": "Table 4: A complete overview of the survey. This table includes the explanation that participants received, as well as all the questions and the answer options. If a question was the start of a branch, the direction of the branch has been written behind the answer options in italic. (This was never shown to the participants.) Note that the survey was performed in SurveyMonkey. 6 The survey had a lay-out as provided by SurveyMonkey, i.e., it consisted of different pages and colors were used to highlight certain important parts in texts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C Verbatim survey overview",
                "sec_num": null
            },
            {
                "text": "Question Nr. Question and Answer Options",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C Verbatim survey overview",
                "sec_num": null
            },
            {
                "text": "Thank you for taking the time to fill out this survey! Before you start, please take the time to read these instructions carefully. If you still have any questions after reading the instructions, please send them to m.a.terhoeve@uva.nl.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction and Instructions",
                "sec_num": null
            },
            {
                "text": "We will give away 10 bol.com vouchers of 10 euros each among the participants. If you would like to take part in the raffle, you can leave your email address at the end of this survey.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction and Instructions",
                "sec_num": null
            },
            {
                "text": "The goal of this survey is to get insight in how summaries help or can help you when studying.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Goal of the study",
                "sec_num": null
            },
            {
                "text": "In what follows you will get questions that aim to develop an understanding for:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What the survey will look like",
                "sec_num": null
            },
            {
                "text": "\u2022 For which types of study material it is useful to have summaries",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What the survey will look like",
                "sec_num": null
            },
            {
                "text": "\u2022 How these summaries can help you with your task",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What the survey will look like",
                "sec_num": null
            },
            {
                "text": "\u2022 What these summaries should look like",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What the survey will look like",
                "sec_num": null
            },
            {
                "text": "We expect this survey to take approximately 10 minutes of your time.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What the survey will look like",
                "sec_num": null
            },
            {
                "text": "Use the next button to go to the next page once you have filled out all the questions on the page. Use the prev button to go back one page.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What the survey will look like",
                "sec_num": null
            },
            {
                "text": "We value your privacy and will process your answers anonymously. The answers of all participants in this survey will be used to gain insight in how pre-made summaries can be helpful for different types of studying activities. The answers will be presented in a research paper about this topic. This will be done either in an aggregated manner, or by citing verbatim examples of the answers. Again, this will all be done anonymously.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "About your privacy",
                "sec_num": null
            },
            {
                "text": "I agree that I have read and understood the instructions. I also understand that my participation in this survey is voluntarily.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "About your privacy",
                "sec_num": null
            },
            {
                "text": "Question Nr. Question and Answer Options",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I agree",
                "sec_num": null
            },
            {
                "text": "Important! Some background knowledge you need to know",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q2",
                "sec_num": null
            },
            {
                "text": "Throughout this survey we make use of the term pre-made summary. It is very important that you understand what this means. On this page we explain this term, so please make sure to read this carefully.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q2",
                "sec_num": null
            },
            {
                "text": "One type of summary is one that you make yourself. Another type of summary is one that has been made for you. In this survey, we focus on this latter type and we call them pre-made summaries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition pre-made summary",
                "sec_num": null
            },
            {
                "text": "Who makes these pre-made summaries?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition pre-made summary",
                "sec_num": null
            },
            {
                "text": "These pre-made summaries can be made by a person, for example your teacher, your friend, a fellow student or someone at some official organisation, etc. The pre-made summaries can also be made by a computer.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Definition pre-made summary",
                "sec_num": null
            },
            {
                "text": "There D Full results trustworthiness and future feature questions",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What kinds of summaries are we talking about?",
                "sec_num": null
            },
            {
                "text": "In this section we report the results for the exploratory questions that we asked about the trustworthiness of a summary generated by a machine versus a human, as well as the results for the questions about features for summarization with a digital voice assistant. We find that participants are divided on the question whether it would make a difference to them whether the summary was generated by a machine or a computer. If we look at all participants together, we find that 48.0.% of the participants answered that it would make a difference, whereas 52.0% answered that it would not. However, if we split the participants based on study background, an interesting difference emerges (Figure 8a ). Participants with a background in STEM indicated significantly more often that it would not make a difference to them, whereas the other groups of students indicated the opposite. Almost all participants who answered that it would make a difference said that they would not trust a computer on being able to find the relevant information, i.e., all seemed to favor the human generated summary. Only one participant advocated for the computer-generated summary as a \"computer is more objective.\" Almost all participants who said it would not matter to them did add the condition that the quality of the generated summary should be as good as if a human had generated it. One person wrote: \"If the summary captures all previously discussed elements it is effectively good for the same purpose. So then it does not matter who generated it.\" This comment exactly captures the motivation of the setup of our survey.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 697,
                        "end": 699,
                        "text": "8a",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "What kinds of summaries are we talking about?",
                "sec_num": null
            },
            {
                "text": "This caution regarding automatically generated summaries is confirmed by the question in which we asked which type of summary participants would trust more -a human-generated one or a machine-generated one. People chose the humangenerated summary significantly more often (Figure 8b) . This also holds for the participants with a STEM background, which aligns with the responses to the open questions we reported earlier -apparently participants do not fully trust that the condition they raised earlier would be satisfied, namely that only if the machine was just as good as the human, it would not matter for them whether the summary was generated by a machine or a human.",
                "cite_spans": [
                    {
                        "start": 272,
                        "end": 283,
                        "text": "(Figure 8b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What kinds of summaries are we talking about?",
                "sec_num": null
            },
            {
                "text": "The results for the most and least useful features for a digital assistant in a summarization scenario are given in Figure 8c and 8d. Adding more details to the summary and answering questions based on the content of the summary are very popular features, whereas summarizing parts of the input material with less detail is not.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 123,
                        "end": 125,
                        "text": "8c",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "What kinds of summaries are we talking about?",
                "sec_num": null
            },
            {
                "text": "Lastly, we asked participants whether they could think of any other features that they would like their digital assistant to have in the outlined scenario. A number of participants answered that they would like the digital assistant to generate questions based on the summary, so that they could test their own understanding. For example, one participant said: \"Make questions for me (to test me)\" and another participant had a related comment: \"Maybe the the digital assistant could find old exam questions to link to parts of the summary where the question is related to, so that there is a function to test if you've understood the summary.\" Another line of answers pointed towards giving explicit relations between the input material and summary, for example: \"Show links between subject materials and what their relation is\" and another person wrote: \"Dynamic linking from summary to original source is a great added value of generating a summary\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "What kinds of summaries are we talking about?",
                "sec_num": null
            },
            {
                "text": "Here we give additional examples for the evaluation questions that can be used for our proposed evaluation methodology. The phrase \"a document that is important for your task\" should be substituted to match the task at hand. For example, in the case of exam preparations, this could be replaced with \"a chapter that you need to learn for your exam preparation\". Only the questions with the intended purpose factors should be used in the evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Examples evaluation questions",
                "sec_num": null
            },
            {
                "text": "Purpose factor Use & Output factor Style: \u2022 The style of which of these two summaries is most useful to you to retrieve a document that is important for your task? \u2022 The style of which of these two summaries is most useful to you to preview a document that is important for your task? \u2022 The style of which of these two summaries is most useful to you to substitute a document that is important for your task? \u2022 The style of which of these two summaries is most useful to you to refresh your memory about a document that is important for your task?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Examples evaluation questions",
                "sec_num": null
            },
            {
                "text": "We follow the definition of the English Oxford Learner's Dictionary (www.oxfordlearnersdictionaries. com/definition/english/) for usefulness: \"the fact of being useful or possible to use\", where useful is defined as \"that can help you to do or achieve what you want\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/maartjeth/survey _ useful _ summarization",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Different modalities are also important to investigate, but we leave this for future work to ensure clarity of our results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "This is based on people's initial responses and not on the follow up question if they selected another option than 'text'.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Because we do not find significant differences in the overall results when we exclude the few participants who did not find their summary helpful and we do not find many correlations w.r.t. how helpful a summary was and a particular context factor, we include all participants in the analysis, regardless of how helpful they found their summary, for completeness.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://surveymonkey.com",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank Jacobijn Sandberg and Ana Lucic for helpful comments and feedback. This research was supported by the Nationale Politie. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            },
            {
                "text": "Why do you think a pre-made summary would not have helped you with any of your recent study activities?Open response -participants are led to Q48Start branch of participants who described an existing summaryIf you have multiple study activities where you used a pre-made summary, please take the one where you found the pre-made summary most useful.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q5",
                "sec_num": null
            },
            {
                "text": "The original study material consisted of Mainly text -participants are led to Q8 Mainly figures -participants are led to Q7 Mainly video -participants are led to Q7 Mainly audio -participants are led to Q7 A combination of some or all of the above -participants are led to Q7 I do not know, because I have not seen the study material -participants are led to Q7 Other (please specify) -participants are led to Q7",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q6",
                "sec_num": null
            },
            {
                "text": "For now we narrow down our survey to study material that is mostly textual. Do you recall any other recent study activity where you made use of a pre-made summary and where the original study material mainly consisted of text? Yes -participants are led to Q8No -participants are led to Q48 The summary helped to retrieve parts of the original study material I used the summary to preview the text that I was about to read I used the summary as a substitute for the original study material I used the summary to refresh my memory of the original study material I used the summary as a reminder that I had to read the original study material ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q7",
                "sec_num": null
            },
            {
                "text": "What was the style of this summary? Informative: the summary simply conveyed the information that was in the original study material Indicative: the summary gave an idea of the topic of the study material, but not more Critical: the summary gave a critical review of the study material Aggregative: the summary put different source texts in relation to one another and by doing this gave an overview of a certain topic Other (please specify)Question Nr. Question and Answer Options",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q20",
                "sec_num": null
            },
            {
                "text": "Overall, how helpful was the pre-made summary for you? Your score can range from (1) Not helpful at all, to (5) Very helpful.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q21",
                "sec_num": null
            },
            {
                "text": "Very helpful",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Not helpful at all",
                "sec_num": null
            },
            {
                "text": "Imagine you could turn this summary into your ideal summary. What would you change?Open response",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q22",
                "sec_num": null
            },
            {
                "text": "Is there anything else you want us to know about the summary that we have not covered yet?Open response",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q23",
                "sec_num": null
            },
            {
                "text": "Is there anything else you want us to know about the original study material that we have not covered yet?Open response -participants are led to Q40Start branch of participants who described an imagined summaryPlease take one of these study activities in mind and imagine you would have had a pre-made summary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q24",
                "sec_num": null
            },
            {
                "text": "The Now we will ask some questions about the purpose of the pre-made summary that would have been helpful.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q25",
                "sec_num": null
            },
            {
                "text": "For what type of people should the summary ideally be intended? Your score can range from (1) Untargetted, to (5) Targetted.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q32",
                "sec_num": null
            },
            {
                "text": "No domain knowledge is expected from the users of the summmary.Targetted: Full domain knowledge is expected from the users of the summmary.(1)(2)(3) (4) (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Untargetted:",
                "sec_num": null
            },
            {
                "text": "How would this summary help you with your task? (Multiple answers possible)The summary would help to retrieve parts of the original study material I would use the summary to preview the text that I was about to read I would use the summary as a substitute for the original study material I would use the summary to refresh my memory of the original study material I would use the summary as a reminder that I had to read the original study material The summary would help to get an overview of the original study material The summary would help to understand the original study material', Other (please specify) Now we will ask some questions about what the summary should look like and cover.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q33",
                "sec_num": null
            },
            {
                "text": "What would be the ideal type of the summary? Open response",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q34",
                "sec_num": null
            },
            {
                "text": "Is there anything else you would want us to know about the original study material that we have not covered yet?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q39",
                "sec_num": null
            },
            {
                "text": "Now, let's assume the pre-made summary was generated by a computer. You can assume that this machine generated summary captures all the needs you have identified in the previous questions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Look out questions",
                "sec_num": null
            },
            {
                "text": "Would it make a difference to you whether the summary was generated by a computer program or by a human? Yes -participants are led to Q41No -participants are led to Q43",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q40",
                "sec_num": null
            },
            {
                "text": "Please explain the difference.Open response",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q41",
                "sec_num": null
            },
            {
                "text": "Which type of summary would you trust more: A summary generated by a human, for example a teacher or a good performing fellow student A summary generated by a computer No difference",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q42",
                "sec_num": null
            },
            {
                "text": "Please explain your answer.Open response",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q43",
                "sec_num": null
            },
            {
                "text": "Which type of summary would you trust more: A summary generated by a human, for example a teacher or a good performing fellow student A summary generated by a computer No difference",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q44",
                "sec_num": null
            },
            {
                "text": "Now imagine that you can interact with the computer program that made the summary, in the form of a digital assistant. Imagine that your digital assistant made an initial summary for you and you can ask questions about it to your digital assistant and the assistant can answer them. Answers can be voice output, but also screen output, e.g. a written summary on the screen. In the next part we would like to investigate how you would interact with the assistant. Please do not feel restricted by the capabilities of today's digital assistants.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Question Nr. Question and Answer Options",
                "sec_num": null
            },
            {
                "text": "Please choose the three most useful features for a digital assistant to have in this scenario.Summarize particular parts of the study material with more detail Summarize particular parts of the study material with less detail Switch between different summary styles (for example highlighting vs a generated small piece of text) Explain why particular pieces ended up in the summary Provide the source of certain parts of the summary on request Search for different related sources based on the content of the summary Answer specific questions based on the content of the summary",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q45",
                "sec_num": null
            },
            {
                "text": "Please choose the three least useful features for a digital assistant to have in this scenario.Summarize particular parts of the study material with more detail Summarize particular parts of the study material with less detail Switch between different summary styles (for example highlighting vs a generated small piece of text) Explain why particular pieces ended up in the summary Provide the source of certain parts of the summary on request Search for different related sources based on the content of the summary Answer specific questions based on the content of the summary",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q46",
                "sec_num": null
            },
            {
                "text": "Can you think of any other features that you would like your digital assistant to have to help you in this scenario?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q47",
                "sec_num": null
            },
            {
                "text": "Thank you for filling out this survey so far! We would still like to ask you two final background questions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background questions",
                "sec_num": null
            },
            {
                "text": "What \u2022 The style of which of these two summaries is most useful to you to prompt you to read a source text that is important for your task?Purpose factor Use & Output factor Format:\u2022 The format of which of these two summaries is most useful to you to retrieve a document that is important for your task? \u2022 The format of which of these two summaries is most useful to you to preview a document that is important for your task? \u2022 The format of which of these two summaries is most useful to you to substitute a document that is important for your task? \u2022 The format of which of these two summaries is most useful to you to refresh your memory about a document that is important for your task? \u2022 The format of which of these two summaries is most useful to you to prompt you to read a source text that is important for your task?Purpose factor Use & Output factor Material:\u2022 The coverage of which of these two summaries is most useful to you to retrieve a document that is important for your task? \u2022 The coverage of which of these two summaries is most useful to you to preview a document that is important for your task?\u2022 The coverage of which of these two summaries is most useful to you to substitute a document that is important for your task? \u2022 The coverage of which of these two summaries is most useful to you to refresh your memory about a document that is important for your task? \u2022 The coverage of which of these two summaries is most useful to you to prompt you to read a source text that is important for your task?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Q48",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Teacher educators' use of digital tools and needs for digital competence in higher education",
                "authors": [
                    {
                        "first": "Lisbeth",
                        "middle": [],
                        "last": "Amhag",
                        "suffix": ""
                    },
                    {
                        "first": "Lisa",
                        "middle": [],
                        "last": "Hellstr\u00f6m",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Stigmar",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Journal of Digital Learning in Teacher Education",
                "volume": "35",
                "issue": "4",
                "pages": "203--220",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lisbeth Amhag, Lisa Hellstr\u00f6m, and Martin Stigmar. 2019. Teacher educators' use of digital tools and needs for digital competence in higher education. Journal of Digital Learning in Teacher Education, 35(4):203-220.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Unsupervised opinion summarization with noising and denoising",
                "authors": [
                    {
                        "first": "Reinald",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Amplayo",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1934--1945",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.175"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Reinald Kim Amplayo and Mirella Lapata. 2020. Un- supervised opinion summarization with noising and denoising. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 1934-1945, Online. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Automatic text summarization: What has been done and what has to be done",
                "authors": [
                    {
                        "first": "Abdelkrime",
                        "middle": [],
                        "last": "Aries",
                        "suffix": ""
                    },
                    {
                        "first": "Djamel",
                        "middle": [
                            "Eddine"
                        ],
                        "last": "Zegour",
                        "suffix": ""
                    },
                    {
                        "first": "Walid-Khaled",
                        "middle": [],
                        "last": "Hidouci",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Abdelkrime Aries, Djamel Eddine Zegour, and Walid- Khaled Hidouci. 2019. Automatic text summariza- tion: What has been done and what has to be done. CoRR, abs/1904.00688.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Modeling the impact of shortand long-term behavior on search personalization",
                "authors": [
                    {
                        "first": "Paul",
                        "middle": [
                            "N"
                        ],
                        "last": "Bennett",
                        "suffix": ""
                    },
                    {
                        "first": "Ryen",
                        "middle": [
                            "W"
                        ],
                        "last": "White",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Chu",
                        "suffix": ""
                    },
                    {
                        "first": "Susan",
                        "middle": [
                            "T"
                        ],
                        "last": "Dumais",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Bailey",
                        "suffix": ""
                    },
                    {
                        "first": "Fedor",
                        "middle": [],
                        "last": "Borisyuk",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyuan",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12",
                "volume": "",
                "issue": "",
                "pages": "185--194",
                "other_ids": {
                    "DOI": [
                        "10.1145/2348283.2348312"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Paul N. Bennett, Ryen W. White, Wei Chu, Susan T. Dumais, Peter Bailey, Fedor Borisyuk, and Xi- aoyuan Cui. 2012. Modeling the impact of short- and long-term behavior on search personalization. In The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012, pages 185-194. ACM.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "The IIR evaluation model: a framework for evaluation of interactive information retrieval systems",
                "authors": [
                    {
                        "first": "Pia",
                        "middle": [],
                        "last": "Borlund",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Information Research",
                "volume": "8",
                "issue": "3",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pia Borlund. 2003. The IIR evaluation model: a frame- work for evaluation of interactive information re- trieval systems. Information Research, 8(3).",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "A study of the use of simulated work task situations in interactive information retrieval evaluations: A meta-evaluation",
                "authors": [
                    {
                        "first": "Pia",
                        "middle": [],
                        "last": "Borlund",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "J. Documentation",
                "volume": "72",
                "issue": "3",
                "pages": "394--413",
                "other_ids": {
                    "DOI": [
                        "10.1108/JD-06-2015-0068"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pia Borlund. 2016. A study of the use of simulated work task situations in interactive information re- trieval evaluations: A meta-evaluation. J. Documen- tation, 72(3):394-413.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Retrieve, rerank and rewrite: Soft template based neural summarization",
                "authors": [
                    {
                        "first": "Ziqiang",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Wenjie",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Sujian",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "152--161",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P18-1015"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. 2018. Retrieve, rerank and rewrite: Soft template based neural summarization. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 152-161, Melbourne, Australia. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Neural summarization by extracting sentences and words",
                "authors": [
                    {
                        "first": "Jianpeng",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "484--494",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P16-1046"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jianpeng Cheng and Mirella Lapata. 2016. Neural sum- marization by extracting sentences and words. In Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 484-494, Berlin, Germany. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Abstractive sentence summarization with attentive recurrent neural networks",
                "authors": [
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "M"
                        ],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "93--98",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N16-1012"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sumit Chopra, Michael Auli, and Alexander M. Rush. 2016. Abstractive sentence summarization with at- tentive recurrent neural networks. In Proceedings of the 2016 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, pages 93-98, San Diego, California. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "A discourse-aware attention model for abstractive summarization of long documents",
                "authors": [
                    {
                        "first": "Arman",
                        "middle": [],
                        "last": "Cohan",
                        "suffix": ""
                    },
                    {
                        "first": "Franck",
                        "middle": [],
                        "last": "Dernoncourt",
                        "suffix": ""
                    },
                    {
                        "first": "Soon",
                        "middle": [],
                        "last": "Doo",
                        "suffix": ""
                    },
                    {
                        "first": "Trung",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Seokhwan",
                        "middle": [],
                        "last": "Bui",
                        "suffix": ""
                    },
                    {
                        "first": "Walter",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Nazli",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Goharian",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "2",
                "issue": "",
                "pages": "615--621",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-2097"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Na- zli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long docu- ments. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers), pages 615-621, New Orleans, Louisiana. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Multi-hop inference for question-driven summarization",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Wenxuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Wai",
                        "middle": [],
                        "last": "Lam",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "6734--6744",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.emnlp-main.547"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yang Deng, Wenxuan Zhang, and Wai Lam. 2020. Multi-hop inference for question-driven summariza- tion. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 6734-6744, Online. Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1423"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "A methodology for extrinsic evaluation of text summarization: Does ROUGE correlate?",
                "authors": [
                    {
                        "first": "Bonnie",
                        "middle": [],
                        "last": "Dorr",
                        "suffix": ""
                    },
                    {
                        "first": "Christof",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    },
                    {
                        "first": "Stacy",
                        "middle": [],
                        "last": "President",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Schwartz",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Zajic",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
                "volume": "",
                "issue": "",
                "pages": "1--8",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bonnie Dorr, Christof Monz, Stacy President, Richard Schwartz, and David Zajic. 2005. A methodol- ogy for extrinsic evaluation of text summarization: Does ROUGE correlate? In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summa- rization, pages 1-8, Ann Arbor, Michigan. Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Duc 2003: Documents, tasks, and measures",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Duc",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "DUC. 2003. Duc 2003: Documents, tasks, and mea- sures. https://duc.nist.gov/duc2003/ tasks.html.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
                "authors": [
                    {
                        "first": "Esin",
                        "middle": [],
                        "last": "Durmus",
                        "suffix": ""
                    },
                    {
                        "first": "He",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Mona",
                        "middle": [],
                        "last": "Diab",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "5055--5070",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.454"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faith- fulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 5055- 5070, Online. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Bringing structure into summaries: Crowdsourcing a benchmark corpus of concept maps",
                "authors": [
                    {
                        "first": "Tobias",
                        "middle": [],
                        "last": "Falke",
                        "suffix": ""
                    },
                    {
                        "first": "Iryna",
                        "middle": [],
                        "last": "Gurevych",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "2951--2961",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/d17-1320"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tobias Falke and Iryna Gurevych. 2017. Bringing structure into summaries: Crowdsourcing a bench- mark corpus of concept maps. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 2951-2961. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Bottom-up abstractive summarization",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Gehrmann",
                        "suffix": ""
                    },
                    {
                        "first": "Yuntian",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "4098--4109",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1443"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing, pages 4098-4109, Brussels, Belgium. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Assessing the factual accuracy of generated text",
                "authors": [
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Goodrich",
                        "suffix": ""
                    },
                    {
                        "first": "Vinay",
                        "middle": [],
                        "last": "Rao",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Saleh",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
                "volume": "",
                "issue": "",
                "pages": "166--175",
                "other_ids": {
                    "DOI": [
                        "10.1145/3292500.3330955"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ben Goodrich, Vinay Rao, Peter J. Liu, and Moham- mad Saleh. 2019. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pages 166-175. ACM.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Application of technology in the digital era education",
                "authors": [
                    {
                        "first": "Harwati",
                        "middle": [],
                        "last": "Hashim",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "International Journal of Research in Counseling and Education",
                "volume": "2",
                "issue": "1",
                "pages": "1--5",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Harwati Hashim. 2018. Application of technology in the digital era education. International Journal of Research in Counseling and Education, 2(1):1-5.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Teaching machines to read and comprehend",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Moritz Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Tom\u00e1s",
                        "middle": [],
                        "last": "Kocisk\u00fd",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Lasse",
                        "middle": [],
                        "last": "Espeholt",
                        "suffix": ""
                    },
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Kay",
                        "suffix": ""
                    },
                    {
                        "first": "Mustafa",
                        "middle": [],
                        "last": "Suleyman",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "1693--1701",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karl Moritz Hermann, Tom\u00e1s Kocisk\u00fd, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Infor- mation Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 1693-1701.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Mohit Bansal, and Ido Dagan. 2021. iFacetSum: Coreference-based interactive faceted summarization for multi-document exploration",
                "authors": [
                    {
                        "first": "Eran",
                        "middle": [],
                        "last": "Hirsch",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Eirew",
                        "suffix": ""
                    },
                    {
                        "first": "Ori",
                        "middle": [],
                        "last": "Shapira",
                        "suffix": ""
                    },
                    {
                        "first": "Avi",
                        "middle": [],
                        "last": "Caciularu",
                        "suffix": ""
                    },
                    {
                        "first": "Arie",
                        "middle": [],
                        "last": "Cattan",
                        "suffix": ""
                    },
                    {
                        "first": "Ori",
                        "middle": [],
                        "last": "Ernst",
                        "suffix": ""
                    },
                    {
                        "first": "Ramakanth",
                        "middle": [],
                        "last": "Pasunuru",
                        "suffix": ""
                    },
                    {
                        "first": "Ronen",
                        "middle": [],
                        "last": "Hadar",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "283--297",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eran Hirsch, Alon Eirew, Ori Shapira, Avi Caciu- laru, Arie Cattan, Ori Ernst, Ramakanth Pasunuru, Hadar Ronen, Mohit Bansal, and Ido Dagan. 2021. iFacetSum: Coreference-based interactive faceted summarization for multi-document exploration. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 283-297, Online and Punta Cana, Dominican Republic. Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "LC-STS: A large scale Chinese short text summarization dataset",
                "authors": [
                    {
                        "first": "Baotian",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Qingcai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Fangze",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1967--1972",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D15-1229"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. LC- STS: A large scale Chinese short text summarization dataset. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1967-1972, Lisbon, Portugal. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Wikihow: A large scale text summarization dataset",
                "authors": [
                    {
                        "first": "Mahnaz",
                        "middle": [],
                        "last": "Koupaee",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mahnaz Koupaee and William Yang Wang. 2018. Wik- ihow: A large scale text summarization dataset. CoRR, abs/1810.09305.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "WikiLingua: A new benchmark dataset for cross-lingual abstractive summa-rization",
                "authors": [
                    {
                        "first": "Faisal",
                        "middle": [],
                        "last": "Ladhak",
                        "suffix": ""
                    },
                    {
                        "first": "Esin",
                        "middle": [],
                        "last": "Durmus",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    },
                    {
                        "first": "Kathleen",
                        "middle": [],
                        "last": "Mckeown",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "volume": "",
                "issue": "",
                "pages": "4034--4048",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.findings-emnlp.360"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Faisal Ladhak, Esin Durmus, Claire Cardie, and Kath- leen McKeown. 2020. WikiLingua: A new bench- mark dataset for cross-lingual abstractive summa- rization. In Findings of the Association for Com- putational Linguistics: EMNLP 2020, pages 4034- 4048, Online. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal ; Abdelrahman Mohamed",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "7871--7880",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.703"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "ROUGE: A package for automatic evaluation of summaries",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Text Summarization Branches Out",
                "volume": "",
                "issue": "",
                "pages": "74--81",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Query-based summarization using MDL principle",
                "authors": [
                    {
                        "first": "Marina",
                        "middle": [],
                        "last": "Litvak",
                        "suffix": ""
                    },
                    {
                        "first": "Natalia",
                        "middle": [],
                        "last": "Vanetik",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Workshop on Summarization and Summary Evaluation Across Source Types and Genres, MultiLing@EACL 2017",
                "volume": "",
                "issue": "",
                "pages": "22--31",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/w17-1004"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Marina Litvak and Natalia Vanetik. 2017. Query-based summarization using MDL principle. In Proceed- ings of the Workshop on Summarization and Sum- mary Evaluation Across Source Types and Genres, MultiLing@EACL 2017, Valencia, Spain, April 3, 2017, pages 22-31. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Topic-aware contrastive learning for abstractive dialogue summarization",
                "authors": [
                    {
                        "first": "Junpeng",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yanyan",
                        "middle": [],
                        "last": "Zou",
                        "suffix": ""
                    },
                    {
                        "first": "Hainan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Hongshen",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoye",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Caixia",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojie",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "volume": "",
                "issue": "",
                "pages": "1229--1243",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Junpeng Liu, Yanyan Zou, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Caixia Yuan, and Xiaojie Wang. 2021. Topic-aware contrastive learning for abstrac- tive dialogue summarization. In Findings of the As- sociation for Computational Linguistics: EMNLP 2021, pages 1229-1243, Punta Cana, Dominican Re- public. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Generating wikipedia by summarizing long sequences",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Etienne",
                        "middle": [],
                        "last": "Saleh",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Pot",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Goodrich",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Sepassi",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summariz- ing long sequences. In 6th International Conference on Learning Representations, ICLR 2018, Vancou- ver, BC, Canada, April 30 -May 3, 2018, Confer- ence Track Proceedings. OpenReview.net.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Text summarization with pretrained encoders",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "3730--3740",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1387"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu and Mirella Lapata. 2019. Text summariza- tion with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730-3740, Hong Kong, China. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Decoding learning: The proof, promise and potential of digital education",
                "authors": [
                    {
                        "first": "Rosemary",
                        "middle": [],
                        "last": "Luckin",
                        "suffix": ""
                    },
                    {
                        "first": "Brett",
                        "middle": [],
                        "last": "Bligh",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Manches",
                        "suffix": ""
                    },
                    {
                        "first": "Shaaron",
                        "middle": [],
                        "last": "Ainsworth",
                        "suffix": ""
                    },
                    {
                        "first": "Charles",
                        "middle": [],
                        "last": "Crook",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Noss",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rosemary Luckin, Brett Bligh, Andrew Manches, Shaaron Ainsworth, Charles Crook, and Richard Noss. 2012. Decoding learning: The proof, promise and potential of digital education. Nesta.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Automatic summarization",
                "authors": [
                    {
                        "first": "Inderjeet",
                        "middle": [],
                        "last": "Mani",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Inderjeet Mani. 2001a. Automatic summarization, vol- ume 3. John Benjamins Publishing.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Summarization evaluation: An overview",
                "authors": [
                    {
                        "first": "Inderjeet",
                        "middle": [],
                        "last": "Mani",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the Third Second Workshop Meeting on Evaluation of Chinese & Japanese Text Retrieval and Text Summarization",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Inderjeet Mani. 2001b. Summarization evaluation: An overview. In Proceedings of the Third Second Work- shop Meeting on Evaluation of Chinese & Japanese Text Retrieval and Text Summarization, NTCIR-2, Tokyo, Japan, March 7-9, 2001. National Institute of Informatics (NII).",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Bringing structure into summaries: a faceted summarization dataset for long scientific documents",
                "authors": [
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Meng",
                        "suffix": ""
                    },
                    {
                        "first": "Khushboo",
                        "middle": [],
                        "last": "Thaker",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Xingdi",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Tong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Daqing",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "volume": "2",
                "issue": "",
                "pages": "1080--1089",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.acl-short.137"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, and Daqing He. 2021. Bringing structure into summaries: a faceted sum- marization dataset for long scientific documents. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (Volume 2: Short Papers), pages 1080-1089, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Personalized text summarization based on important terms identification",
                "authors": [
                    {
                        "first": "R\u00f3bert",
                        "middle": [],
                        "last": "M\u00f3ro",
                        "suffix": ""
                    },
                    {
                        "first": "M\u00e1ria",
                        "middle": [],
                        "last": "Bielikov\u00e1",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "23rd International Workshop on Database and Expert Systems Applications, DEXA 2012",
                "volume": "",
                "issue": "",
                "pages": "131--135",
                "other_ids": {
                    "DOI": [
                        "10.1109/DEXA.2012.47"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "R\u00f3bert M\u00f3ro and M\u00e1ria Bielikov\u00e1. 2012. Personal- ized text summarization based on important terms identification. In 23rd International Workshop on Database and Expert Systems Applications, DEXA 2012, Vienna, Austria, September 3-7, 2012, pages 131-135. IEEE Computer Society.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
                "authors": [
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Nallapati",
                        "suffix": ""
                    },
                    {
                        "first": "Feifei",
                        "middle": [],
                        "last": "Zhai",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "3075--3081",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based se- quence model for extractive summarization of doc- uments. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pages 3075- 3081. AAAI Press.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "2018a. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
                "authors": [
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Shay",
                        "middle": [
                            "B"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1797--1807",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/d18-1206"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018a. Don't give me the details, just the summary! topic-aware convolutional neural networks for ex- treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1797-1807. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Ranking sentences for extractive summarization with reinforcement learning",
                "authors": [
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Shay",
                        "middle": [
                            "B"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "1747--1759",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-1158"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018b. Ranking sentences for extractive summariza- tion with reinforcement learning. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Pa- pers), pages 1747-1759, New Orleans, Louisiana. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Diversity driven attention model for query-based abstractive summarization",
                "authors": [
                    {
                        "first": "Preksha",
                        "middle": [],
                        "last": "Nema",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Mitesh",
                        "suffix": ""
                    },
                    {
                        "first": "Anirban",
                        "middle": [],
                        "last": "Khapra",
                        "suffix": ""
                    },
                    {
                        "first": "Balaraman",
                        "middle": [],
                        "last": "Laha",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ravindran",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1063--1072",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P17-1098"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Preksha Nema, Mitesh M. Khapra, Anirban Laha, and Balaraman Ravindran. 2017. Diversity driven atten- tion model for query-based abstractive summariza- tion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 1063-1072, Vancouver, Canada. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Evaluating content selection in summarization: The pyramid method",
                "authors": [
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    },
                    {
                        "first": "Rebecca",
                        "middle": [],
                        "last": "Passonneau",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004",
                "volume": "",
                "issue": "",
                "pages": "145--152",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ani Nenkova and Rebecca Passonneau. 2004. Evaluat- ing content selection in summarization: The pyra- mid method. In Proceedings of the Human Lan- guage Technology Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 145-152, Boston, Massachusetts, USA. Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Screenplay summarization using latent narrative structure",
                "authors": [
                    {
                        "first": "Pinelopi",
                        "middle": [],
                        "last": "Papalampidi",
                        "suffix": ""
                    },
                    {
                        "first": "Frank",
                        "middle": [],
                        "last": "Keller",
                        "suffix": ""
                    },
                    {
                        "first": "Lea",
                        "middle": [],
                        "last": "Frermann",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1920--1933",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.174"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pinelopi Papalampidi, Frank Keller, Lea Frermann, and Mirella Lapata. 2020. Screenplay summarization us- ing latent narrative structure. In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics, pages 1920-1933, Online. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "A deep reinforced model for abstractive summarization",
                "authors": [
                    {
                        "first": "Romain",
                        "middle": [],
                        "last": "Paulus",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "6th International Conference on Learning Representations, ICLR 2018",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive sum- marization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenReview.net.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "A comparison of texts and their summaries: Memorial consequences",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Lynne",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "R"
                        ],
                        "last": "Reder",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Anderson",
                        "suffix": ""
                    }
                ],
                "year": 1980,
                "venue": "Journal of Verbal Learning and Verbal Behavior",
                "volume": "19",
                "issue": "2",
                "pages": "121--134",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lynne M Reder and John R Anderson. 1980. A com- parison of texts and their summaries: Memorial con- sequences. Journal of Verbal Learning and Verbal Behavior, 19(2):121-134.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Udo Kruschwitz, and Massimo Poesio. 2015. The SEN-SEI project: Making sense of human conversations",
                "authors": [
                    {
                        "first": "Giuseppe",
                        "middle": [],
                        "last": "Riccardi",
                        "suffix": ""
                    },
                    {
                        "first": "Fr\u00e9d\u00e9ric",
                        "middle": [],
                        "last": "B\u00e9chet",
                        "suffix": ""
                    },
                    {
                        "first": "Morena",
                        "middle": [],
                        "last": "Danieli",
                        "suffix": ""
                    },
                    {
                        "first": "Beno\u00eet",
                        "middle": [],
                        "last": "Favre",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "J"
                        ],
                        "last": "Gaizauskas",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Future and Emergent Trends in Language Technology -First International Workshop",
                "volume": "9577",
                "issue": "",
                "pages": "10--33",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-319-33500-1_2"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Giuseppe Riccardi, Fr\u00e9d\u00e9ric B\u00e9chet, Morena Danieli, Beno\u00eet Favre, Robert J. Gaizauskas, Udo Kr- uschwitz, and Massimo Poesio. 2015. The SEN- SEI project: Making sense of human conversations. In Future and Emergent Trends in Language Tech- nology -First International Workshop, FETLT 2015, Seville, Spain, November 19-20, 2015, Revised Se- lected Papers, volume 9577 of Lecture Notes in Computer Science, pages 10-33. Springer.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "A neural attention model for abstractive sentence summarization",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [
                            "M"
                        ],
                        "last": "Rush",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "379--389",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D15-1044"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sen- tence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan- guage Processing, pages 379-389, Lisbon, Portugal. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "The New York Times annotated corpus. Linguistic Data Consortium",
                "authors": [
                    {
                        "first": "Evan",
                        "middle": [],
                        "last": "Sandhaus",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "",
                "volume": "6",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Evan Sandhaus. 2008. The New York Times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Get to the point: Summarization with pointergenerator networks",
                "authors": [
                    {
                        "first": "Abigail",
                        "middle": [],
                        "last": "See",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1073--1083",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P17-1099"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer- generator networks. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073- 1083, Vancouver, Canada. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Automatic summarizing: factors and directions",
                "authors": [
                    {
                        "first": "Karen",
                        "middle": [],
                        "last": "Sp\u00e4rck",
                        "suffix": ""
                    },
                    {
                        "first": "Jones",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Advances in automatic text summarization",
                "volume": "1",
                "issue": "",
                "pages": "1--12",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karen Sp\u00e4rck Jones. 1998. Automatic summarizing: factors and directions. In Advances in automatic text summarization, 1, pages 1-12. MIT press Cam- bridge, Mass, USA.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Abstractive document summarization with a graphbased attentional neural model",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    },
                    {
                        "first": "Jianguo",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1171--1181",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P17-1108"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017. Abstractive document summarization with a graph- based attentional neural model. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1171-1181, Vancouver, Canada. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Conversations with documents: An exploration of document-centered assistance",
                "authors": [
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Maartje Ter Hoeve",
                        "suffix": ""
                    },
                    {
                        "first": "Elnaz",
                        "middle": [],
                        "last": "Sim",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Nouri",
                        "suffix": ""
                    },
                    {
                        "first": "Maarten",
                        "middle": [],
                        "last": "Fourney",
                        "suffix": ""
                    },
                    {
                        "first": "Ryen",
                        "middle": [
                            "W"
                        ],
                        "last": "De Rijke",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "White",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "CHIIR '20: Conference on Human Information Interaction and Retrieval",
                "volume": "",
                "issue": "",
                "pages": "43--52",
                "other_ids": {
                    "DOI": [
                        "10.1145/3343413.3377971"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maartje ter Hoeve, Robert Sim, Elnaz Nouri, Adam Fourney, Maarten de Rijke, and Ryen W. White. 2020. Conversations with documents: An explo- ration of document-centered assistance. In CHIIR '20: Conference on Human Information Interaction and Retrieval, Vancouver, BC, Canada, March 14- 18, 2020, pages 43-52. ACM.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4- 9, 2017, Long Beach, CA, USA, pages 5998-6008.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "TL;DR: Mining Reddit to learn automatic summarization",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "V\u00f6lske",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Potthast",
                        "suffix": ""
                    },
                    {
                        "first": "Shahbaz",
                        "middle": [],
                        "last": "Syed",
                        "suffix": ""
                    },
                    {
                        "first": "Benno",
                        "middle": [],
                        "last": "Stein",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Workshop on New Frontiers in Summarization",
                "volume": "",
                "issue": "",
                "pages": "59--63",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W17-4508"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Michael V\u00f6lske, Martin Potthast, Shahbaz Syed, and Benno Stein. 2017. TL;DR: Mining Reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages 59-63, Copenhagen, Denmark. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Asking and answering questions to evaluate the factual consistency of summaries",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "5008--5020",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.450"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the fac- tual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "A sentence compression based framework to query-focused multidocument summarization",
                "authors": [
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Hema",
                        "middle": [],
                        "last": "Raghavan",
                        "suffix": ""
                    },
                    {
                        "first": "Vittorio",
                        "middle": [],
                        "last": "Castelli",
                        "suffix": ""
                    },
                    {
                        "first": "Radu",
                        "middle": [],
                        "last": "Florian",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo- rian, and Claire Cardie. 2016. A sentence com- pression based framework to query-focused multi- document summarization. CoRR, abs/1606.07548.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Extracting summary knowledge graphs from long documents",
                "authors": [
                    {
                        "first": "Zeqiu",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Rik",
                        "middle": [],
                        "last": "Koncel-Kedziorski",
                        "suffix": ""
                    },
                    {
                        "first": "Mari",
                        "middle": [],
                        "last": "Ostendorf",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zeqiu Wu, Rik Koncel-Kedziorski, Mari Ostendorf, and Hannaneh Hajishirzi. 2020. Extracting sum- mary knowledge graphs from long documents. CoRR, abs/2009.09162.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Factual consistency evaluation for text summarization via counterfactual estimation",
                "authors": [
                    {
                        "first": "Yuexiang",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Yaliang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Bolin",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana",
                "volume": "",
                "issue": "",
                "pages": "100--110",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.findings-emnlp.10"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, and Bolin Ding. 2021. Factual consistency evaluation for text summarization via counterfactual estimation. In Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 100-110. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Discourse-aware neural extractive text summarization",
                "authors": [
                    {
                        "first": "Jiacheng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "5021--5031",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.451"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Discourse-aware neural extractive text sum- marization. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 5021-5031, Online. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Graph-based neural multi-document summarization",
                "authors": [
                    {
                        "first": "Michihiro",
                        "middle": [],
                        "last": "Yasunaga",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Kshitijh",
                        "middle": [],
                        "last": "Meelu",
                        "suffix": ""
                    },
                    {
                        "first": "Ayush",
                        "middle": [],
                        "last": "Pareek",
                        "suffix": ""
                    },
                    {
                        "first": "Krishnan",
                        "middle": [],
                        "last": "Srinivasan",
                        "suffix": ""
                    },
                    {
                        "first": "Dragomir",
                        "middle": [],
                        "last": "Radev",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "452--462",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/K17-1045"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, and Dragomir Radev. 2017. Graph-based neural multi-document summarization. In Proceedings of the 21st Confer- ence on Computational Natural Language Learning (CoNLL 2017), pages 452-462, Vancouver, Canada. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Summarizing and exploring tabular data in conversational search",
                "authors": [
                    {
                        "first": "Shuo",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuyun",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Krisztian",
                        "middle": [],
                        "last": "Balog",
                        "suffix": ""
                    },
                    {
                        "first": "Jamie",
                        "middle": [],
                        "last": "Callan",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "1537--1540",
                "other_ids": {
                    "DOI": [
                        "10.1145/3397271.3401205"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shuo Zhang, Zhuyun Dai, Krisztian Balog, and Jamie Callan. 2020a. Summarizing and exploring tabu- lar data in conversational search. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, pages 1537-1540. ACM.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Bertscore: Evaluating text generation with BERT",
                "authors": [
                    {
                        "first": "Tianyi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Varsha",
                        "middle": [],
                        "last": "Kishore",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [
                            "Q"
                        ],
                        "last": "Weinberger",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "th International Conference on Learning Representations, ICLR 2020",
                "volume": "8",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020b. Bertscore: Evaluating text generation with BERT. In 8th Inter- national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "uris": null,
                "fig_num": null,
                "text": "",
                "type_str": "figure",
                "num": null
            },
            "FIGREF1": {
                "uris": null,
                "fig_num": null,
                "text": "(a) Most current automatic text summarization techniques. Left: input document. Right: summary. Example of summarizing while taking users' wishes and desires into account. Left: input document. Right: summary.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF2": {
                "uris": null,
                "fig_num": "1",
                "text": "Figure 1: Example of most current summarization techniques vs. summarization while incorporating the users in the process.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF3": {
                "uris": null,
                "fig_num": "2",
                "text": "Figure 2: Participant details.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF4": {
                "uris": null,
                "fig_num": "3",
                "text": "Figure 3: Overview of the survey procedure.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF5": {
                "uris": null,
                "fig_num": "4",
                "text": "Figure 4: Results for the input factor questions. Specific input factor in italics. Answer type in brackets: MC = Multiple Choice, MR = Multiple Response. ** indicates significance (\u03c7 2 ), after Bonferroni correction, with p 0.001. If two options are flagged with **, these options are not significantly different from each other, yet both have been chosen significantly more often than the other options.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF7": {
                "uris": null,
                "fig_num": "5",
                "text": "Figure 5: Results for the purpose factor questions. Specific purpose factor in italics. Answer type in brackets: MC = Multiple Choice, MR = Multiple Response, LS = Likert Scale. ** indicates significance (\u03c7 2 ), after Bonferroni correction, with p 0.001, * with p < 0.05. \u2020 indicates noteworthy results where significance was lost after correction for the number of tests. If two options are flagged, these options are not significantly different from each other, yet both were chosen significantly more often than the other options.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF8": {
                "uris": null,
                "fig_num": "6",
                "text": "Figure 6: Results for the output factor questions. Specific output factor in italics. Answer type in brackets: MC = Multiple Choice, MR = Multiple Response, LS = Likert Scale. ** indicates significance (\u03c7 2 or Fisher's exact test), after Bonferroni correction, with p 0.001, * with p < 0.05.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF9": {
                "uris": null,
                "fig_num": "7",
                "text": "Figure 7: Overview survey design.",
                "type_str": "figure",
                "num": null
            },
            "TABREF4": {
                "text": "): What was the type of the summary? (MC)",
                "content": "<table><tr><td/><td/><td/><td colspan=\"2\">Lecture notes</td><td>19.2</td></tr><tr><td/><td/><td/><td colspan=\"2\">Blog post</td><td>2.7</td></tr><tr><td colspan=\"2\">Summary type</td><td colspan=\"3\">Highlights Abstractive text Short video</td><td>2.7</td><td>19.2</td><td>53.4**</td></tr><tr><td/><td/><td/><td colspan=\"2\">Slide show</td><td>0.0</td></tr><tr><td/><td/><td/><td colspan=\"2\">Other</td><td>2.7</td><td>all resps.</td></tr><tr><td/><td/><td/><td/><td colspan=\"2\">0% Percentage of Respondents 50% 100%</td></tr><tr><td colspan=\"6\">(b) Format (2): How was the summary</td></tr><tr><td colspan=\"6\">structured? (MR)</td></tr><tr><td colspan=\"2\">Summary structure</td><td colspan=\"4\">0% Percentage of Respondents 50% 100% Running text Highlights Special formatting Diagrams Tables Graphs Figures Headings Sections / paragraphs Other 14.8 8.3 remembered 24.6 8.3 3.3 83.3 67.2 83.3 70.5 41.7 47.5 41.7 24.6 41.7 26.2 33.3 19.7 58.3 57.4 16.7 imagined</td></tr><tr><td colspan=\"6\">(c) Material: How much of the study</td></tr><tr><td colspan=\"6\">material was covered by the summary?</td></tr><tr><td colspan=\"4\">(LS)</td><td/></tr><tr><td/><td/><td/><td colspan=\"2\">None</td><td>0.0</td></tr><tr><td colspan=\"3\">Summary coverage</td><td colspan=\"2\">Almost none Some Most</td><td>0.0</td><td>15.1</td><td>63.0**</td></tr><tr><td/><td/><td/><td/><td>All</td><td>21.9</td><td>all resps.</td></tr><tr><td/><td/><td/><td/><td colspan=\"2\">0% Percentage of Respondents 50% 100%</td></tr><tr><td colspan=\"6\">(d) Style: What was the style of this</td></tr><tr><td colspan=\"6\">summary? (MC)</td></tr><tr><td/><td colspan=\"3\">Informative</td><td/><td>50.0</td><td>91.8**</td></tr><tr><td>Summary style</td><td colspan=\"3\">Indicative Critical Aggregative</td><td colspan=\"2\">4.9 8.3 8.3** 3.3</td><td>33.3**</td></tr><tr><td/><td/><td/><td/><td/><td>remembered</td></tr><tr><td/><td/><td/><td>Other</td><td/><td>imagined</td></tr><tr><td/><td/><td/><td colspan=\"3\">0% 25% 50% 75% 100% Percentage of Respondents</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF6": {
                "text": "Implications for future research directions.",
                "content": "<table/>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF7": {
                "text": "Overview of different context factors classes defined by Sp\u00e4rck Jones (1998), with descriptions of the factors within these classes.",
                "content": "<table/>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF8": {
                "text": "are no restrictions on what these pre-made summaries can look like. On the contrary, that is one of the things we aim to find out with this survey! But, to give some examples, you could think of a written overview of a text book, highlights in text to draw your attention to important bits, blog posts, etc. These are really just examples and don't let them limit your creativity! You can come up with any example of a pre-made summary that is helpful for you.",
                "content": "<table><tr><td/><td>Yes, I understand what a pre-made summary is!</td></tr><tr><td/><td>Yes</td></tr><tr><td>Q3</td><td>Please think back to your recent study activities. Examples of study activities can be:</td></tr><tr><td/><td>studying for an exam, writing a paper, doing homework exercises, etc. Note that these</td></tr><tr><td/><td>are just examples, any other study activity is fine too.</td></tr><tr><td/><td>Did you use a pre-made summary in any of these study activities?</td></tr><tr><td/><td>Yes -participants are led to Q6</td></tr><tr><td/><td>No -participants are led to Q4</td></tr><tr><td>Q4</td><td>Can you think of one of your recent study activities where a pre-made summary</td></tr><tr><td/><td>would have been useful for you?</td></tr><tr><td/><td>Yes -participants are led to Q25</td></tr><tr><td/><td>No -participants are led to Q5</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            }
        }
    }
}
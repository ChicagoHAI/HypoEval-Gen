{
    "paper_id": "2212",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2025-02-14T13:46:52.903213Z"
    },
    "title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation",
    "authors": [
        {
            "first": "Yixin",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Yale University",
                "location": {
                    "addrLine": "2 Salesforce AI"
                }
            },
            "email": "yixin.liu@yale.edu"
        },
        {
            "first": "Alexander",
            "middle": [
                "R"
            ],
            "last": "Fabbri",
            "suffix": "",
            "affiliation": {},
            "email": "afabbri@salesforce.com"
        },
        {
            "first": "Pengfei",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Carnegie Mellon University",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Yilun",
            "middle": [],
            "last": "Zhao",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Yale University",
                "location": {
                    "addrLine": "2 Salesforce AI"
                }
            },
            "email": ""
        },
        {
            "first": "Linyong",
            "middle": [],
            "last": "Nan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Yale University",
                "location": {
                    "addrLine": "2 Salesforce AI"
                }
            },
            "email": ""
        },
        {
            "first": "Ruilin",
            "middle": [],
            "last": "Han",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Yale University",
                "location": {
                    "addrLine": "2 Salesforce AI"
                }
            },
            "email": ""
        },
        {
            "first": "Simeng",
            "middle": [],
            "last": "Han",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Shafiq",
            "middle": [],
            "last": "Joty",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Yale University",
                "location": {
                    "addrLine": "2 Salesforce AI"
                }
            },
            "email": ""
        },
        {
            "first": "Chien-Sheng",
            "middle": [],
            "last": "Wu",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Caiming",
            "middle": [],
            "last": "Xiong",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Dragomir",
            "middle": [],
            "last": "Radev",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Yale University",
                "location": {
                    "addrLine": "2 Salesforce AI"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high interannotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets.\n(3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.",
    "pdf_parse": {
        "paper_id": "2212",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high interannotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "(3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Human evaluation plays an essential role in both assessing the rapid development of summarization systems in recent years (Lewis et al., 2020a; Zhang et al., 2020a; Brown et al., 2020; Sanh et al., 2022; He et al., 2022) and in assessing the ability of automatic metrics to evaluate such systems as a proxy * Equal contribution for manual evaluation (Bhandari et al., 2020; Fabbri et al., 2022a; Gao and Wan, 2022) . However, while human evaluation is regarded as the gold standard for evaluating both summarization systems and automatic metrics, as suggested by Clark et al. (2021) an evaluation study does not become \"gold\" automatically without proper practices. For example, achieving a high inter-annotator agreement among annotators can be difficult (Goyal et al., 2022) , and there can be a near-zero correlation between the annotations of crowd-workers and expert annotators (Fabbri et al., 2022a) . Also, a human evaluation study without a large enough sample size can fail to find statistically significant results due to insufficient statistical power (Card et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 122,
                        "end": 143,
                        "text": "(Lewis et al., 2020a;",
                        "ref_id": null
                    },
                    {
                        "start": 144,
                        "end": 164,
                        "text": "Zhang et al., 2020a;",
                        "ref_id": null
                    },
                    {
                        "start": 165,
                        "end": 184,
                        "text": "Brown et al., 2020;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 185,
                        "end": 203,
                        "text": "Sanh et al., 2022;",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 204,
                        "end": 220,
                        "text": "He et al., 2022)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 350,
                        "end": 373,
                        "text": "(Bhandari et al., 2020;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 374,
                        "end": 395,
                        "text": "Fabbri et al., 2022a;",
                        "ref_id": null
                    },
                    {
                        "start": 396,
                        "end": 414,
                        "text": "Gao and Wan, 2022)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 563,
                        "end": 582,
                        "text": "Clark et al. (2021)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 756,
                        "end": 776,
                        "text": "(Goyal et al., 2022)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 883,
                        "end": 905,
                        "text": "(Fabbri et al., 2022a)",
                        "ref_id": null
                    },
                    {
                        "start": 1063,
                        "end": 1082,
                        "text": "(Card et al., 2020)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Therefore, we believe it is important to ensure that human evaluation can indeed serve as a solid foundation for evaluating summarization systems and automatic metrics. For this, we propose using a robust human evaluation protocol for evaluating the salience of summaries that is more objective by dissecting the summaries into finegrained content units and defining the annotation task based on those units. Specifically, we introduce the Atomic Content Unit (ACU) protocol for summary salience evaluation ( \u00a73), which is modified from the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols. We demonstrate that with the ACU protocol, a high inter-annotator agreement can be established among crowd-workers, which leads to more stable system evaluation results and better reproducibility.",
                "cite_spans": [
                    {
                        "start": 549,
                        "end": 579,
                        "text": "(Nenkova and Passonneau, 2004)",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 596,
                        "end": 618,
                        "text": "(Shapira et al., 2019)",
                        "ref_id": "BIBREF72"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We then collect, through both in-house annotation and crowdsourcing, RoSE, a large human evaluation benchmark of human-annotated summaries with the ACU evaluation protocol on recent state-of-the-art summarization systems, which yields higher statistical power ( \u00a74). To support evaluation across datasets and domains, our benchmark consists of test sets over three summarization datasets, CNN/DailyMail (CNNDM) (Nalla- pati et al., 2016) , XSum (Narayan et al., 2018) , and SamSum (Gliwa et al., 2019) , and annotations on the validation set of CNNDM to facilitate automatic metric training. To gain further insights into the characteristics of different evaluation protocols, we conduct human evaluation with three other protocols ( \u00a75). Specifically, we analyze protocol differences in the context of both fine-tuned models and large language models (LLMs) in a zero-shot setting such as GPT-3 (Brown et al., 2020) . We find that different protocols can lead to drastically different results, which can be affected by annotators' prior preferences, highlighting the importance of aligning the protocol with the summary quality intended to be evaluated. We note that our benchmark enables a more trustworthy evaluation of automatic metrics ( \u00a76), as shown by statistical characteristics such as tighter confidence intervals and more statistically significant comparisons ( \u00a76.2). Our evaluation includes recent methods based on LLMs (Fu et al., 2023; Liu et al., 2023) , and we found that they cannot outperform traditional metrics despite their successes on related benchmarks such as SummEval (Fabbri et al., 2022a) .",
                "cite_spans": [
                    {
                        "start": 419,
                        "end": 437,
                        "text": "pati et al., 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 445,
                        "end": 467,
                        "text": "(Narayan et al., 2018)",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 481,
                        "end": 501,
                        "text": "(Gliwa et al., 2019)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 890,
                        "end": 916,
                        "text": "GPT-3 (Brown et al., 2020)",
                        "ref_id": null
                    },
                    {
                        "start": 1434,
                        "end": 1451,
                        "text": "(Fu et al., 2023;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 1452,
                        "end": 1469,
                        "text": "Liu et al., 2023)",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 1596,
                        "end": 1618,
                        "text": "(Fabbri et al., 2022a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We summarize our key findings in Tab. 1. Our contributions are the following: (1) We propose the ACU protocol for high-agreement human evaluation of summary salience. (2) We curate the RoSE benchmark, consisting of 22000 summary-level annotations and requiring over 150 hours of in-house annotation, across three summarization datasets, which can lay a solid foundation for training and evaluating automatic metrics. 1 (3) We compare four human evaluation protocols for summarization and show how they can lead to drastically different model preferences. (4) We evaluate automatic metrics across different human evaluation protocols and call for human evaluation to be conducted with a clear evaluation target aligned with the evaluated systems or metrics, such that task-specific qualities can be evaluated without the impact of general, input-agnostic preferences of annotators. We note that the implications of our findings can become even more critical with the progress of LLMs trained with human preference feedback (Ouyang et al., 2022) and call for a more rigorous human evaluation of LLM performance.",
                "cite_spans": [
                    {
                        "start": 1022,
                        "end": 1043,
                        "text": "(Ouyang et al., 2022)",
                        "ref_id": "BIBREF61"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Human Evaluation Benchmarks Human annotations are essential to the analysis of summarization research progress. Thus, recent efforts have focused on aggregating model outputs and annotating them according to specific quality dimensions (Huang et al., 2020; Bhandari et al., 2020; Stiennon et al., 2020; Zhang and Bansal, 2021; Fabbri et al., 2022a; Gao and Wan, 2022) . The most relevant work to ours is Bhandari et al. (2020) , which annotates summaries according to semantic content units, motivated by the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols. However, this benchmark only covers a single dataset (CNNDM) without a focus on similarly-performing state-of-the-art systems, which may skew metric analysis (Tang et al., 2022a) and not fully reflect realistic scenarios (Deutsch et al., 2022) . In contrast, our benchmark consists only of outputs from recently-introduced models over three datasets.",
                "cite_spans": [
                    {
                        "start": 236,
                        "end": 256,
                        "text": "(Huang et al., 2020;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 257,
                        "end": 279,
                        "text": "Bhandari et al., 2020;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 280,
                        "end": 302,
                        "text": "Stiennon et al., 2020;",
                        "ref_id": "BIBREF75"
                    },
                    {
                        "start": 303,
                        "end": 326,
                        "text": "Zhang and Bansal, 2021;",
                        "ref_id": "BIBREF89"
                    },
                    {
                        "start": 327,
                        "end": 348,
                        "text": "Fabbri et al., 2022a;",
                        "ref_id": null
                    },
                    {
                        "start": 349,
                        "end": 367,
                        "text": "Gao and Wan, 2022)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 404,
                        "end": 426,
                        "text": "Bhandari et al. (2020)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 517,
                        "end": 547,
                        "text": "(Nenkova and Passonneau, 2004)",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 564,
                        "end": 586,
                        "text": "(Shapira et al., 2019)",
                        "ref_id": "BIBREF72"
                    },
                    {
                        "start": 756,
                        "end": 776,
                        "text": "(Tang et al., 2022a)",
                        "ref_id": null
                    },
                    {
                        "start": 819,
                        "end": 841,
                        "text": "(Deutsch et al., 2022)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Summarization Meta-Evaluation With a human evaluation dataset, there exist many directions of meta-evaluation, or re-evaluation of the current state of evaluation, such as metric performance analyses, understanding model strengths, and human evaluation protocol comparisons.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Within metric meta-analysis, several studies have focused on the analysis of ROUGE (Lin, 2004b) , and its variations (Rankel et al., 2013; Graham, 2015) , across domains such as news (Lin, 2004a) , meeting summarization (Liu and Liu, 2008) , and scientific articles (Cohan and Goharian, 2016) . Other studies analyze a broader set of metrics (Peyrard, 2019; Bhandari et al., 2020; Deutsch and Roth, 2020; Fabbri et al., 2022a; Gabriel et al., 2021; Kasai et al., 2022b) , including those specific to factual consistency evaluation (Kryscinski et al., 2020; Durmus et al., 2020; Wang et al., 2020; Maynez et al., 2020; Laban et al., 20d; Fabbri et al., 2022b; Honovich et al., 2022; Tam et al., 2022) .",
                "cite_spans": [
                    {
                        "start": 83,
                        "end": 95,
                        "text": "(Lin, 2004b)",
                        "ref_id": null
                    },
                    {
                        "start": 117,
                        "end": 138,
                        "text": "(Rankel et al., 2013;",
                        "ref_id": "BIBREF67"
                    },
                    {
                        "start": 139,
                        "end": 152,
                        "text": "Graham, 2015)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 183,
                        "end": 195,
                        "text": "(Lin, 2004a)",
                        "ref_id": null
                    },
                    {
                        "start": 220,
                        "end": 239,
                        "text": "(Liu and Liu, 2008)",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 266,
                        "end": 292,
                        "text": "(Cohan and Goharian, 2016)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 342,
                        "end": 357,
                        "text": "(Peyrard, 2019;",
                        "ref_id": "BIBREF65"
                    },
                    {
                        "start": 358,
                        "end": 380,
                        "text": "Bhandari et al., 2020;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 381,
                        "end": 404,
                        "text": "Deutsch and Roth, 2020;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 405,
                        "end": 426,
                        "text": "Fabbri et al., 2022a;",
                        "ref_id": null
                    },
                    {
                        "start": 427,
                        "end": 448,
                        "text": "Gabriel et al., 2021;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 449,
                        "end": 469,
                        "text": "Kasai et al., 2022b)",
                        "ref_id": null
                    },
                    {
                        "start": 531,
                        "end": 556,
                        "text": "(Kryscinski et al., 2020;",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 557,
                        "end": 577,
                        "text": "Durmus et al., 2020;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 578,
                        "end": 596,
                        "text": "Wang et al., 2020;",
                        "ref_id": "BIBREF83"
                    },
                    {
                        "start": 597,
                        "end": 617,
                        "text": "Maynez et al., 2020;",
                        "ref_id": "BIBREF55"
                    },
                    {
                        "start": 618,
                        "end": 636,
                        "text": "Laban et al., 20d;",
                        "ref_id": null
                    },
                    {
                        "start": 637,
                        "end": 658,
                        "text": "Fabbri et al., 2022b;",
                        "ref_id": null
                    },
                    {
                        "start": 659,
                        "end": 681,
                        "text": "Honovich et al., 2022;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 682,
                        "end": 699,
                        "text": "Tam et al., 2022)",
                        "ref_id": "BIBREF77"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Regarding re-evaluating model performance, a recent line of work has focused on evaluating zeroshot large language models (Goyal et al., 2022; Liang et al., 2022; Tam et al., 2022) , noting their high performance compared to smaller models.",
                "cite_spans": [
                    {
                        "start": 122,
                        "end": 142,
                        "text": "(Goyal et al., 2022;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 143,
                        "end": 162,
                        "text": "Liang et al., 2022;",
                        "ref_id": null
                    },
                    {
                        "start": 163,
                        "end": 180,
                        "text": "Tam et al., 2022)",
                        "ref_id": "BIBREF77"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "As for the further understanding of human evaluation, prior work has compared approaches to human evaluation (Hardy et al., 2019) , studied annotation protocols for quality dimensions such as linguistic quality (Steen and Markert, 2021) and factual consistency (Tang et al., 2022b) , and noted the effects of human annotation inconsistencies on system rankings (Owczarzak et al., 2012) . The unreliability and cost of human evaluation in certain settings have been emphasized (Chaganty et al., 2018; Clark et al., 2021) , with some work noting that thousands of costly data points may need to be collected in order to draw statistically significant conclusions (Wei and Jia, 2021) . Our meta-analysis focuses on this latter aspect, and we further analyze potential confounding factors in evaluation such as length and protocol design, with respect to both small and large zero-shot language models.",
                "cite_spans": [
                    {
                        "start": 109,
                        "end": 129,
                        "text": "(Hardy et al., 2019)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 211,
                        "end": 236,
                        "text": "(Steen and Markert, 2021)",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 261,
                        "end": 281,
                        "text": "(Tang et al., 2022b)",
                        "ref_id": null
                    },
                    {
                        "start": 361,
                        "end": 385,
                        "text": "(Owczarzak et al., 2012)",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 476,
                        "end": 499,
                        "text": "(Chaganty et al., 2018;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 500,
                        "end": 519,
                        "text": "Clark et al., 2021)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 661,
                        "end": 680,
                        "text": "(Wei and Jia, 2021)",
                        "ref_id": "BIBREF84"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "We now describe our Atomic Content Unit (ACU) annotation protocol for reference-based summary salience evaluation, including the procedure of writing ACUs based on reference summaries and matching the written ACUs with system outputs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Atomic Content Units for Summarization Evaluation",
                "sec_num": "3"
            },
            {
                "text": "In this work, we focus on a specific summarization meta-evaluation study on summary salience. Salience is a desired summary quality that requires the summary to include all and only important information of the input article. The human evaluation of summary salience can be conducted in either reference-free or reference-based manners. The former asks the annotators to assess the summary directly based on the input article (Fabbri et al., 2022a) , while the latter requires the annotators to assess the information overlap between the system output and reference summary (Bhandari et al., 2020) , under the assumption that the reference summary is the gold standard of summary salience.foot_1 Given that reference-based protocols are more constrained, we focus on reference-based evaluation for our human judgment dataset collection, and we conduct a comparison of protocols in \u00a75.",
                "cite_spans": [
                    {
                        "start": 426,
                        "end": 448,
                        "text": "(Fabbri et al., 2022a)",
                        "ref_id": null
                    },
                    {
                        "start": 574,
                        "end": 597,
                        "text": "(Bhandari et al., 2020)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Preliminaries",
                "sec_num": "3.1"
            },
            {
                "text": "Inspired by the Pyramid (Nenkova and Passonneau, 2004) and LitePyramid (Shapira et al., 2019) protocols and subsequent annotation collection efforts (Bhandari et al., 2020; Zhang and Bansal, 2021) , the ACU protocol is designed to reduce the subjectivity of reference-based human evaluation by simplifying the basic annotation unit -the annotators only need to decide on the presence of a single fact, extracted from one text sequence, in another text sequence, to which a binary label can be assigned with more objectivity. Specifically, the evaluation process is decomposed into two steps: (1) ACU Writing -extracting facts from one text sequence, and (2) ACU Matching -checking for the presence of the extracted facts in another sequence. We formulate the ACU protocol as a recall-based protocol, such that the first step only needs to be performed once for the reference summary, allowing for reproducibility and reuse of these units when performing matching on new system outputs. ACU Writing While the LitePyramid approach defines its basic content unit as a sentence containing a brief fact, we follow Bhandari et al. (2020) to emphasize a shorter, more fine-grained information unit. Specifically, we define the ACU protocol with the concept of atomic facts -elementary information units in the reference summaries, which no longer need to be further split for the purpose of reducing ambiguity in human evaluation. 3 Then, ACUs are constructed based on one atomic fact and other minimal, necessary information. Fig. 1 shows an example of the written ACUs. To ensure annotation quality, we (the authors) write all the ACUs used in this work. We define guidelines to standardize the annotation process; for each summary sentence the annotator creates an ACU constituting the main information from the subject of the main clause (e.g., root), followed by additional ACUs for other facts while including the minimal necessary information from the root. We provide rules for dealing with quotations, extraneous adjectives, noisy summaries, and additional cases. We note that there can still be inherent subjectivity in the written ACUs among different annotators even with the provided guidelines. However, such subjectivity should be unbiased in summary comparison because all the candidate summaries are evaluated by the same set of written ACUs. ACU Matching Given ACUs written for a set of reference summaries, our protocol evaluates summarization system performance by checking the presence of the ACUs in the system-generated summaries as illustrated in Fig. 1 . For this step, we recruit annotators on Amazon Mechanical Turkfoot_3 (MTurk). The annotators must pass a qualification test, and additional requirements are specified in Appendix A. Besides displaying the ACUs and the system outputs, we also provide the reference summaries to be used as context for the ACUs. Scoring Summaries with ACU ACU matching annotations can be aggregated into summary scores. We first define an un-normalized ACU score f of a candidate summary s given a set of ACUs A as: where A s is a subset of A that is matched with s.",
                "cite_spans": [
                    {
                        "start": 24,
                        "end": 54,
                        "text": "(Nenkova and Passonneau, 2004)",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 71,
                        "end": 93,
                        "text": "(Shapira et al., 2019)",
                        "ref_id": "BIBREF72"
                    },
                    {
                        "start": 149,
                        "end": 172,
                        "text": "(Bhandari et al., 2020;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 173,
                        "end": 196,
                        "text": "Zhang and Bansal, 2021)",
                        "ref_id": "BIBREF89"
                    },
                    {
                        "start": 1109,
                        "end": 1131,
                        "text": "Bhandari et al. (2020)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1525,
                        "end": 1526,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 2569,
                        "end": 2570,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "ACU Annotation Protocol",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "f (s, A) = |As| |A| ,",
                        "eq_num": "(1)"
                    }
                ],
                "section": "ACU Annotation Protocol",
                "sec_num": "3.2"
            },
            {
                "text": "We note that f by default is a recall based score with respect to the reference summary r. Therefore, we also define a normalized ACU score f as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "ACU Annotation Protocol",
                "sec_num": "3.2"
            },
            {
                "text": "f\u03b1(s, A, r) = e min (0,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "ACU Annotation Protocol",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "1- |s| |r| \u03b1 ) f (s, A),",
                        "eq_num": "(2)"
                    }
                ],
                "section": "ACU Annotation Protocol",
                "sec_num": "3.2"
            },
            {
                "text": "where |s|, |r| are the length (i.e., number of words) of the candidate summary s and the reference summary r respectively, and \u03b1 is a positive number controlling the strength of the normalization. This normalization is in effect a redundancy penalty, which penalizes the summaries longer than the reference and resembles the brevity penalty in BLEU (Papineni et al., 2002) . In practice, we set the value of \u03b1 by de-correlating f with the summary length using the collected ACU annotations.",
                "cite_spans": [
                    {
                        "start": 349,
                        "end": 372,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF64"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "ACU Annotation Protocol",
                "sec_num": "3.2"
            },
            {
                "text": "We collect ACU annotations on three summarization datasets: CNNDM (Nallapati et al., 2016) , XSum (Narayan et al., 2018) , and SamSum (Gliwa et al., 2019) . To reflect the latest progress in text summarization, we collect and annotate the generated summaries of pre-trained summarization systems proposed in recent years. 5 Detailed informa-tion about the summarization systems we used can be found in Appendix A.2. Table 2 shows the statistics of the collected annotations. The annotations are collected from the test set of the above datasets, and additionally from the validation set of CNNDM to facilitate the training of automatic evaluation metrics. In total, we collect around 21.8k ACU-level annotations and around 22k summary-level annotations, aggregated over around 50k individual summary-level judgments.",
                "cite_spans": [
                    {
                        "start": 66,
                        "end": 90,
                        "text": "(Nallapati et al., 2016)",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 98,
                        "end": 120,
                        "text": "(Narayan et al., 2018)",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 134,
                        "end": 154,
                        "text": "(Gliwa et al., 2019)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 422,
                        "end": 423,
                        "text": "2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "ACU Annotation Collection",
                "sec_num": "3.3"
            },
            {
                "text": "To calculate inter-annotator agreement, we use Krippendorff's alpha (Krippendorff, 2011) . The aggregated summary-level agreement score of ACU matching is 0.7571, and the ACU-level agreement score is 0.7528. These agreement scores are higher than prior collections, such as RealSumm (Bhandari et al., 2020) and SummEval (Fabbri et al., 2022a) , which have an average agreement score of crowd-workers 0.66 and 0.49, respectively.",
                "cite_spans": [
                    {
                        "start": 68,
                        "end": 88,
                        "text": "(Krippendorff, 2011)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 283,
                        "end": 306,
                        "text": "(Bhandari et al., 2020)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 320,
                        "end": 342,
                        "text": "(Fabbri et al., 2022a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "ACU Annotation Collection",
                "sec_num": "3.3"
            },
            {
                "text": "We first analyze the robustness of our collected annotations and a case study on the system outputs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RoSE Benchmark Analysis",
                "sec_num": "4"
            },
            {
                "text": "We analyze the statistical power of our collected human annotations to study whether it can yield stable and trustworthy results (Card et al., 2020) . Statistical power is the probability that the null hypothesis of a statistical significance test is rejected given there is a real effect. For example, for a human evaluation study that compares the performance of two genuinely different systems, a statistical power of 0.80 means there is an 80% chance that a significant difference will be observed. Further details can be found in Appendix B.1.",
                "cite_spans": [
                    {
                        "start": 129,
                        "end": 148,
                        "text": "(Card et al., 2020)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Power Analysis",
                "sec_num": "4.1"
            },
            {
                "text": "We conduct the power analysis for pair-wise system comparisons with ACU scores (Eq. 1) focusing on two factors, the number of test examples and the observed system difference. Specifically, we run the power analysis with varying sample sizes, and group the system pairs into buckets according to their performance difference, as determined by ROUGE1 recall scores (Fig. 2 ). 6 We observe the following: (1) A high statistical power 7 is difficult to reach when the system performance is similar. 6 We note that these scores are proxies of the true system differences, and the power analysis is based on the assumption that the systems have significantly different performance. 7 An experiment is usually considered sufficiently powered if the statistical power is over 0.80. Notably, while the sample size of the human evaluation performed in recent work is typically around 50-100,foot_5 such sample size can only reach a power of 0.80 when the ROUGE1 recall score difference is above 5. (2) Increasing the sample size can effectively raise the statistical power. For example, when the system performance difference is within the range of 1-2 points, the power of a 500-sample set is around 0.50 while a 100-sample set only has a power of around 0.20. The results of power analysis on three datasets with both ROUGE and ACU score differences are provided in Appendix B.2 with the same patterns, which indicates that our dataset can provide more stable summarization system evaluation thanks to its higher statistical power.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 370,
                        "end": 371,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Power Analysis",
                "sec_num": "4.1"
            },
            {
                "text": "As a case study, in Tab. GSum (Dou et al., 2021) 44.47 34.87 77.61 45.47",
                "cite_spans": [
                    {
                        "start": 30,
                        "end": 48,
                        "text": "(Dou et al., 2021)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summarization System Analysis",
                "sec_num": "4.2"
            },
            {
                "text": "MatchSum (Zhong et al., 2020) 42.50 33.69 74.99 43.84 BRIO-Ext (Liu et al., 2022) 41.72 33.58 73.67 44.44",
                "cite_spans": [
                    {
                        "start": 9,
                        "end": 29,
                        "text": "(Zhong et al., 2020)",
                        "ref_id": "BIBREF93"
                    },
                    {
                        "start": 63,
                        "end": 81,
                        "text": "(Liu et al., 2022)",
                        "ref_id": "BIBREF51"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summarization System Analysis",
                "sec_num": "4.2"
            },
            {
                "text": "BART (Lewis et al., 2020a) 38.83 32.34 71.00 44.04",
                "cite_spans": [
                    {
                        "start": 5,
                        "end": 26,
                        "text": "(Lewis et al., 2020a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summarization System Analysis",
                "sec_num": "4.2"
            },
            {
                "text": "CTRLSum (He et al., 2020) 44.58 36.13 70.56 45.69 BRIO (Liu et al., 2022) 44.03 37.20 69.58 47.83",
                "cite_spans": [
                    {
                        "start": 8,
                        "end": 25,
                        "text": "(He et al., 2020)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 55,
                        "end": 73,
                        "text": "(Liu et al., 2022)",
                        "ref_id": "BIBREF51"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summarization System Analysis",
                "sec_num": "4.2"
            },
            {
                "text": "CLIFF (Cao and Wang, 2021) 38.51 32.96 67.74 44.19 PEGASUS (Zhang et al., 2020a) 37.56 32.03 65.65 43.80 SimCLS (Liu and Liu, 2021) 40.47 36.01 62.91 46.46 FROST (Narayan et al., 20d) 38. 44 33.68 62.65 44.90 GOLD (Pang and He, 2021) 38.10 33.80 60.65 44.86 GLOBAL (Ma et al., 2021) 36.40 34.07 55.50 45.17 summaries. Meanwhile, the systems that generate longer summaries may be favored by users who prefer more informative summaries. Therefore, we join the previous work (Sun et al., 2019; Song et al., 2021; Gehrmann et al., 2022; Goyal et al., 2022) in advocating treating summary lengths as a separate aspect of summary quality in evaluation, as in earlier work in summarization research.foot_6 ",
                "cite_spans": [
                    {
                        "start": 6,
                        "end": 26,
                        "text": "(Cao and Wang, 2021)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 59,
                        "end": 80,
                        "text": "(Zhang et al., 2020a)",
                        "ref_id": null
                    },
                    {
                        "start": 81,
                        "end": 104,
                        "text": "37.56 32.03 65.65 43.80",
                        "ref_id": null
                    },
                    {
                        "start": 112,
                        "end": 131,
                        "text": "(Liu and Liu, 2021)",
                        "ref_id": null
                    },
                    {
                        "start": 162,
                        "end": 178,
                        "text": "(Narayan et al.,",
                        "ref_id": null
                    },
                    {
                        "start": 179,
                        "end": 183,
                        "text": "20d)",
                        "ref_id": null
                    },
                    {
                        "start": 188,
                        "end": 233,
                        "text": "44 33.68 62.65 44.90 GOLD (Pang and He, 2021)",
                        "ref_id": null
                    },
                    {
                        "start": 265,
                        "end": 282,
                        "text": "(Ma et al., 2021)",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 472,
                        "end": 490,
                        "text": "(Sun et al., 2019;",
                        "ref_id": "BIBREF76"
                    },
                    {
                        "start": 491,
                        "end": 509,
                        "text": "Song et al., 2021;",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 510,
                        "end": 532,
                        "text": "Gehrmann et al., 2022;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 533,
                        "end": 552,
                        "text": "Goyal et al., 2022)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summarization System Analysis",
                "sec_num": "4.2"
            },
            {
                "text": "Apart from ACU annotations, we collect human annotations with three different protocols to better understand their characteristics. Specifically, two reference-free protocols are investigated: Prior protocol evaluates the annotators' preferences of summaries without the input document, while Ref-free protocol evaluates if summaries cover the salient information of the input document. We also consider one reference-based protocol, Ref-based, which evaluates the content similarity between the generated and reference summaries. Appendix D.1 provides detailed instructions for each protocol.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Annotation Protocols",
                "sec_num": "5"
            },
            {
                "text": "We collected three annotations per summary on a 100-example subset of the above CNNDM test set using the same pool of workers from our ACU qualification. Except for ACU, all of the summaries from different systems are evaluated within a single task with a score from 1 (worst) to 5 (best), similar to the EASL protocol (Sakaguchi and Van Durme, 2018) . We collect (1) annotations of the 12 above systems, with an inter-annotator agreement (Krippendorff's alpha) of 0.3455, 0.2201, 0.2741 on Prior, Ref-free, Ref-based protocols respectively;",
                "cite_spans": [
                    {
                        "start": 319,
                        "end": 350,
                        "text": "(Sakaguchi and Van Durme, 2018)",
                        "ref_id": "BIBREF68"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Annotation Collection",
                "sec_num": "5.1"
            },
            {
                "text": "(2) annotations for summaries from GPT-3 (Brown et al., 2020), 10 T0 (Sanh et al., 2022) , BRIO, and BART to better understand annotation protocols with respect to recently introduced large language models applied to zero-shot summarization.",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 88,
                        "text": "(Sanh et al., 2022)",
                        "ref_id": "BIBREF69"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Annotation Collection",
                "sec_num": "5.1"
            },
            {
                "text": "We investigate both the summary-level and systemlevel correlations of evaluation results of different protocols to study their inherent similarity. Details of correlation calculation are in Appendix C.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "Results on Fine-tuned Models We show the system-level protocol correlation when evaluating the fine-tuned models in Tab. 4, and the summarylevel correlation can be found in Appendix D.2. We use the normalized ACU score (Eq. 2) because the other evaluation protocols are supposed to resemble an F1 score, while the ACU score is by definition recall-based. We have the following observations:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "(1) The Ref-free protocol has a strong correlation with the Prior protocol, suggesting that the latter may have a large impact on the annotator's document-based judgments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "(2) Both the Prior and Ref-free protocols have a strong correlation with summary length, showing that annotators may favor longer summaries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "(3) The Ref-free protocol and the Ref-based protocol have a negative correlation while ideally they are supposed to measure similar quality aspects.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "We perform power analysis on the results following the procedure in \u00a74.1 and found that ACU protocol can yield higher statistical power than the Ref-based protocol, suggesting that the ACU protocol leads to more robust evaluation results. We also found that the reference-free Prior and Ref-free protocols have higher power than the referencebased protocols. However, we note that they are not directly comparable because they have different underlying evaluation targets, as shown by the near-zero correlation between them. Further details are provided in Appendix D.2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "Results on Large Language Models The results are shown in Tab. 5. Apart from the system outputs, we also annotate reference summaries for referencefree protocols. We found that under the Ref-free protocol, GPT-3 receives the highest score while the reference summary is the least favorite one, similar to the findings of recent work (Goyal et al., 2022; Liang et al., 2022) . However, we found the same pattern with the Prior protocol, showing that the annotators have a prior preference for GPT-3.",
                "cite_spans": [
                    {
                        "start": 333,
                        "end": 353,
                        "text": "(Goyal et al., 2022;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 354,
                        "end": 373,
                        "text": "Liang et al., 2022)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "We provide an example in Appendix D.2 comparing GPT-3 and BRIO summaries under different protocols. Given the strong correlation between the Prior and Ref-free protocols, we note that there is a risk that the annotators' decisions are affected by their prior preferences that are not genuinely related to the task requirement. As a further investigation, we conduct an annotator-based case study including 4 annotators who annotated around 20 examples in this task, in which we compare two summary-level correlations (Eq. 3) given a specific annotator: (1) the correlation between their own Ref-free protocol scores and Prior scores;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "(2) the correlation between their Ref-free scores and the Ref-free scores averaged over the other annotations on each example. We found that the average value of the former is 0.404 while the latter is only 0.188, suggesting that the annotators' own Prior score is a better prediction of their Ref-free score than the Ref-free score of other annotators.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "We analyze several representative automatic metrics, with additional results in Appendix E on 50 Table 6 : The Kendall's correlation between the automatic metric scores and ACU scores of system outputs on CNNDM, XSum, and SamSum datasets. The correlation is calculated at both the system level and the summary level. We use the recall score of the automatic metrics when available to align with the ACU scores.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 103,
                        "end": 104,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Automatic Metrics",
                "sec_num": "6"
            },
            {
                "text": "automatic metric variants. We focus the metric evaluation on ACU annotations because of two insights from \u00a75: (1) Reference-based metrics should be evaluated with reference-based human evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Automatic Metrics",
                "sec_num": "6"
            },
            {
                "text": "(2) ACU protocol provides higher statistical power than the summary-level Ref-based protocol.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Automatic Metrics",
                "sec_num": "6"
            },
            {
                "text": "We use the correlations between automatic metric scores and ACU annotation scores of system outputs to analyze and compare automatic metric performance. The following metrics are evaluated:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Evaluation with ACU Annotations",
                "sec_num": "6.1"
            },
            {
                "text": "(1) lexical overlap based metrics, ROUGE (Lin, 2004b) , METEOR (Lavie and Agarwal, 2007) , CHRF (Popovi\u0107, 2015) ; (2) pre-trained language model based metrics, BERTScore (Zhang et al., 2020c) , BARTScore (Yuan et al., 2021) ;",
                "cite_spans": [
                    {
                        "start": 41,
                        "end": 53,
                        "text": "(Lin, 2004b)",
                        "ref_id": null
                    },
                    {
                        "start": 63,
                        "end": 88,
                        "text": "(Lavie and Agarwal, 2007)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 96,
                        "end": 111,
                        "text": "(Popovi\u0107, 2015)",
                        "ref_id": "BIBREF66"
                    },
                    {
                        "start": 170,
                        "end": 191,
                        "text": "(Zhang et al., 2020c)",
                        "ref_id": null
                    },
                    {
                        "start": 204,
                        "end": 223,
                        "text": "(Yuan et al., 2021)",
                        "ref_id": "BIBREF86"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Evaluation with ACU Annotations",
                "sec_num": "6.1"
            },
            {
                "text": "(3) question-answering based metrics, Sum-maQA (Scialom et al., 2019) , QAEval (Deutsch et al., 2021a) ; (4) Lite 3 Pyramid (Zhang and Bansal, 2021), which automates the LitePyramid evaluation process; (5) evaluation methods based on large language models, GPTScore (Fu et al., 2023) and G-Eval (Liu et al., 2023) , with two variants that are based on GPT-3.5 11 (G-Eval-3.5) and GPT-4 12 (OpenAI, 2023) (G-Eval-4) respectively. We note that for LLM-based evaluation we require the metric to calculate the recall score. For G-11 OpenAI's gpt-3.5-turbo-0301: https://platform. openai.com/docs/models/gpt-3-5.",
                "cite_spans": [
                    {
                        "start": 47,
                        "end": 69,
                        "text": "(Scialom et al., 2019)",
                        "ref_id": "BIBREF71"
                    },
                    {
                        "start": 79,
                        "end": 102,
                        "text": "(Deutsch et al., 2021a)",
                        "ref_id": null
                    },
                    {
                        "start": 266,
                        "end": 283,
                        "text": "(Fu et al., 2023)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 295,
                        "end": 313,
                        "text": "(Liu et al., 2023)",
                        "ref_id": "BIBREF48"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Evaluation with ACU Annotations",
                "sec_num": "6.1"
            },
            {
                "text": "12 OpenAI's gpt-4-0314: https://platform.openai. com/docs/models/gpt-4. Eval-3.5 we report two variants that are based on greedy decoding (G-Eval-3.5) and sampling (G-Eval-3.5-S) respectively, Details of the LLM-based evaluation are in Appendix E.2. Tab. 6 shows the results, with additional results of more metrics in Appendix E.3. We note:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Evaluation with ACU Annotations",
                "sec_num": "6.1"
            },
            {
                "text": "(1) Several automatic metrics from the different families of methods (e.g., ROUGE, BARTScore) are all able to achieve a relatively high correlation with the ACU scores, especially at the system level.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Evaluation with ACU Annotations",
                "sec_num": "6.1"
            },
            {
                "text": "(2) Metric performance varies across different datasets. In particular, metrics tend to have stronger correlations on the SamSum dataset and weaker correlations on the XSum dataset. We hypothesize that one reason is that the reference summaries of the XSum dataset contain more complex structures.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Evaluation with ACU Annotations",
                "sec_num": "6.1"
            },
            {
                "text": "(3) Despite their successes (Fu et al., 2023; Liu et al., 2023) in other human evaluation benchmarks such as SummEval, LLM-based automatic evaluation cannot outperform traditional methods such as ROUGE on RoSE. Moreover, their low summarylevel correlation with ACU scores suggests that their predicted scores may not be well-calibrated.",
                "cite_spans": [
                    {
                        "start": 28,
                        "end": 45,
                        "text": "(Fu et al., 2023;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 46,
                        "end": 63,
                        "text": "Liu et al., 2023)",
                        "ref_id": "BIBREF48"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Evaluation with ACU Annotations",
                "sec_num": "6.1"
            },
            {
                "text": "Following Deutsch et al. (2022) , we further investigate metric performance when evaluating system pairs with varying performance differences. Specifically, we group the system pairs based on the difference of their ACU scores into different buckets and calculate the modified Kendall's correlation (Deutsch et al., 2022) on each bucket. The system pairs in each bucket are provided in Appendix E.4. Tab. 7 shows that the automatic metrics generally perform worse when they are used to evaluate similar-performing systems. ",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 31,
                        "text": "Deutsch et al. (2022)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 299,
                        "end": 321,
                        "text": "(Deutsch et al., 2022)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Metric Evaluation with ACU Annotations",
                "sec_num": "6.1"
            },
            {
                "text": "We analyze the metric evaluation with respect to the statistical characteristics and the impact of different human evaluation protocols on metric evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis of Metric Evaluation",
                "sec_num": "6.2"
            },
            {
                "text": "Confidence Interval We select several representative automatic metrics and calculate the confidence intervals of their system-level correlations with the ACU scores using bootstrapping. Similar to Deutsch et al. (2021b) , we find that the confidence intervals are large. However, we found that having a larger sample size can effectively reduce the confidence interval, which further shows the importance of increasing the statistical power of the human evaluation dataset as discussed in \u00a74.1. We provide further details in Appendix E.5.",
                "cite_spans": [
                    {
                        "start": 197,
                        "end": 219,
                        "text": "Deutsch et al. (2021b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis of Metric Evaluation",
                "sec_num": "6.2"
            },
            {
                "text": "We conduct a power analysis of pair-wise metric comparison with around 200 pairs, which corresponds to the chance of a statistical significance result being found. More details can be found in Appendix E.6. The results are in Fig. 3 , showing similar patterns as in the power analysis of summarization system comparison ( \u00a74.1):",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 231,
                        "end": 232,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Power Analysis of Metric Comparison",
                "sec_num": null
            },
            {
                "text": "(1) Significant results are difficult to find when the metric performance is similar;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Power Analysis of Metric Comparison",
                "sec_num": null
            },
            {
                "text": "(2) Increasing the sample size can effectively increase the chance of finding significant results. automatic metrics generally perform better under reference-based evaluation protocols, but can have negative correlations with reference-free protocols.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Power Analysis of Metric Comparison",
                "sec_num": null
            },
            {
                "text": "We introduce RoSE, a benchmark whose underlying protocol and scale allow for more robust summarization evaluation across three datasets. With our benchmark, we re-evaluate the current state of human evaluation and its implications for both summarization system and automatic metric development, and we suggest the following:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Implications",
                "sec_num": "7"
            },
            {
                "text": "(1) Alignment in metric evaluation. To evaluate automatic metrics, it is important to use an appropriate human evaluation protocol that captures the intended quality dimension to be measured. For example, reference-based automatic metrics should be evaluated by reference-based human evaluation, which disentangles metric performance from the impact of reference summaries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Implications",
                "sec_num": "7"
            },
            {
                "text": "(2) Alignment in system evaluation. We advocate for targeted evaluation, which clearly defines the intended evaluation quality. Specifically, text summarization, as a conditional generation task, should be defined by both the source and target texts along with pre-specified, desired characteristics. Clearly specifying characteristics to be measured can lead to more reliable and objective evaluation results. This will be even more important for LLMs pretrained with human preference feedback for disentangling annotators' prior preferences for LLMs with the task-specific summary quality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Implications",
                "sec_num": "7"
            },
            {
                "text": "(3) Alignment between NLP datasets and tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Implications",
                "sec_num": "7"
            },
            {
                "text": "Human judgments for summary quality can be diverse and affected by various factors such as summary lengths, and reference summaries are not al-ways favored. Therefore, existing summarization datasets (e.g. CNNDM) should only be used for the appropriate tasks. For example, they can be used to define a summarization task with specific requirements (e.g. maximum summary lengths), and be important for studying reference-based metrics.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Implications",
                "sec_num": "7"
            },
            {
                "text": "Biases may be present in the data annotator as well as in the data the models were pretrained on. Furthermore, we only include English-language data in our benchmark and analysis. Recent work has noted that language models may be susceptible to learning such data biases (Lucy and Bamman, 2021) , thus we request that the users be aware of potential issues in downstream use cases.",
                "cite_spans": [
                    {
                        "start": 271,
                        "end": 294,
                        "text": "(Lucy and Bamman, 2021)",
                        "ref_id": "BIBREF53"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations",
                "sec_num": "8"
            },
            {
                "text": "As described in Appendix D.1, we take measures to ensure a high quality benchmark. There will inevitably be noise in the dataset collection process, either in the ACU writing or matching step, and high agreement of annotations does not necessarily coincide with correctness. However, we believe that the steps taken to spot check ACU writing and filter workers for ACU matching allow us to curate a high-quality benchmark. Furthermore, we encourage the community to analyze and improve RoSE in the spirit of evolving, living benchmarks (Gehrmann et al., 2021) .",
                "cite_spans": [
                    {
                        "start": 536,
                        "end": 559,
                        "text": "(Gehrmann et al., 2021)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations",
                "sec_num": "8"
            },
            {
                "text": "For reference-based evaluation, questions about reference quality arise naturally. We also note that the original Pyramid protocol was designed for multi-reference evaluation and weighting of semantic content units, while we do not weight ACUs during aggregation. As discussed above, we argue that our benchmark and analysis are still valuable given the purpose of studying conditional generation and evaluating automatic metrics for semantic overlap in targeted evaluation. We view the collection of high-quality reference summaries as a valuable, orthogonal direction to this work, and we plan to explore ACU weighting in future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations",
                "sec_num": "8"
            },
            {
                "text": "We discuss the detailed settings of ACU collection in \u00a73.2. To ensure the consistency of written ACUs among different annotators, we require each annotator to be familiar with the annotation protocol and proofread each other's annotations to resolve any differences in initial annotations. After establishing a consistent understanding of the task, we have each reference summary annotated by one annotator. We note that there are multiple valid ways of writing the same atomic fact. In preliminary protocol analysis, we had multiple annotators write ACUs for the same reference summaries and did not find large differences in downstream interannotator agreement for ACU matching. The average time to write ACUs of one summary ranges from 2 to 5 minutes, and the overall annotation time for ACU writing is around 150 hours.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.1 Detailed Settings",
                "sec_num": null
            },
            {
                "text": "We use the following qualifications, in addition to a qualification test, to recruit MTurk workers with good track records: HIT approval rate greater than or equal to 98%, number of HITs approved greater than or equal to 10000, and located in either the United Kingdom or the United States. Workers were compensated between $0.15 and $0.55 per summary-level ACU HITs, with HITs bucketed according to the number of ACUs to be matched. For protocol comparison HITs, workers were compensated between $1 and $3. All HITs were carefully calibrated to equal a $12/hour pay rate.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.1 Detailed Settings",
                "sec_num": null
            },
            {
                "text": "The datasets we used for the collection are CN-NDM, XSum and SamSum. The data release licenses are the Apache License for CNNDM and XSum, and CC BY-NC-ND 4.0 for SamSum. Our collected benchmark will be released under the 3-Clause BSD license.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.1 Detailed Settings",
                "sec_num": null
            },
            {
                "text": "We list the summarization models for ACUs annotations on CNNDM, XSum, and SamSum in \u00a73.3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.2 Summarization Models",
                "sec_num": null
            },
            {
                "text": "BART (Lewis et al., 2020b) introduce a denoising autoencoder for pretraining sequence to sequence tasks which is applicable to both natural language understanding and generation tasks. Pegasus (Zhang et al., 2020b) introduce a model pretrained with a novel objective function designed for summarization by which important sentences are removed from an input document and then generated from the remaining sentences. MatchSum (Zhong et al., 2020) propose a summary-level extractive system using semantic match between the extracted summary and the source document. CTRLSum (He et al., 2020) introduce a method for controllable summarization based on keyword or descriptive prompt control tokens. CLIFF (Cao and Wang, 2021) propose to use contrastive learning to improve factual consistency. We use the CLIFF output that uses an underlying BART model. GOLD (Pang and He, 2021) frames text generation as an offline reinforcement learning problem, using importance weighting and assigning weights to examples that receive a higher probability from the generation model. GSum (Dou et al., 2021) is a framework for incorporating forms of summarization guidance.",
                "cite_spans": [
                    {
                        "start": 5,
                        "end": 26,
                        "text": "(Lewis et al., 2020b)",
                        "ref_id": null
                    },
                    {
                        "start": 193,
                        "end": 214,
                        "text": "(Zhang et al., 2020b)",
                        "ref_id": null
                    },
                    {
                        "start": 425,
                        "end": 445,
                        "text": "(Zhong et al., 2020)",
                        "ref_id": "BIBREF93"
                    },
                    {
                        "start": 572,
                        "end": 589,
                        "text": "(He et al., 2020)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 701,
                        "end": 721,
                        "text": "(Cao and Wang, 2021)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 1071,
                        "end": 1089,
                        "text": "(Dou et al., 2021)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CNNDM Systems:",
                "sec_num": null
            },
            {
                "text": "SimCLS (Liu and Liu, 2021 ) is a two-stage summarization model where candidates from BART are reranking by a RoBERTa (Liu et al., 2019) scoring model trained using contrastive learning. FROST (Narayan et al., 20d) propose to do content planning in both pretraining and finetuning summarization models with plans in the form of entity chains. GLOBAL (Ma et al., 2021) propose a variation of beam search that takes into account the global attention distribution. BRIO (Liu et al., 2022) proposes to train a summarization model both as a token-level generator and an evaluator of sequence candidates through contrastive reranking. BRIO-Ext (Liu et al., 2022 ) uses BRIO's reranker on candidate extractive summaries from Match-Sum.",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 25,
                        "text": "(Liu and Liu, 2021",
                        "ref_id": null
                    },
                    {
                        "start": 117,
                        "end": 135,
                        "text": "(Liu et al., 2019)",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 192,
                        "end": 208,
                        "text": "(Narayan et al.,",
                        "ref_id": null
                    },
                    {
                        "start": 209,
                        "end": 213,
                        "text": "20d)",
                        "ref_id": null
                    },
                    {
                        "start": 349,
                        "end": 366,
                        "text": "(Ma et al., 2021)",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 466,
                        "end": 484,
                        "text": "(Liu et al., 2022)",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 637,
                        "end": 654,
                        "text": "(Liu et al., 2022",
                        "ref_id": "BIBREF51"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CNNDM Systems:",
                "sec_num": null
            },
            {
                "text": "The following models were included in protocolcomparison annotations. T0 (Sanh et al., 2022) introduces a prompt-based model that is fine-tuned on multiple tasks, including summarization. GPT-3 (Brown et al., 2020) is the davinci-002 model trained on human demonstrations and model outputs highly rated by humans. 13XSum Systems For XSum we reuse several of the above models with their XSum-trained checkpoints as well as several variations from the above paper due to the scarcity of widely-available, easilyreproducible XSum outputs. BART (Lewis et al., (Cao and Wang, 2021) 22.09 21.93 20.17 44.52 FROST (Narayan et al., 20d) 27.93 27.77 19.86 47.83 BRIO-Ctr (Liu et al., 2022) 26.42 26.29 19.65 48.06 Table 9 : Summarization system analysis on the XSum dataset. ACU is the ACU score (Eq. 1), nACU is the normalized ACU score (Eq. 2), Len is the average summary length, and R1F is the ROUGE1 F1 score. ACU and nACU are calculated on the 500 annotated examples (the value is multiplied by 100) while Len and R1F are calculated on the entire test set. The systems are sorted by Len. CLIFF P is based on PEGASUS, while CLIFF B is based on BART.",
                "cite_spans": [
                    {
                        "start": 73,
                        "end": 92,
                        "text": "(Sanh et al., 2022)",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 188,
                        "end": 214,
                        "text": "GPT-3 (Brown et al., 2020)",
                        "ref_id": null
                    },
                    {
                        "start": 541,
                        "end": 555,
                        "text": "(Lewis et al.,",
                        "ref_id": null
                    },
                    {
                        "start": 556,
                        "end": 576,
                        "text": "(Cao and Wang, 2021)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 607,
                        "end": 623,
                        "text": "(Narayan et al.,",
                        "ref_id": null
                    },
                    {
                        "start": 624,
                        "end": 628,
                        "text": "20d)",
                        "ref_id": null
                    },
                    {
                        "start": 662,
                        "end": 680,
                        "text": "(Liu et al., 2022)",
                        "ref_id": "BIBREF51"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 711,
                        "end": 712,
                        "text": "9",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "CNNDM Systems:",
                "sec_num": null
            },
            {
                "text": "UniLM (Dong et al., 2019 ) is a model pretrained on unidirection, bidirection, and sequenceto-sequence language modeling tasks.",
                "cite_spans": [
                    {
                        "start": 6,
                        "end": 24,
                        "text": "(Dong et al., 2019",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CNNDM Systems:",
                "sec_num": null
            },
            {
                "text": "Ctrl-DiaSumm (Liu and Chen, 2021) propose controlled generation using named entity plans. (Feng et al., 2021) use DialogGPT (Zhang et al., 2020d) to annotate input dialogues before finetuning.",
                "cite_spans": [
                    {
                        "start": 90,
                        "end": 109,
                        "text": "(Feng et al., 2021)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 124,
                        "end": 145,
                        "text": "(Zhang et al., 2020d)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CNNDM Systems:",
                "sec_num": null
            },
            {
                "text": "CODS (Wu et al., 2021) propose a two-stage generation model that first generates a sketch that is then used as a signal to the second-stage summarizer.",
                "cite_spans": [
                    {
                        "start": 5,
                        "end": 22,
                        "text": "(Wu et al., 2021)",
                        "ref_id": "BIBREF85"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PLM-BART",
                "sec_num": null
            },
            {
                "text": "MV-BART (Chen and Yang, 2020) propose a multiview encoder and a decoder that attends to these conversation views.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PLM-BART",
                "sec_num": null
            },
            {
                "text": "S-BART (Chen and Yang, 2020) encodes utterances as well as action and discourse graphs and introduces a decoder that attends to these different levels of granularity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PLM-BART",
                "sec_num": null
            },
            {
                "text": "We report the ACU scores of the summarization systems we annotated on the XSum and SamSum datasets ( \u00a74.2) in Tab. 9 and Tab. 10, respectively. The results on CNNDM can be found in Tab. 3. For the normalized ACU score (Eq. 2), we set the normalization strength \u03b1 to 2, 5, 0.5, on CNNDM, XSum, SamSum, respectively, by a grid search for de-correlating the summary length and the normalized score at the summary level.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.3 ACU Scores of Summarization Models",
                "sec_num": null
            },
            {
                "text": "ACU nACU Len R1F",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System",
                "sec_num": null
            },
            {
                "text": "MV-BART (Chen and Yang, 2020) 47.65 33.01 23.57 53.80 Ctrl-DiaSumm (Liu and Chen, 2021) 49.05 37.20 22.97 56.33 PLM-BART (Feng et al., 2021) 43.74 32.61 21.43 53.46 S-BART (Chen and Yang, 2020) 34.57 25.95 21.01 50.36 CODS (Wu et al., 2021) 38.40 33.41 20.11 52.48 BART (Lewis et al., 2020b) 42.85 34.05 19.62 52.30 UniLM (Dong et al., 2019) 32.74 26.10 18.84 49.20 PEGASUS (Zhang et al., 2020b) 37.02 31.99 17.32 50.87",
                "cite_spans": [
                    {
                        "start": 8,
                        "end": 29,
                        "text": "(Chen and Yang, 2020)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 67,
                        "end": 87,
                        "text": "(Liu and Chen, 2021)",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 121,
                        "end": 140,
                        "text": "(Feng et al., 2021)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 172,
                        "end": 193,
                        "text": "(Chen and Yang, 2020)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 223,
                        "end": 240,
                        "text": "(Wu et al., 2021)",
                        "ref_id": "BIBREF85"
                    },
                    {
                        "start": 270,
                        "end": 291,
                        "text": "(Lewis et al., 2020b)",
                        "ref_id": null
                    },
                    {
                        "start": 322,
                        "end": 341,
                        "text": "(Dong et al., 2019)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 374,
                        "end": 395,
                        "text": "(Zhang et al., 2020b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System",
                "sec_num": null
            },
            {
                "text": "Table 10: Summarization system analysis on the Sam-Sum dataset. ACU is the ACU score (Eq. 1), nACU is the normalized ACU score (Eq. 2), Len is the average Summary length, and R1F is the ROUGE1 F1 score. ACU and nACU are calculated on the 500 annotated examples (the value is multiplied by 100) while Len and R1F are calculated on the entire test set. The systems are sorted by Len.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "System",
                "sec_num": null
            },
            {
                "text": "We describe the algorithm for the power analysis in \u00a74.1 in Alg.1. While prior work (Card et al., 2020; Wei and Jia, 2021) uses parametric methods to estimate statistical power, we conduct the power analysis with the bootstrapping test (Tibshirani and Efron, 1993) as recent work (Deutsch et al., 2021b) has shown that the assumptions of the parametric methods do not always hold for human evaluation of text summarization. The process involves (1) iteratively sampling a set of examples with a certain sample size from an existing dataset, (2) running the significance test on the sampled set, and (3) estimating the power by averaging across the trials.",
                "cite_spans": [
                    {
                        "start": 84,
                        "end": 103,
                        "text": "(Card et al., 2020;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 104,
                        "end": 122,
                        "text": "Wei and Jia, 2021)",
                        "ref_id": "BIBREF84"
                    },
                    {
                        "start": 280,
                        "end": 303,
                        "text": "(Deutsch et al., 2021b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.1 Detailed Settings",
                "sec_num": null
            },
            {
                "text": "The essence of the test is to have a series of simulated datasets sampled from the existing dataset and run the significance test on the sampled sets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.1 Detailed Settings",
                "sec_num": null
            },
            {
                "text": "Here the existing dataset X consists of humanannotated scores of system outputs. We use paired bootstrapping for the significance test. The power analysis is conducted over all the system pairs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B.1 Detailed Settings",
                "sec_num": null
            },
            {
                "text": "Fig. 4 , Fig. 5 , and Fig. 6 show the power analysis results on CNNDM, XSum and SamSum respectively in \u00a74.1, where the system pairs are grouped by their performance difference in either ACU or ROUGE1 recall scores. Similar to our findings on CNNDM in \u00a74.1, we observe that increasing the sample size can effectively raise the statistical power. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 5,
                        "end": 6,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 14,
                        "end": 15,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 27,
                        "end": 28,
                        "text": "6",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "B.2 Powers of ACU Annotations",
                "sec_num": null
            },
            {
                "text": "We use correlations to analyze the inherent similarity between different human evaluation protocols, and the performance of automatic metrics, which is evaluated based on the correlations between the metric-calculated summary scores and the humanannotated summary scores. Specifically, given m system outputs on each of the n data samples and two different evaluation methods (e.g., human evaluation and an automatic metric) resulting in two n-row, m-column score matrices X and Y , the summary-level correlation is an average of samplewise correlations:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C Calculating Correlations",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "r sum (X, Y ) = i C(X i , Y i ) n ,",
                        "eq_num": "(3)"
                    }
                ],
                "section": "C Calculating Correlations",
                "sec_num": null
            },
            {
                "text": "where X i , Y i are the evaluation results on the i-th data sample and C is a function calculating a correlation coefficient (e.g., the Pearson correlation coefficient). In contrast, the system-level correlation is calculated on the aggregated system scores:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C Calculating Correlations",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "r sys (X, Y ) = C( X, \u0232 ),",
                        "eq_num": "(4)"
                    }
                ],
                "section": "C Calculating Correlations",
                "sec_num": null
            },
            {
                "text": "where X and \u0232 contain m entries which are the system scores from the two evaluation methods averaged across n data samples, e.g., X0 = i X i,0 /n.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C Calculating Correlations",
                "sec_num": null
            },
            {
                "text": "The 100 examples chosen for annotation in \u00a75 are a subset of the CNNDM ACU test set, and as here we aim to analyze trends among protocols as opposed to observing statistically significant differences among systems, we believe 100 examples suffice for this collection. We summarize and compare different protocols in Tab. 11. We provide the following instructions to annotators for non-ACU annotations. We will release the full interface and instructions. Prior: We ask the annotator to imagine each of the candidate summaries to be evaluated as a summary of a longer news article and answer the following question: how good do you think this summary is? Ref-free: The rating measures how well the summary captures the key points of the news article. Consider whether all and only the important aspects are contained in the summary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D.1 Data Collection Details",
                "sec_num": null
            },
            {
                "text": "Prior \u2717 \u2717 \u2717 Ref-free \u2713 \u2717 \u2717 Ref-based \u2717 \u2713 \u2717 ACU \u2717 \u2713 \u2713",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D.1 Data Collection Details",
                "sec_num": null
            },
            {
                "text": "The rating measures how similar two summaries are. The similarity depends on if the summaries contain similar information, not if they use the same words.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ref-based:",
                "sec_num": null
            },
            {
                "text": "We present the result analysis of \u00a75.2 here.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D.2 Results Analysis",
                "sec_num": null
            },
            {
                "text": "We show the summary-level Pearson's Correlation Coefficients among different protocols in Tab. 12.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summary Level Correlation",
                "sec_num": null
            },
            {
                "text": "The power analysis results on the Prior, Ref-free, Ref-based, and ACU protocols are shown in Fig. 7 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 98,
                        "end": 99,
                        "text": "7",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Power Analysis",
                "sec_num": null
            },
            {
                "text": "We show a case study in Tab. 13 comparing the summaries generated by BRIO and GPT-3. GPT-3 scores higher on Prior and Reffree (3.33/3.33 for BRIO and 3.66/4.00 for GPT-3). However, the BRIO summary scores 0.77 on unnormalized ACU annotations while GPT-3 scores 0.33. Also, Ref-based annotations favor BRIO over ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case Study",
                "sec_num": null
            },
            {
                "text": "We provide additional metric details as well as results for other metrics in \u00a76. Note that for ROUGE, the n-grams of the candidate and reference texts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "Statistics (Grusky et al., 2018) reports summary statistics such as the length, novel and repeated n-grams in the summary, the compression ratio between the summary and article, and measures of the level of extraction. Coverage is the percentage of words that are part of an extractive fragment and density is the average length of the extractive fragment each summary word belongs to.",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 32,
                        "text": "(Grusky et al., 2018)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "MoverScore (Zhao et al., 2019) measures semantic distance with Word Mover's Distance (Kusner et al., 2015) on pooled BERT n-gram embeddings.",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 30,
                        "text": "(Zhao et al., 2019)",
                        "ref_id": "BIBREF92"
                    },
                    {
                        "start": 85,
                        "end": 106,
                        "text": "(Kusner et al., 2015)",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "SUPERT (Gao et al., 2020) measures the semantic similarity of summaries with pseudo-reference summaries created by extracting salient sentences from the source documents. BLANC (Vasilyev et al., 2020) measures the performance gains of a pre-trained language model on language understanding tasks on the input document when given access to a document summary.",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 25,
                        "text": "(Gao et al., 2020)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 177,
                        "end": 200,
                        "text": "(Vasilyev et al., 2020)",
                        "ref_id": "BIBREF81"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "(a) Reference Summary: Chelsea weren't awarded a penalty for David Ospina's clash with Oscar. Arsenal goalkeeper clattered Oscar inside the box. Brazilian was taken off at half-time, with Didier Drogba replacing him.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "(b) System Summary (BRIO, (Liu et al., 2022) ): Oscar collided with Arsenal goalkeeper David Ospina in the 16th minute of the London derby . The Brazilian was substituted at half-time and Jose Mourinho said he suffered 'possible concussion' . Oscar was knocked back by the goalkeeper but Michael Oliver didn't award Chelsea a penalty .",
                "cite_spans": [
                    {
                        "start": 26,
                        "end": 44,
                        "text": "(Liu et al., 2022)",
                        "ref_id": "BIBREF51"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "(c) System Summary (GPT-3, (Brown et al., 2020) ): Oscar was forced to leave the match against Arsenal after sustaining a possible concussion from a collision with the opposing goalkeeper. The referee did not award Chelsea a penalty, despite the collision appearing to warrant one. Sky Sports pundits agreed that the collision should have been penalized, with some suggesting it could have even warranted a red card.",
                "cite_spans": [
                    {
                        "start": 19,
                        "end": 47,
                        "text": "(GPT-3, (Brown et al., 2020)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "(d) ACUs with corresponding evaluations:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "\u2022 Chelsea weren't awarded a penalty. Table 13 : Example of a reference summary, system summaries and corresponding ACU annotations on CNNDM.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 43,
                        "end": 45,
                        "text": "13",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "The presence or absence of the ACUs for BRIO (in blue) and GPT-3 (in green) are marked by (\u2713) and (\u2717).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "QAEval (Deutsch et al., 2021a) reports both an F1 and exact match (em) score. We do not report the learned answer overlap metric.",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 30,
                        "text": "(Deutsch et al., 2021a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "SummaQA (Scialom et al., 2019) reports an F1 score and model confidence. We plan to report QuestEval (Scialom et al., 2021) in a future version.",
                "cite_spans": [
                    {
                        "start": 8,
                        "end": 30,
                        "text": "(Scialom et al., 2019)",
                        "ref_id": "BIBREF71"
                    },
                    {
                        "start": 101,
                        "end": 123,
                        "text": "(Scialom et al., 2021)",
                        "ref_id": "BIBREF70"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "Lite 3 Pyramid includes four variations of the metric depending on the entailment model (two vs three-class entailment model) and how the output is used (as a probability vs a 0/1 label). CTC (Deng et al., 2021) proposes metrics for Compression, transduction, and creation tasks as variations of textual alignment. Relevance is scored as the average bi-directional alignment between generated and reference summaries. SimCSE (Gao et al., 2021) apply contrastive learning to learn improved sentence representations, which can then be used to compare generated and reference summary similarity.",
                "cite_spans": [
                    {
                        "start": 192,
                        "end": 211,
                        "text": "(Deng et al., 2021)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 425,
                        "end": 443,
                        "text": "(Gao et al., 2021)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "UniEval (Zhong et al., 2022 ) frames text evaluation as the answer to yes or no questions, in our case whether the summary is relevant or not, and constructs pseudo-data to fine-tune language models for this setting.",
                "cite_spans": [
                    {
                        "start": 8,
                        "end": 27,
                        "text": "(Zhong et al., 2022",
                        "ref_id": "BIBREF94"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.1 Metrics",
                "sec_num": null
            },
            {
                "text": "In \u00a76.1 we evaluate two different LLM-based automatic evaluation methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.2 Metrics based on Large Language Models",
                "sec_num": null
            },
            {
                "text": "GPTScore (Fu et al., 2023) formulates the text evaluation as the text-filling task and takes the token probability predicted by the LLMs as the quality score. We use the following prompt for calculating the recall score of the system outputs:",
                "cite_spans": [
                    {
                        "start": 9,
                        "end": 26,
                        "text": "(Fu et al., 2023)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.2 Metrics based on Large Language Models",
                "sec_num": null
            },
            {
                "text": "Answer the question based on the following reference summary and candidate summary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.2 Metrics based on Large Language Models",
                "sec_num": null
            },
            {
                "text": "Question: Can all of the information in the reference summary be found in the candidate summary? (a). Yes. (b). No.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.2 Metrics based on Large Language Models",
                "sec_num": null
            },
            {
                "text": "Reference Summary: {{Reference}} Candidate Summary: {{Candidate}}",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.2 Metrics based on Large Language Models",
                "sec_num": null
            },
            {
                "text": "The LLM-predicted probability of the last token, \"Yes\", is used as the recall score. We use the OpenAI's text-davinci-003 as the LLM.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer: Yes",
                "sec_num": null
            },
            {
                "text": "G-Eval (Liu et al., 2023) introduces a similar task as GPTscore, but has the LLM to predict a numerical score directly instead of using the LLMpredicted probability. We use the following prompt for the task: You will receive a reference summary and a candidate summary. Your task is to compare these two summaries and assess the extent to which the candidate summary covers the information presented in the reference summary.",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 25,
                        "text": "(Liu et al., 2023)",
                        "ref_id": "BIBREF48"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer: Yes",
                "sec_num": null
            },
            {
                "text": "Please indicate your agreement with the following statement: \"All of the information in the reference summary can be found in the candidate summary.\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer: Yes",
                "sec_num": null
            },
            {
                "text": "Use the following 5-point scale when determining your response:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer: Yes",
                "sec_num": null
            },
            {
                "text": "1. Strongly Disagree -Agreement (1-5):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer: Yes",
                "sec_num": null
            },
            {
                "text": "We note that we set the sampling temperature to 0 to ensure more deterministic behavior for G-Eval-3.5 and G-Eval-4. We also experiment with a sampling strategy with GPT-3.5 (G-Eval-3.5-S), where we sample 5 outputs with a temperate 1 and take the average score as the final prediction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer: Yes",
                "sec_num": null
            },
            {
                "text": "We collect in total 50 different automatic metrics (including different variations of the same metric), and evaluate their performance using our collected ACU benchmark on CNNDM, XSum and SamSum datasets with three different correlation coefficients ( \u00a76.1). Tab. 15 reports the system-level correlation with the un-normalized ACU score (Eq. 1). Tab. 16 reports the summary-level correlation with the unnormalized ACU score (Eq. 1). Tab. 17 reports the system-level correlation with the normalized ACU score (Eq. 2). Tab. 18 reports the summary-level correlation with the normalized ACU score (Eq. 2).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.3 Metric Correlation with ACU Scores",
                "sec_num": null
            },
            {
                "text": "For metric elevation in \u00a76.1, we provide the system pairs in the six different buckets grouped by their performance differences below. Bucket 1: CLIFF V.S. FROST, CTRLSUM V.S. GSUM, BART V.S. CLIFF, GOLD V.S. FROST, BART V.S. FROST, CLIFF V.S. GOLD, BRIO V.S. GSUM, GOLD V.S. PEGASUS, BRIO V.S. CTRLSUM, BART V.S. GOLD, BRIO-EXT V.S.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.4 System Pairs for Fine-grained Metric Evaluation",
                "sec_num": null
            },
            {
                "text": "Bucket 2: FROST V.S. PEGASUS, CLIFF V.S. PEGASUS, PEGASUS V.S. GLOB, BRIO-EXT V.S. SIMCLS, BART V.S. PEGASUS, BRIO V.S. MATCHSUM, BART V.S. SIMCLS, GOLD V.S. GLOB, CLIFF V.S. SIMCLS, MATCHSUM V.S. GSUM, SIMCLS V.S. FROST. Bucket 3: MATCHSUM V.S. SIMCLS, FROST V.S. GLOB, MATCHSUM V.S. CTRLSUM, CLIFF V.S. GLOB, BRIO V.S. BRIO-EXT, GOLD V.S. SIMCLS, BART V.S. GLOB, BRIO-EXT V.S. GSUM, BRIO-EXT V.S. CTRLSUM, BART V.S. BRIO-EXT, SIMCLS V.S. PEGASUS. Bucket 4: CLIFF V.S. BRIO-EXT, BRIO-EXT V.S. FROST, BRIO V.S. SIMCLS, GOLD V.S. BRIO-EXT, BART V.S. MATCHSUM, CLIFF V.S. MATCHSUM, SIMCLS V.S. GSUM, MATCH-SUM V.S. FROST, SIMCLS V.S. GLOB, SIMCLS V.S. CTRLSUM, BRIO-EXT V.S. PEGASUS. Bucket 5: GOLD V.S. MATCHSUM, MATCH-SUM V.S. PEGASUS, BART V.S. BRIO, BRIO-EXT V.S. GLOB, BRIO V.S. CLIFF, BRIO V.S. FROST, BART V.S. GSUM, BART V.S. CTRL-SUM, BRIO V.S. GOLD, CLIFF V.S. GSUM, FROST V.S. GSUM. Bucket 6: CLIFF V.S. CTRLSUM, MATCHSUM V.S. GLOB, CTRLSUM V.S. FROST, GOLD V.S. GSUM, BRIO V.S. PEGASUS, GOLD V.S. CTRL-SUM, PEGASUS V.S. GSUM, CTRLSUM V.S. PEGASUS, BRIO V.S. GLOB, GLOB V.S. GSUM, CTRLSUM V.S. GLOB.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "MATCHSUM.",
                "sec_num": null
            },
            {
                "text": "We select several automatic metrics and calculate the confidence intervals of their system-level correlations with the ACU scores ( \u00a76.2). The results are in Fig. 8 . Similar to Deutsch et al. (2021b) , we found that the confidence intervals are large. However, having a larger sample size can effectively reduce the confidence interval. Specifically, we use re-sampling to generate a series of synthetic sample sets with several different sizes and calculate the confidence interval by averaging over the sampled sets with the same size. As shown in Fig. 9 , larger sample sizes lead to more stable results.",
                "cite_spans": [
                    {
                        "start": 178,
                        "end": 200,
                        "text": "Deutsch et al. (2021b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 163,
                        "end": 164,
                        "text": "8",
                        "ref_id": "FIGREF8"
                    },
                    {
                        "start": 556,
                        "end": 557,
                        "text": "9",
                        "ref_id": "FIGREF9"
                    }
                ],
                "eq_spans": [],
                "section": "E.5 Confidence Interval",
                "sec_num": null
            },
            {
                "text": "We use Alg.1 to conduct a power analysis of metric comparison based on their Kendall's correlations with ACU scores ( \u00a76.2). We choose 20 metrics for comparison, resulting 190 metric pairs in total, which are (1) BARTScore-r-parabank, (2) BERTScore-r-deberta, (3) BERTScore-r-roberta, (4) BLANC, (5) CHRF, (6) CTC, (7) Meteor, (8) ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.6 Power Analysis of Metric Comparison",
                "sec_num": null
            },
            {
                "text": "We present the correlations between automatic metrics and different human evaluation protocols in Tab. 19 as discussed in \u00a76.2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E.7 Metric Correlation with Different Human Evaluation Protocols",
                "sec_num": null
            },
            {
                "text": "We provide a brief survey for the human evaluation practices of 55 selected papers on text summariza- tion published at NAACLfoot_8 , ACLfoot_9 , and EMNLPfoot_10 from 2022. We follow the design of a similar study in Gehrmann et al. ( 2022) as described below. The results are shown in Tab. 14. Performed Human Evaluation: Report \"yes\", if a human evaluation of any kind is done. We report that 71% of analyzed papers did human evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F Human Evaluation Practices in Recent Text Summarization Research",
                "sec_num": null
            },
            {
                "text": "Significance Test: Report \"yes\", if a significance test is done on the human annotation results. Of the 39 papers that conducted human evaluation, a total of 27 papers reported the result of a significance test (68%), which is much higher compared to the 25% reported in the previous survey (Gehrmann et al., 2022) .",
                "cite_spans": [
                    {
                        "start": 291,
                        "end": 314,
                        "text": "(Gehrmann et al., 2022)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F Human Evaluation Practices in Recent Text Summarization Research",
                "sec_num": null
            },
            {
                "text": "Power Analysis: Report \"yes\", if a power analysis of any kind is mentioned. Of the 39 papers that conducted human evaluation, none of the papers did power analysis, the same as the result provided in the previous survey of Gehrmann et al. (2022) .",
                "cite_spans": [
                    {
                        "start": 223,
                        "end": 245,
                        "text": "Gehrmann et al. (2022)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F Human Evaluation Practices in Recent Text Summarization Research",
                "sec_num": null
            },
            {
                "text": "Inter-annotator Agreement: Report \"yes\", if any kind of agreement test is conducted to evaluate the quality of human annotation themselves. Overall, we report a total of 12 papers (28%) that did agreement tests and documented specific agreement values. 9 out of the 12 papers recorded the specific agreement test, with Krippendorff's alpha as the most commonly used measurement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F Human Evaluation Practices in Recent Text Summarization Research",
                "sec_num": null
            },
            {
                "text": "Participants (crowd-worker, expert, etc.) Report \"yes\", if at least the number of human evaluators, document sample size, annotators per document, or their demographics is mentioned. We show the sample size of the conducted human evaluation ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "F Human Evaluation Practices in Recent Text Summarization Research",
                "sec_num": null
            },
            {
                "text": "We release our benchmark and evaluation scripts at https://github.com/Yale-LILY/ROSE.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We note salience can be an inherently subjective quality, and the reference summary of common datasets may not always be the actual \"gold standard\", discussed more in \u00a77.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We note that it can be impossible to provide a practical definition of atomic facts. Instead, we use it as a general concept for fine-grained information units.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://www.mturk.com/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We release all of the system outputs with a unified, cased, untokenized format to facilitate future research.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We provide a brief survey of the practices of human evaluation in recent text summarization research in Appendix F.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "For example, the DUC evaluation campaigns set a prespecified maximum summary length, or summary budget.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://beta.openai.com/docs/ model-index-for-researchers",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://aclanthology.org/events/naacl-2022/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://aclanthology.org/events/acl-2022/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://preview.aclanthology.org/ emnlp-22-ingestion/volumes/2022.emnlp-main/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank the anonymous reviewers for their constructive comments. We are grateful to Arman Cohan for insightful discussions and suggestions, Daniel Deutsch for the initial discussions, Richard Yuanzhe Pang for sharing system outputs, and Philippe Laban for valuable comments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Reevaluating evaluation in text summarization",
                "authors": [
                    {
                        "first": "Manik",
                        "middle": [],
                        "last": "Bhandari",
                        "suffix": ""
                    },
                    {
                        "first": "Pranav",
                        "middle": [],
                        "last": "Narayan Gour",
                        "suffix": ""
                    },
                    {
                        "first": "Atabak",
                        "middle": [],
                        "last": "Ashfaq",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "9347--9359",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.emnlp-main.751"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Manik Bhandari, Pranav Narayan Gour, Atabak Ash- faq, Pengfei Liu, and Graham Neubig. 2020. Re- evaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9347-9359, Online. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Language models are few-shot learners",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Tom",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Nick",
                        "middle": [],
                        "last": "Mann",
                        "suffix": ""
                    },
                    {
                        "first": "Melanie",
                        "middle": [],
                        "last": "Ryder",
                        "suffix": ""
                    },
                    {
                        "first": "Jared",
                        "middle": [],
                        "last": "Subbiah",
                        "suffix": ""
                    },
                    {
                        "first": "Prafulla",
                        "middle": [],
                        "last": "Kaplan",
                        "suffix": ""
                    },
                    {
                        "first": "Arvind",
                        "middle": [],
                        "last": "Dhariwal",
                        "suffix": ""
                    },
                    {
                        "first": "Pranav",
                        "middle": [],
                        "last": "Neelakantan",
                        "suffix": ""
                    },
                    {
                        "first": "Girish",
                        "middle": [],
                        "last": "Shyam",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Sastry",
                        "suffix": ""
                    },
                    {
                        "first": "Sandhini",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    },
                    {
                        "first": "Ariel",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Gretchen",
                        "middle": [],
                        "last": "Herbert-Voss",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Krueger",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Henighan",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "M"
                        ],
                        "last": "Ramesh",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Ziegler",
                        "suffix": ""
                    },
                    {
                        "first": "Clemens",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Winter",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Hesse",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Mateusz",
                        "middle": [],
                        "last": "Sigler",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Litwin",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Gray",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Chess",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Berner",
                        "suffix": ""
                    },
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Mccandlish",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Ad- vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process- ing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "CLIFF: Contrastive learning for improving faithfulness and factuality in abstractive summarization",
                "authors": [
                    {
                        "first": "Shuyang",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "6633--6649",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.emnlp-main.532"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shuyang Cao and Lu Wang. 2021. CLIFF: Contrastive learning for improving faithfulness and factuality in abstractive summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6633-6649, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "With little power comes great responsibility",
                "authors": [
                    {
                        "first": "Dallas",
                        "middle": [],
                        "last": "Card",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Henderson",
                        "suffix": ""
                    },
                    {
                        "first": "Urvashi",
                        "middle": [],
                        "last": "Khandelwal",
                        "suffix": ""
                    },
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Kyle",
                        "middle": [],
                        "last": "Mahowald",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "9263--9274",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.emnlp-main.745"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dallas Card, Peter Henderson, Urvashi Khandelwal, Robin Jia, Kyle Mahowald, and Dan Jurafsky. 2020. With little power comes great responsibility. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9263-9274, Online. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "The price of debiasing automatic metrics in natural language evalaution",
                "authors": [
                    {
                        "first": "Arun",
                        "middle": [],
                        "last": "Chaganty",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Mussmann",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "643--653",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P18-1060"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic metrics in natural language evalaution. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 643-653, Melbourne, Australia. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Multi-view sequenceto-sequence models with conversational structure for abstractive dialogue summarization",
                "authors": [
                    {
                        "first": "Jiaao",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Diyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "4106--4118",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.emnlp-main.336"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiaao Chen and Diyi Yang. 2020. Multi-view sequence- to-sequence models with conversational structure for abstractive dialogue summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4106- 4118, Online. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "All that's 'human' is not gold: Evaluating human evaluation of generated text",
                "authors": [
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Tal",
                        "middle": [],
                        "last": "August",
                        "suffix": ""
                    },
                    {
                        "first": "Sofia",
                        "middle": [],
                        "last": "Serrano",
                        "suffix": ""
                    },
                    {
                        "first": "Nikita",
                        "middle": [],
                        "last": "Haduong",
                        "suffix": ""
                    },
                    {
                        "first": "Suchin",
                        "middle": [],
                        "last": "Gururangan",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "7282--7296",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.acl-long.565"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that's 'human' is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7282-7296, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Revisiting summarization evaluation for scientific articles",
                "authors": [
                    {
                        "first": "Arman",
                        "middle": [],
                        "last": "Cohan",
                        "suffix": ""
                    },
                    {
                        "first": "Nazli",
                        "middle": [],
                        "last": "Goharian",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)",
                "volume": "",
                "issue": "",
                "pages": "806--813",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arman Cohan and Nazli Goharian. 2016. Revisiting summarization evaluation for scientific articles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 806-813, Portoro\u017e, Slovenia. European Lan- guage Resources Association (ELRA).",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Compression, transduction, and creation: A unified framework for evaluating natural language generation",
                "authors": [
                    {
                        "first": "Mingkai",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengzhong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Xing",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiting",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "7580--7605",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.emnlp-main.599"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric Xing, and Zhiting Hu. 2021. Compression, transduction, and creation: A unified framework for evaluating natural language generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7580-7605, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "2021a. Towards question-answering as an automatic metric for evaluating the content quality of a summary",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Deutsch",
                        "suffix": ""
                    },
                    {
                        "first": "Tania",
                        "middle": [],
                        "last": "Bedrax-Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "9",
                "issue": "",
                "pages": "774--789",
                "other_ids": {
                    "DOI": [
                        "10.1162/tacl_a_00397"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. 2021a. Towards question-answering as an automatic metric for evaluating the content quality of a sum- mary. Transactions of the Association for Computa- tional Linguistics, 9:774-789.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "2021b. A statistical analysis of summarization evaluation metrics using resampling methods",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Deutsch",
                        "suffix": ""
                    },
                    {
                        "first": "Rotem",
                        "middle": [],
                        "last": "Dror",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "9",
                "issue": "",
                "pages": "1132--1146",
                "other_ids": {
                    "DOI": [
                        "10.1162/tacl_a_00417"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Daniel Deutsch, Rotem Dror, and Dan Roth. 2021b. A statistical analysis of summarization evaluation met- rics using resampling methods. Transactions of the Association for Computational Linguistics, 9:1132- 1146.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Reexamining system-level correlations of automatic summarization evaluation metrics",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Deutsch",
                        "suffix": ""
                    },
                    {
                        "first": "Rotem",
                        "middle": [],
                        "last": "Dror",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "6038--6052",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2022.naacl-main.442"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Daniel Deutsch, Rotem Dror, and Dan Roth. 2022. Re- examining system-level correlations of automatic summarization evaluation metrics. In Proceedings of the 2022 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, pages 6038-6052, Seattle, United States. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "SacreROUGE: An open-source library for using and developing summarization evaluation metrics",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Deutsch",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)",
                "volume": "",
                "issue": "",
                "pages": "120--125",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.nlposs-1.17"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Daniel Deutsch and Dan Roth. 2020. SacreROUGE: An open-source library for using and developing sum- marization evaluation metrics. In Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS), pages 120-125, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Unified language model pre-training for natural language understanding and generation",
                "authors": [
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Hsiao-Wuen",
                        "middle": [],
                        "last": "Hon",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "13042--13054",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi- aodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Pro- cessing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13042-13054.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "GSum: A general framework for guided neural abstractive summarization",
                "authors": [
                    {
                        "first": "Zi-Yi",
                        "middle": [],
                        "last": "Dou",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroaki",
                        "middle": [],
                        "last": "Hayashi",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengbao",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "4830--4842",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.naacl-main.384"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2021. GSum: A gen- eral framework for guided neural abstractive summa- rization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 4830-4842, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
                "authors": [
                    {
                        "first": "Esin",
                        "middle": [],
                        "last": "Durmus",
                        "suffix": ""
                    },
                    {
                        "first": "He",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Mona",
                        "middle": [],
                        "last": "Diab",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "5055--5070",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.454"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faith- fulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 5055- 5070, Online. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "2022a. Summeval: Re-evaluating summarization evaluation",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Fabbri",
                        "suffix": ""
                    },
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Kryscinski",
                        "suffix": ""
                    },
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Mc-Cann",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Dragomir",
                        "middle": [],
                        "last": "Radev",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "9",
                "issue": "0",
                "pages": "391--409",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander Fabbri, Wojciech Kryscinski, Bryan Mc- Cann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2022a. Summeval: Re-evaluating summariza- tion evaluation. Transactions of the Association for Computational Linguistics, 9(0):391-409.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "QAFactEval: Improved QA-based factual consistency evaluation for summarization",
                "authors": [
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Fabbri",
                        "suffix": ""
                    },
                    {
                        "first": "Chien-Sheng",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "2587--2601",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2022.naacl-main.187"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022b. QAFactEval: Improved QA-based factual consistency evaluation for sum- marization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 2587-2601, Seattle, United States. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Language model as an annotator: Exploring DialoGPT for dialogue summarization",
                "authors": [
                    {
                        "first": "Xiachong",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaocheng",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Libo",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "1479--1491",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.acl-long.117"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, and Ting Liu. 2021. Language model as an annota- tor: Exploring DialoGPT for dialogue summarization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), pages 1479-1491, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Gptscore: Evaluate as you desire",
                "authors": [
                    {
                        "first": "Jinlan",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "See-Kiong",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengbao",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. ArXiv, abs/2302.04166.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "GO FIGURE: A meta evaluation of factuality in summarization",
                "authors": [
                    {
                        "first": "Saadia",
                        "middle": [],
                        "last": "Gabriel",
                        "suffix": ""
                    },
                    {
                        "first": "Asli",
                        "middle": [],
                        "last": "Celikyilmaz",
                        "suffix": ""
                    },
                    {
                        "first": "Rahul",
                        "middle": [],
                        "last": "Jha",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "volume": "",
                "issue": "",
                "pages": "478--487",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.findings-acl.42"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2021. GO FIGURE: A meta eval- uation of factuality in summarization. In Findings of the Association for Computational Linguistics: ACL- IJCNLP 2021, pages 478-487, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "DialSummEval: Revisiting summarization evaluation for dialogues",
                "authors": [
                    {
                        "first": "Mingqi",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "5693--5709",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2022.naacl-main.418"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mingqi Gao and Xiaojun Wan. 2022. DialSummEval: Revisiting summarization evaluation for dialogues. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 5693-5709, Seattle, United States. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "SimCSE: Simple contrastive learning of sentence embeddings",
                "authors": [
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Xingcheng",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "6894--6910",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.emnlp-main.552"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence em- beddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process- ing, pages 6894-6910, Online and Punta Cana, Do- minican Republic. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "SUPERT: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Steffen",
                        "middle": [],
                        "last": "Eger",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1347--1354",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.124"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization. In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 1347- 1354, Online. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "The GEM benchmark: Natural language generation, its evaluation and metrics",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Gehrmann",
                        "suffix": ""
                    },
                    {
                        "first": "Tosin",
                        "middle": [],
                        "last": "Adewumi",
                        "suffix": ""
                    },
                    {
                        "first": "Karmanya",
                        "middle": [],
                        "last": "Aggarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Pawan",
                        "middle": [],
                        "last": "Sasanka Ammanamanchi",
                        "suffix": ""
                    },
                    {
                        "first": "Anuoluwapo",
                        "middle": [],
                        "last": "Aremu",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bosselut",
                        "suffix": ""
                    },
                    {
                        "first": "Raghavi",
                        "middle": [],
                        "last": "Khyathi",
                        "suffix": ""
                    },
                    {
                        "first": "Miruna-Adriana",
                        "middle": [],
                        "last": "Chandu",
                        "suffix": ""
                    },
                    {
                        "first": "Dipanjan",
                        "middle": [],
                        "last": "Clinciu",
                        "suffix": ""
                    },
                    {
                        "first": "Kaustubh",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Wanyu",
                        "middle": [],
                        "last": "Dhole",
                        "suffix": ""
                    },
                    {
                        "first": "Esin",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "Durmus",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Du\u0161ek",
                        "suffix": ""
                    },
                    {
                        "first": "Varun",
                        "middle": [],
                        "last": "Chinenye Emezue",
                        "suffix": ""
                    },
                    {
                        "first": "Cristina",
                        "middle": [],
                        "last": "Gangal",
                        "suffix": ""
                    },
                    {
                        "first": "Tatsunori",
                        "middle": [],
                        "last": "Garbacea",
                        "suffix": ""
                    },
                    {
                        "first": "Yufang",
                        "middle": [],
                        "last": "Hashimoto",
                        "suffix": ""
                    },
                    {
                        "first": "Yacine",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    },
                    {
                        "first": "Harsh",
                        "middle": [],
                        "last": "Jernite",
                        "suffix": ""
                    },
                    {
                        "first": "Yangfeng",
                        "middle": [],
                        "last": "Jhamtani",
                        "suffix": ""
                    },
                    {
                        "first": "Shailza",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Mihir",
                        "middle": [],
                        "last": "Jolly",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Kale",
                        "suffix": ""
                    },
                    {
                        "first": "Faisal",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Aman",
                        "middle": [],
                        "last": "Ladhak",
                        "suffix": ""
                    },
                    {
                        "first": "Mounica",
                        "middle": [],
                        "last": "Madaan",
                        "suffix": ""
                    },
                    {
                        "first": "Khyati",
                        "middle": [],
                        "last": "Maddela",
                        "suffix": ""
                    },
                    {
                        "first": "Saad",
                        "middle": [],
                        "last": "Mahajan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mahamood",
                        "suffix": ""
                    },
                    {
                        "first": "Prasad",
                        "middle": [],
                        "last": "Bodhisattwa",
                        "suffix": ""
                    },
                    {
                        "first": "Pedro",
                        "middle": [
                            "Henrique"
                        ],
                        "last": "Majumder",
                        "suffix": ""
                    },
                    {
                        "first": "Angelina",
                        "middle": [],
                        "last": "Martins",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Mcmillan-Major",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mille",
                        "suffix": ""
                    },
                    {
                        "first": "Moin",
                        "middle": [],
                        "last": "Emiel Van Miltenburg",
                        "suffix": ""
                    },
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Nadeem",
                        "suffix": ""
                    },
                    {
                        "first": "Vitaly",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Andre",
                        "middle": [],
                        "last": "Nikolaev",
                        "suffix": ""
                    },
                    {
                        "first": "Salomey",
                        "middle": [],
                        "last": "Niyongabo Rubungo",
                        "suffix": ""
                    },
                    {
                        "first": "Ankur",
                        "middle": [],
                        "last": "Osei",
                        "suffix": ""
                    },
                    {
                        "first": "Laura",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    },
                    {
                        "first": "Niranjan",
                        "middle": [],
                        "last": "Perez-Beltrachini",
                        "suffix": ""
                    },
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Rao",
                        "suffix": ""
                    },
                    {
                        "first": "Vikas",
                        "middle": [],
                        "last": "Raunak",
                        "suffix": ""
                    },
                    {
                        "first": "Juan",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Diego",
                        "middle": [],
                        "last": "Rodriguez",
                        "suffix": ""
                    },
                    {
                        "first": "Sashank",
                        "middle": [],
                        "last": "Santhanam",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 1st Workshop on Natural Language Generation",
                "volume": "",
                "issue": "",
                "pages": "96--120",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.gem-1.10"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khy- athi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ond\u0159ej Du\u0161ek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jham- tani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bod- hisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Jo\u00e3o Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimo- rina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96-120, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Gehrmann",
                        "suffix": ""
                    },
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Thibault",
                        "middle": [],
                        "last": "Sellam",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Gehrmann, Elizabeth Clark, and Thibault Sel- lam. 2022. Repairing the cracked foundation: A sur- vey of obstacles in evaluation practices for generated text. ArXiv preprint, abs/2202.06935.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization",
                "authors": [
                    {
                        "first": "Bogdan",
                        "middle": [],
                        "last": "Gliwa",
                        "suffix": ""
                    },
                    {
                        "first": "Iwona",
                        "middle": [],
                        "last": "Mochol",
                        "suffix": ""
                    },
                    {
                        "first": "Maciej",
                        "middle": [],
                        "last": "Biesek",
                        "suffix": ""
                    },
                    {
                        "first": "Aleksander",
                        "middle": [],
                        "last": "Wawer",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2nd Workshop on New Frontiers in Summarization",
                "volume": "",
                "issue": "",
                "pages": "70--79",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-5409"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek- sander Wawer. 2019. SAMSum corpus: A human- annotated dialogue dataset for abstractive summa- rization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70-79, Hong Kong, China. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "News summarization and evaluation in the era of gpt-3",
                "authors": [
                    {
                        "first": "Tanya",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Junyi",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Jessy",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Durrett",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE",
                "authors": [
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "128--137",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D15-1013"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yvette Graham. 2015. Re-evaluating automatic sum- marization with BLEU and 192 shades of ROUGE. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 128- 137, Lisbon, Portugal. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
                "authors": [
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Grusky",
                        "suffix": ""
                    },
                    {
                        "first": "Mor",
                        "middle": [],
                        "last": "Naaman",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "708--719",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-1065"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long Pa- pers), pages 708-719, New Orleans, Louisiana. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "HighRES: Highlight-based reference-less evaluation of summarization",
                "authors": [
                    {
                        "first": "Hardy",
                        "middle": [],
                        "last": "Hardy",
                        "suffix": ""
                    },
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Vlachos",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3381--3392",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1330"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hardy Hardy, Shashi Narayan, and Andreas Vlachos. 2019. HighRES: Highlight-based reference-less eval- uation of summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3381-3392, Florence, Italy. Asso- ciation for Computational Linguistics.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Ctrlsum: Towards generic controllable text summarization",
                "authors": [
                    {
                        "first": "Junxian",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Kry\u015bci\u0144ski",
                        "suffix": ""
                    },
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Mccann",
                        "suffix": ""
                    },
                    {
                        "first": "Nazneen",
                        "middle": [],
                        "last": "Rajani",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Junxian He, Wojciech Kry\u015bci\u0144ski, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2020. Ctrlsum: Towards generic controllable text summarization.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Z-code++: A pre-trained language model optimized for abstractive summarization",
                "authors": [
                    {
                        "first": "Pengcheng",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Baolin",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Liyang",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Song",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Ruochen",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Hassan",
                        "middle": [],
                        "last": "Hany",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Awadalla",
                        "suffix": ""
                    },
                    {
                        "first": "Chenguang",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pengcheng He, Baolin Peng, Liyang Lu, Song Wang, Jie Mei, Yang Liu, Ruochen Xu, Hany Hassan Awadalla, Yu Shi, Chenguang Zhu, et al. 2022. Z-code++: A pre-trained language model optimized for abstractive summarization. ArXiv preprint, abs/2208.09770.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "TRUE: Re-evaluating factual consistency evaluation",
                "authors": [
                    {
                        "first": "Or",
                        "middle": [],
                        "last": "Honovich",
                        "suffix": ""
                    },
                    {
                        "first": "Roee",
                        "middle": [],
                        "last": "Aharoni",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Herzig",
                        "suffix": ""
                    },
                    {
                        "first": "Hagai",
                        "middle": [],
                        "last": "Taitelbaum",
                        "suffix": ""
                    },
                    {
                        "first": "Doron",
                        "middle": [],
                        "last": "Kukliansy",
                        "suffix": ""
                    },
                    {
                        "first": "Vered",
                        "middle": [],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Scialom",
                        "suffix": ""
                    },
                    {
                        "first": "Idan",
                        "middle": [],
                        "last": "Szpektor",
                        "suffix": ""
                    },
                    {
                        "first": "Avinatan",
                        "middle": [],
                        "last": "Hassidim",
                        "suffix": ""
                    },
                    {
                        "first": "Yossi",
                        "middle": [],
                        "last": "Matias",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering",
                "volume": "",
                "issue": "",
                "pages": "161--175",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2022.dialdoc-1.19"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. TRUE: Re-evaluating factual consistency evaluation. In Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 161- 175, Dublin, Ireland. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "What have we achieved on text summarization?",
                "authors": [
                    {
                        "first": "Dandan",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Leyang",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Sen",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Guangsheng",
                        "middle": [],
                        "last": "Bao",
                        "suffix": ""
                    },
                    {
                        "first": "Kun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "446--469",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.emnlp-main.33"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What have we achieved on text summarization? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 446-469, Online. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "2022b. Bidimensional leaderboards: Generate and evaluate language hand in hand",
                "authors": [
                    {
                        "first": "Jungo",
                        "middle": [],
                        "last": "Kasai",
                        "suffix": ""
                    },
                    {
                        "first": "Keisuke",
                        "middle": [],
                        "last": "Sakaguchi",
                        "suffix": ""
                    },
                    {
                        "first": "Le",
                        "middle": [],
                        "last": "Ronan",
                        "suffix": ""
                    },
                    {
                        "first": "Lavinia",
                        "middle": [],
                        "last": "Bras",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Dunagan",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Morrison",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Fabbri",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "3540--3557",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2022.naacl-main.259"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander Fabbri, Yejin Choi, and Noah A. Smith. 2022b. Bidimen- sional leaderboards: Generate and evaluate language hand in hand. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 3540-3557, Seattle, United States. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Computing krippendorff's alpha-reliability",
                "authors": [
                    {
                        "first": "Klaus",
                        "middle": [],
                        "last": "Krippendorff",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Klaus Krippendorff. 2011. Computing krippendorff's alpha-reliability.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Evaluating the factual consistency of abstractive text summarization",
                "authors": [
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Kryscinski",
                        "suffix": ""
                    },
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Mccann",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "9332--9346",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.emnlp-main.750"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "From word embeddings to document distances",
                "authors": [
                    {
                        "first": "Matt",
                        "middle": [
                            "J"
                        ],
                        "last": "Kusner",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [
                            "I"
                        ],
                        "last": "Kolkin",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [
                            "Q"
                        ],
                        "last": "Weinberger",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015",
                "volume": "37",
                "issue": "",
                "pages": "957--966",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kil- ian Q. Weinberger. 2015. From word embeddings to document distances. In Proceedings of the 32nd In- ternational Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 957-966. JMLR.org.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "SummaC: Re-Visiting NLIbased Models for Inconsistency Detection in Summarization",
                "authors": [
                    {
                        "first": "Philippe",
                        "middle": [],
                        "last": "Laban",
                        "suffix": ""
                    },
                    {
                        "first": "Tobias",
                        "middle": [],
                        "last": "Schnabel",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [
                            "N"
                        ],
                        "last": "Bennett",
                        "suffix": ""
                    },
                    {
                        "first": "Marti",
                        "middle": [
                            "A"
                        ],
                        "last": "Hearst",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "20",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 20d. SummaC: Re-Visiting NLI- based Models for Inconsistency Detection in Summa- rization. ArXiv preprint, abs/d.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments",
                "authors": [
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    },
                    {
                        "first": "Abhaya",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the Second Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "228--231",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceed- ings of the Second Workshop on Statistical Machine Translation, pages 228-231, Prague, Czech Republic. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Marjan",
                        "middle": [],
                        "last": "Ghazvininejad",
                        "suffix": ""
                    },
                    {
                        "first": "Abdelrahman",
                        "middle": [],
                        "last": "Mohamed",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "7871--7880",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.703"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and com- prehension. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Marjan",
                        "middle": [],
                        "last": "Ghazvininejad",
                        "suffix": ""
                    },
                    {
                        "first": "Abdelrahman",
                        "middle": [],
                        "last": "Mohamed",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "7871--7880",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.703"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020b. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and com- prehension. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Looking for a few good metrics: Automatic summarization evaluation-how many samples are enough",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "NTCIR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin. 2004a. Looking for a few good met- rics: Automatic summarization evaluation-how many samples are enough? In NTCIR.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "ROUGE: A package for automatic evaluation of summaries",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Text Summarization Branches Out",
                "volume": "",
                "issue": "",
                "pages": "74--81",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin. 2004b. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Correlation between ROUGE and human evaluation of extractive meeting summaries",
                "authors": [
                    {
                        "first": "Feifan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of ACL-08: HLT, Short Papers",
                "volume": "",
                "issue": "",
                "pages": "201--204",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Feifan Liu and Yang Liu. 2008. Correlation between ROUGE and human evaluation of extractive meeting summaries. In Proceedings of ACL-08: HLT, Short Papers, pages 201-204, Columbus, Ohio. Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Iter",
                        "suffix": ""
                    },
                    {
                        "first": "Yichong",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Shuo",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ruochen",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Chenguang",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evalua- tion using gpt-4 with better human alignment. ArXiv, abs/2303.16634.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Roberta: A robustly optimized bert pretraining approach",
                "authors": [
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Jingfei",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Mandar",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. ArXiv preprint, abs/1907.11692.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "SimCLS: A simple framework for contrastive learning of abstractive summarization",
                "authors": [
                    {
                        "first": "Yixin",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "volume": "2",
                "issue": "",
                "pages": "1065--1072",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.acl-short.135"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yixin Liu and Pengfei Liu. 2021. SimCLS: A sim- ple framework for contrastive learning of abstractive summarization. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 1065-1072, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "BRIO: Bringing order to abstractive summarization",
                "authors": [
                    {
                        "first": "Yixin",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Dragomir",
                        "middle": [],
                        "last": "Radev",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "2890--2903",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2022.acl-long.207"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. 2022. BRIO: Bringing order to abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 2890-2903, Dublin, Ireland. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Controllable neural dialogue summarization with personal named entity planning",
                "authors": [
                    {
                        "first": "Zhengyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Nancy",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "92--106",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.emnlp-main.8"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhengyuan Liu and Nancy Chen. 2021. Controllable neural dialogue summarization with personal named entity planning. In Proceedings of the 2021 Con- ference on Empirical Methods in Natural Language Processing, pages 92-106, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Gender and representation bias in GPT-3 generated stories",
                "authors": [
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Lucy",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Bamman",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the Third Workshop on Narrative Understanding",
                "volume": "",
                "issue": "",
                "pages": "48--55",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.nuse-1.5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Li Lucy and David Bamman. 2021. Gender and rep- resentation bias in GPT-3 generated stories. In Pro- ceedings of the Third Workshop on Narrative Un- derstanding, pages 48-55, Virtual. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Global-aware beam search for neural abstractive summarization",
                "authors": [
                    {
                        "first": "Ye",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Zixun",
                        "middle": [],
                        "last": "Lan",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Zong",
                        "suffix": ""
                    },
                    {
                        "first": "Kaizhu",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "34",
                "issue": "",
                "pages": "16545--16557",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ye Ma, Zixun Lan, Lu Zong, and Kaizhu Huang. 2021. Global-aware beam search for neural abstractive sum- marization. In Advances in Neural Information Pro- cessing Systems, volume 34, pages 16545-16557.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "On faithfulness and factuality in abstractive summarization",
                "authors": [
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Maynez",
                        "suffix": ""
                    },
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Bernd",
                        "middle": [],
                        "last": "Bohnet",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Mcdonald",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1906--1919",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.173"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factu- ality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, On- line. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
                "authors": [
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Nallapati",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Caglar",
                        "middle": [],
                        "last": "Cicero Dos Santos",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Gulcehre",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "280--290",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/K16-1028"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstrac- tive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Lan- guage Learning, pages 280-290, Berlin, Germany. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
                "authors": [
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Shay",
                        "middle": [
                            "B"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1797--1807",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1206"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for ex- treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1797-1807, Brussels, Bel- gium. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "20d. Planning with Learned Entity Prompts for Abstractive Summarization",
                "authors": [
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Yao",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Maynez",
                        "suffix": ""
                    },
                    {
                        "first": "Gon\u00e7alo",
                        "middle": [],
                        "last": "Sim\u00f5es",
                        "suffix": ""
                    },
                    {
                        "first": "Vitaly",
                        "middle": [],
                        "last": "Nikolaev",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Mcdonald",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shashi Narayan, Yao Zhao, Joshua Maynez, Gon\u00e7alo Sim\u00f5es, Vitaly Nikolaev, and Ryan McDonald. 20d. Planning with Learned Entity Prompts for Abstrac- tive Summarization. ArXiv preprint, abs/d.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Evaluating content selection in summarization: The pyramid method",
                "authors": [
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    },
                    {
                        "first": "Rebecca",
                        "middle": [],
                        "last": "Passonneau",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004",
                "volume": "",
                "issue": "",
                "pages": "145--152",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ani Nenkova and Rebecca Passonneau. 2004. Evaluat- ing content selection in summarization: The pyramid method. In Proceedings of the Human Language Technology Conference of the North American Chap- ter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 145-152, Boston, Mas- sachusetts, USA. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "Gpt-4 technical report",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Openai",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "Training language models to follow instructions with human feedback",
                "authors": [
                    {
                        "first": "Long",
                        "middle": [],
                        "last": "Ouyang",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Diogo",
                        "middle": [],
                        "last": "Almeida",
                        "suffix": ""
                    },
                    {
                        "first": "Carroll",
                        "middle": [],
                        "last": "Wainwright",
                        "suffix": ""
                    },
                    {
                        "first": "Pamela",
                        "middle": [],
                        "last": "Mishkin",
                        "suffix": ""
                    },
                    {
                        "first": "Chong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Sandhini",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Katarina",
                        "middle": [],
                        "last": "Slama",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Gray",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Schulman",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Hilton",
                        "suffix": ""
                    },
                    {
                        "first": "Fraser",
                        "middle": [],
                        "last": "Kelton",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "Maddie",
                        "middle": [],
                        "last": "Simens",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Welinder",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Christiano",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Leike",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems.",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Assessing the effect of inconsistent assessors on summarization evaluation",
                "authors": [
                    {
                        "first": "Karolina",
                        "middle": [],
                        "last": "Owczarzak",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "A"
                        ],
                        "last": "Rankel",
                        "suffix": ""
                    },
                    {
                        "first": "Hoa",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Trang",
                        "middle": [],
                        "last": "Dang",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "M"
                        ],
                        "last": "Conroy",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "359--362",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karolina Owczarzak, Peter A. Rankel, Hoa Trang Dang, and John M. Conroy. 2012. Assessing the effect of inconsistent assessors on summarization evaluation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 359-362, Jeju Island, Korea. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Text generation by learning from demonstrations",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Yuanzhe",
                        "suffix": ""
                    },
                    {
                        "first": "Pang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "He",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Yuanzhe Pang and He He. 2021. Text genera- tion by learning from demonstrations. In 9th Inter- national Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {
                    "DOI": [
                        "10.3115/1073083.1073135"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "Studying summarization evaluation metrics in the appropriate scoring range",
                "authors": [
                    {
                        "first": "Maxime",
                        "middle": [],
                        "last": "Peyrard",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "5093--5100",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1502"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maxime Peyrard. 2019. Studying summarization eval- uation metrics in the appropriate scoring range. In Proceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 5093- 5100, Florence, Italy. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "chrF: character n-gram F-score for automatic MT evaluation",
                "authors": [
                    {
                        "first": "Maja",
                        "middle": [],
                        "last": "Popovi\u0107",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "392--395",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W15-3049"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maja Popovi\u0107. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "A decade of automatic content evaluation of news summaries: Reassessing the state of the art",
                "authors": [
                    {
                        "first": "Peter",
                        "middle": [
                            "A"
                        ],
                        "last": "Rankel",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "M"
                        ],
                        "last": "Conroy",
                        "suffix": ""
                    },
                    {
                        "first": "Hoa",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Trang",
                        "middle": [],
                        "last": "Dang",
                        "suffix": ""
                    },
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "131--136",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter A. Rankel, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. 2013. A decade of automatic content evaluation of news summaries: Reassessing the state of the art. In Proceedings of the 51st Annual Meet- ing of the Association for Computational Linguistics (Volume 2: Short Papers), pages 131-136, Sofia, Bul- garia. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "Efficient online scalar annotation with bounded support",
                "authors": [
                    {
                        "first": "Keisuke",
                        "middle": [],
                        "last": "Sakaguchi",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Van Durme",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "208--218",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P18-1020"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Keisuke Sakaguchi and Benjamin Van Durme. 2018. Ef- ficient online scalar annotation with bounded support. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 208-218, Melbourne, Australia. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF69": {
                "ref_id": "b69",
                "title": "Multitask prompted training enables zero-shot task generalization",
                "authors": [
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Sanh",
                        "suffix": ""
                    },
                    {
                        "first": "Albert",
                        "middle": [],
                        "last": "Webson",
                        "suffix": ""
                    },
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Bach",
                        "suffix": ""
                    },
                    {
                        "first": "Lintang",
                        "middle": [],
                        "last": "Sutawika",
                        "suffix": ""
                    },
                    {
                        "first": "Zaid",
                        "middle": [],
                        "last": "Alyafeai",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Chaffin",
                        "suffix": ""
                    },
                    {
                        "first": "Arnaud",
                        "middle": [],
                        "last": "Stiegler",
                        "suffix": ""
                    },
                    {
                        "first": "Arun",
                        "middle": [],
                        "last": "Raja",
                        "suffix": ""
                    },
                    {
                        "first": "Manan",
                        "middle": [],
                        "last": "Dey",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Saiful Bari",
                        "suffix": ""
                    },
                    {
                        "first": "Canwen",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Urmish",
                        "middle": [],
                        "last": "Thakker",
                        "suffix": ""
                    },
                    {
                        "first": "Shanya",
                        "middle": [],
                        "last": "Sharma Sharma",
                        "suffix": ""
                    },
                    {
                        "first": "Eliza",
                        "middle": [],
                        "last": "Szczechla",
                        "suffix": ""
                    },
                    {
                        "first": "Taewoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Gunjan",
                        "middle": [],
                        "last": "Chhablani",
                        "suffix": ""
                    },
                    {
                        "first": "Nihal",
                        "middle": [],
                        "last": "Nayak",
                        "suffix": ""
                    },
                    {
                        "first": "Debajyoti",
                        "middle": [],
                        "last": "Datta",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Tian-Jian",
                        "suffix": ""
                    },
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Matteo",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Sheng",
                        "middle": [],
                        "last": "Manica",
                        "suffix": ""
                    },
                    {
                        "first": "Zheng Xin",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Harshit",
                        "middle": [],
                        "last": "Yong",
                        "suffix": ""
                    },
                    {
                        "first": "Rachel",
                        "middle": [],
                        "last": "Pandey",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Bawden",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Tr- ishala Neeraj, Jos Rozen, Abheesht Sharma, An- drea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multi- task prompted training enables zero-shot task gener- alization. In International Conference on Learning Representations.",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "QuestEval: Summarization asks for fact-based evaluation",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Scialom",
                        "suffix": ""
                    },
                    {
                        "first": "Paul-Alexis",
                        "middle": [],
                        "last": "Dray",
                        "suffix": ""
                    },
                    {
                        "first": "Sylvain",
                        "middle": [],
                        "last": "Lamprier",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Piwowarski",
                        "suffix": ""
                    },
                    {
                        "first": "Jacopo",
                        "middle": [],
                        "last": "Staiano",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Gallinari",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "6594--6604",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.emnlp-main.529"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. QuestEval: Summariza- tion asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natu- ral Language Processing, pages 6594-6604, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF71": {
                "ref_id": "b71",
                "title": "Answers unite! unsupervised metrics for reinforced summarization models",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Scialom",
                        "suffix": ""
                    },
                    {
                        "first": "Sylvain",
                        "middle": [],
                        "last": "Lamprier",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Piwowarski",
                        "suffix": ""
                    },
                    {
                        "first": "Jacopo",
                        "middle": [],
                        "last": "Staiano",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "3246--3256",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1320"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Thomas Scialom, Sylvain Lamprier, Benjamin Pi- wowarski, and Jacopo Staiano. 2019. Answers unite! unsupervised metrics for reinforced summarization models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3246-3256, Hong Kong, China. Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "Crowdsourcing lightweight pyramids for manual summary evaluation",
                "authors": [
                    {
                        "first": "Ori",
                        "middle": [],
                        "last": "Shapira",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Gabay",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Hadar",
                        "middle": [],
                        "last": "Ronen",
                        "suffix": ""
                    },
                    {
                        "first": "Ramakanth",
                        "middle": [],
                        "last": "Pasunuru",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    },
                    {
                        "first": "Yael",
                        "middle": [],
                        "last": "Amsterdamer",
                        "suffix": ""
                    },
                    {
                        "first": "Ido",
                        "middle": [],
                        "last": "Dagan",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter",
                "volume": "1",
                "issue": "",
                "pages": "682--687",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1072"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ra- makanth Pasunuru, Mohit Bansal, Yael Amsterdamer, and Ido Dagan. 2019. Crowdsourcing lightweight pyramids for manual summary evaluation. In Pro- ceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 682-687, Min- neapolis, Minnesota. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF73": {
                "ref_id": "b73",
                "title": "A new approach to overgenerating and scoring abstractive summaries",
                "authors": [
                    {
                        "first": "Kaiqiang",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Bingqing",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "1392--1404",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.naacl-main.110"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kaiqiang Song, Bingqing Wang, Zhe Feng, and Fei Liu. 2021. A new approach to overgenerating and scoring abstractive summaries. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1392-1404, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF74": {
                "ref_id": "b74",
                "title": "How to evaluate a summarizer: Study design and statistical analysis for manual linguistic quality evaluation",
                "authors": [
                    {
                        "first": "Julius",
                        "middle": [],
                        "last": "Steen",
                        "suffix": ""
                    },
                    {
                        "first": "Katja",
                        "middle": [],
                        "last": "Markert",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 16th Conference of the European Chapter",
                "volume": "",
                "issue": "",
                "pages": "1861--1875",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.eacl-main.160"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Julius Steen and Katja Markert. 2021. How to evaluate a summarizer: Study design and statistical analysis for manual linguistic quality evaluation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1861-1875, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF75": {
                "ref_id": "b75",
                "title": "Learning to summarize from human feedback",
                "authors": [
                    {
                        "first": "Nisan",
                        "middle": [],
                        "last": "Stiennon",
                        "suffix": ""
                    },
                    {
                        "first": "Long",
                        "middle": [],
                        "last": "Ouyang",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "M"
                        ],
                        "last": "Ziegler",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Chelsea",
                        "middle": [],
                        "last": "Voss",
                        "suffix": ""
                    },
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Christiano",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. In Proceedings of the 34th International Conference on Neural In- formation Processing Systems, NIPS'20, Red Hook, NY, USA. Curran Associates Inc.",
                "links": null
            },
            "BIBREF76": {
                "ref_id": "b76",
                "title": "How to compare summarizers without target length? pitfalls, solutions and re-examination of the neural summarization literature",
                "authors": [
                    {
                        "first": "Simeng",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Ori",
                        "middle": [],
                        "last": "Shapira",
                        "suffix": ""
                    },
                    {
                        "first": "Ido",
                        "middle": [],
                        "last": "Dagan",
                        "suffix": ""
                    },
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation",
                "volume": "",
                "issue": "",
                "pages": "21--29",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W19-2303"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Simeng Sun, Ori Shapira, Ido Dagan, and Ani Nenkova. 2019. How to compare summarizers without target length? pitfalls, solutions and re-examination of the neural summarization literature. In Proceedings of the Workshop on Methods for Optimizing and Eval- uating Neural Language Generation, pages 21-29, Minneapolis, Minnesota. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF77": {
                "ref_id": "b77",
                "title": "Evaluating the factual consistency of large language models through summarization",
                "authors": [
                    {
                        "first": "Derek",
                        "middle": [],
                        "last": "Tam",
                        "suffix": ""
                    },
                    {
                        "first": "Anisha",
                        "middle": [],
                        "last": "Mascarenhas",
                        "suffix": ""
                    },
                    {
                        "first": "Shiyue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Sarah",
                        "middle": [],
                        "last": "Kwan",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    },
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and Colin Raffel. 2022. Eval- uating the factual consistency of large language models through summarization. ArXiv preprint, abs/2211.08412.",
                "links": null
            },
            "BIBREF78": {
                "ref_id": "b78",
                "title": "2022a. Understanding factual errors in summarization: Errors, summarizers, datasets",
                "authors": [
                    {
                        "first": "Liyan",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Tanya",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "R"
                        ],
                        "last": "Fabbri",
                        "suffix": ""
                    },
                    {
                        "first": "Philippe",
                        "middle": [],
                        "last": "Laban",
                        "suffix": ""
                    },
                    {
                        "first": "Jiacheng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Semih",
                        "middle": [],
                        "last": "Yahvuz",
                        "suffix": ""
                    },
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Kry\u015bci\u0144ski",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [
                            "F"
                        ],
                        "last": "Rousseau",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Durrett",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Liyan Tang, Tanya Goyal, Alexander R. Fabbri, Philippe Laban, Jiacheng Xu, Semih Yahvuz, Wojciech Kry\u015b- ci\u0144ski, Justin F. Rousseau, and Greg Durrett. 2022a. Understanding factual errors in summarization: Er- rors, summarizers, datasets, error detectors.",
                "links": null
            },
            "BIBREF79": {
                "ref_id": "b79",
                "title": "Yashar Mehdad, and Dragomir Radev. 2022b. Investigating crowdsourcing protocols for evaluating the factual consistency of summaries",
                "authors": [
                    {
                        "first": "Xiangru",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Fabbri",
                        "suffix": ""
                    },
                    {
                        "first": "Haoran",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Ziming",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Griffin",
                        "middle": [],
                        "last": "Adams",
                        "suffix": ""
                    },
                    {
                        "first": "Borui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Asli",
                        "middle": [],
                        "last": "Celikyilmaz",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "5680--5692",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2022.naacl-main.417"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xiangru Tang, Alexander Fabbri, Haoran Li, Ziming Mao, Griffin Adams, Borui Wang, Asli Celikyilmaz, Yashar Mehdad, and Dragomir Radev. 2022b. Inves- tigating crowdsourcing protocols for evaluating the factual consistency of summaries. In Proceedings of the 2022 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, pages 5680-5692, Seattle, United States. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF80": {
                "ref_id": "b80",
                "title": "An introduction to the bootstrap",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Robert",
                        "suffix": ""
                    },
                    {
                        "first": "Bradley",
                        "middle": [],
                        "last": "Tibshirani",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Efron",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Monographs on statistics and applied probability",
                "volume": "57",
                "issue": "",
                "pages": "1--436",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robert J Tibshirani and Bradley Efron. 1993. An intro- duction to the bootstrap. Monographs on statistics and applied probability, 57:1-436.",
                "links": null
            },
            "BIBREF81": {
                "ref_id": "b81",
                "title": "Fill in the BLANC: Human-free quality estimation of document summaries",
                "authors": [
                    {
                        "first": "Oleg",
                        "middle": [],
                        "last": "Vasilyev",
                        "suffix": ""
                    },
                    {
                        "first": "Vedant",
                        "middle": [],
                        "last": "Dharnidharka",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Bohannon",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems",
                "volume": "",
                "issue": "",
                "pages": "11--20",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.eval4nlp-1.2"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Oleg Vasilyev, Vedant Dharnidharka, and John Bohan- non. 2020. Fill in the BLANC: Human-free quality estimation of document summaries. In Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, pages 11-20, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF82": {
                "ref_id": "b82",
                "title": "Cider: Consensus-based image description evaluation",
                "authors": [
                    {
                        "first": "C",
                        "middle": [
                            "Lawrence"
                        ],
                        "last": "Ramakrishna Vedantam",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Zitnick",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015",
                "volume": "",
                "issue": "",
                "pages": "4566--4575",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2015.7299087"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image descrip- tion evaluation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 4566-4575. IEEE Computer Society.",
                "links": null
            },
            "BIBREF83": {
                "ref_id": "b83",
                "title": "Asking and answering questions to evaluate the factual consistency of summaries",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "5008--5020",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.450"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the fac- tual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics, pages 5008-5020, Online. Asso- ciation for Computational Linguistics.",
                "links": null
            },
            "BIBREF84": {
                "ref_id": "b84",
                "title": "The statistical advantage of automatic NLG metrics at the system level",
                "authors": [
                    {
                        "first": "Johnny",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Jia",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "6840--6854",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.acl-long.533"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Johnny Wei and Robin Jia. 2021. The statistical advan- tage of automatic NLG metrics at the system level. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), pages 6840-6854, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF85": {
                "ref_id": "b85",
                "title": "Controllable abstractive dialogue summarization with sketch supervision",
                "authors": [
                    {
                        "first": "Chien-Sheng",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Linqing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "volume": "",
                "issue": "",
                "pages": "5108--5122",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.findings-acl.454"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, and Caiming Xiong. 2021. Controllable abstractive dialogue summarization with sketch su- pervision. In Findings of the Association for Com- putational Linguistics: ACL-IJCNLP 2021, pages 5108-5122, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF86": {
                "ref_id": "b86",
                "title": "Bartscore: Evaluating generated text as text generation",
                "authors": [
                    {
                        "first": "Weizhe",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "34",
                "issue": "",
                "pages": "27263--27277",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text genera- tion. In Advances in Neural Information Processing Systems, volume 34, pages 27263-27277. Curran As- sociates, Inc.",
                "links": null
            },
            "BIBREF87": {
                "ref_id": "b87",
                "title": "PEGASUS: pre-training with extracted gap-sentences for abstractive summarization",
                "authors": [
                    {
                        "first": "Jingqing",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yao",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Saleh",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020",
                "volume": "13",
                "issue": "",
                "pages": "11328--11339",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe- ter J. Liu. 2020a. PEGASUS: pre-training with ex- tracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 11328-11339. PMLR.",
                "links": null
            },
            "BIBREF88": {
                "ref_id": "b88",
                "title": "PEGASUS: pre-training with extracted gap-sentences for abstractive summarization",
                "authors": [
                    {
                        "first": "Jingqing",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yao",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Saleh",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020",
                "volume": "13",
                "issue": "",
                "pages": "11328--11339",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe- ter J. Liu. 2020b. PEGASUS: pre-training with ex- tracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 11328-11339. PMLR.",
                "links": null
            },
            "BIBREF89": {
                "ref_id": "b89",
                "title": "Finding a balanced degree of automation for summary evaluation",
                "authors": [
                    {
                        "first": "Shiyue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "6617--6632",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.emnlp-main.531"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shiyue Zhang and Mohit Bansal. 2021. Finding a bal- anced degree of automation for summary evaluation. In Proceedings of the 2021 Conference on Empiri- cal Methods in Natural Language Processing, pages 6617-6632, Online and Punta Cana, Dominican Re- public. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF90": {
                "ref_id": "b90",
                "title": "Bertscore: Evaluating text generation with BERT",
                "authors": [
                    {
                        "first": "Tianyi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Varsha",
                        "middle": [],
                        "last": "Kishore",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [
                            "Q"
                        ],
                        "last": "Weinberger",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "th International Conference on Learning Representations, ICLR 2020",
                "volume": "8",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020c. Bertscore: Eval- uating text generation with BERT. In 8th Inter- national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
                "links": null
            },
            "BIBREF91": {
                "ref_id": "b91",
                "title": "DIALOGPT : Largescale generative pre-training for conversational response generation",
                "authors": [
                    {
                        "first": "Yizhe",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Siqi",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Yen-Chun",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    },
                    {
                        "first": "Xiang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Dolan",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "270--278",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-demos.30"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020d. DIALOGPT : Large- scale generative pre-training for conversational re- sponse generation. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 270-278, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF92": {
                "ref_id": "b92",
                "title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Maxime",
                        "middle": [],
                        "last": "Peyrard",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [
                            "M"
                        ],
                        "last": "Meyer",
                        "suffix": ""
                    },
                    {
                        "first": "Steffen",
                        "middle": [],
                        "last": "Eger",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "563--578",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1053"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF93": {
                "ref_id": "b93",
                "title": "Extractive summarization as text matching",
                "authors": [
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yiran",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Danqing",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "6197--6208",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.552"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. 2020. Extractive summarization as text matching. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197-6208, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF94": {
                "ref_id": "b94",
                "title": "Towards a unified multidimensional evaluator for text generation",
                "authors": [
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Da",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Yuning",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Yizhu",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Chenguang",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Ji",
                        "middle": [],
                        "last": "Heng",
                        "suffix": ""
                    },
                    {
                        "first": "Jiawei",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multi- dimensional evaluator for text generation.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "uris": null,
                "fig_num": "1",
                "text": "Figure 1: Example of a reference summary, a system summary and corresponding ACU annotations on CNNDM.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF1": {
                "uris": null,
                "fig_num": "3",
                "text": "Figure 3: Power analysis of pair-wise metric comparison w.r.t. their system-level Kendall's correlation coefficients with ACU scores on CNNDM. The metric pairs are grouped by the correlation differences with ACU scores. Different lines represent different sample sizes.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF2": {
                "uris": null,
                "fig_num": "45",
                "text": "Figure 4: Power analysis of human evaluation for system comparison on the annotated CNNDM test examples. Different lines represent results with different sample sizes. The system pairs are grouped by performance differences in ACU scores in Fig.4a, and by ROUGE1 recall scores in Fig.4b.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF3": {
                "uris": null,
                "fig_num": "6",
                "text": "Figure 6: Power analysis of human evaluation for system comparison on the annotated SamSum test examples. Different lines represent results with different sample sizes. The system pairs are grouped by performance differences in ACU scores in Fig.6a, and by ROUGE1 recall scores in Fig.6b.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF5": {
                "uris": null,
                "fig_num": "7",
                "text": "Figure 7: Power analysis of human evaluation for system comparison under different evaluation protocols on the annotated CNNDM test examples. Different lines represent results with different sample sizes. The system pairs are grouped into 10 buckets with similar sizes based on their performance difference under human evaluation. Fig.7a corresponds to the Prior protocol, Fig.7b the Ref-free protocol, Fig.7c the Ref-based protocol, and Fig.7d the ACU protocol with normalized ACU scores.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF6": {
                "uris": null,
                "fig_num": null,
                "text": "\u2713 \u2713 \u2022 David Ospina clashed with Oscar. \u2713 \u2713 \u2022 David Ospina clattered Oscar. \u2713 \u2713 \u2022 David Ospina plays for Arsenal. \u2713 \u2717 \u2022 David Ospina is a goalkeeper. \u2713 \u2717 \u2022 The clash occurred inside the box. \u2717 \u2717 \u2022 Oscar is Brazilian. \u2713 \u2717 \u2022 Oscar was taken off at half time. \u2713 \u2717 \u2022 Didier Drogba replaced Oscar. \u2717 \u2717",
                "type_str": "figure",
                "num": null
            },
            "FIGREF8": {
                "uris": null,
                "fig_num": "8",
                "text": "Figure 8: Confidence intervals of the system-level Kendall's correlation coefficients between automatic metrics and ACU scores.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF9": {
                "uris": null,
                "fig_num": "9",
                "text": "Figure 9: Confidence intervals of the system-level Kendall's correlation coefficients between ROUGE1 recall scores and ACU scores under different sample sizes.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF10": {
                "uris": null,
                "fig_num": "10",
                "text": "Figure 10: Sample size of the conducted human evaluation study in recent text summarization research based on our survey (Appendix F).",
                "type_str": "figure",
                "num": null
            },
            "TABREF0": {
                "text": "Summary of the key findings in our work.",
                "content": "<table/>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF2": {
                "text": "",
                "content": "<table/>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF5": {
                "text": "Summarization system analysis on CNNDM. ACU is the ACU score (Eq. 1), nACU is the normalized ACU score (Eq. 2), Len is the average summary length, and R1F is the ROUGE1 F1 score. ACU and nACU are calculated on the 500 annotated examples (the value is multiplied by 100) while Len and R1F are calculated on the entire test set. The systems are sorted by Len.",
                "content": "<table/>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF6": {
                "text": "Prior Ref-free Ref-based nACU System-level Pearson's correlations between different protocols on the fine-tuned models. nACU is the normalized ACU score. Len. is the summary length.",
                "content": "<table><tr><td>Prior</td><td>-</td><td>0.926</td><td>-0.061</td><td>0.048</td></tr><tr><td>Ref-free</td><td>0.926</td><td>-</td><td>-0.247</td><td>-0.093</td></tr><tr><td colspan=\"3\">Ref-based -0.061 -0.247</td><td>-</td><td>0.762</td></tr><tr><td>nACU</td><td>0.048</td><td>-0.093</td><td>0.762</td><td>-</td></tr><tr><td>Len.</td><td>0.833</td><td>0.875</td><td>-0.550</td><td>-0.296</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF7": {
                "text": "Prior Ref-free Ref-based ACU Len. Model performance under different annotation protocols. Len. is the summary length. Ref. is the reference summary. Prior, Ref-free, Ref-based protocols have a score range from 1 to 5.",
                "content": "<table><tr><td colspan=\"2\">BART 3.58</td><td>3.52</td><td>2.93</td><td colspan=\"2\">0.367 69.5</td></tr><tr><td>BRIO</td><td>3.51</td><td>3.49</td><td>3.07</td><td colspan=\"2\">0.429 66.4</td></tr><tr><td>T0</td><td>3.33</td><td>3.24</td><td>2.84</td><td colspan=\"2\">0.295 61.6</td></tr><tr><td colspan=\"2\">GPT-3 3.72</td><td>3.76</td><td>2.74</td><td colspan=\"2\">0.268 69.5</td></tr><tr><td>Ref.</td><td>2.85</td><td>2.94</td><td>-</td><td>-</td><td>54.9</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF8": {
                "text": "Pyramid .849 .452 .714  .245 1.00 .467 GPTScore",
                "content": "<table><tr><td/><td>CNNDM</td><td>XSum</td><td>SamSum</td></tr><tr><td/><td colspan=\"3\">Sys. Sum. Sys. Sum. Sys. Sum.</td></tr><tr><td>ROUGE1</td><td colspan=\"3\">.788 .468 .714 .293 .929 .439</td></tr><tr><td>ROUGE2</td><td colspan=\"3\">.758 .453 .643 .266 1.00 .395</td></tr><tr><td>ROUGEL</td><td colspan=\"3\">.879 .454 .643 .258 .929 .415</td></tr><tr><td>METEOR</td><td colspan=\"3\">.758 .407 .571 .268 .857 .373</td></tr><tr><td>CHRF</td><td colspan=\"3\">.758 .436 .571 .275 .857 .396</td></tr><tr><td>BERTScore</td><td colspan=\"3\">.515 .448 .571 .277 .857 .417</td></tr><tr><td>BARTScore</td><td colspan=\"3\">.727 .453 .714 .282 .929 .430</td></tr><tr><td>QAEval</td><td colspan=\"3\">.849 .358 .429 .198 .929 .384</td></tr><tr><td>SummaQA</td><td colspan=\"3\">.727 .119 .143 .019 .643 .102</td></tr><tr><td>Lite 3</td><td/><td/></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF10": {
                "text": "The system-level Kendall's correlation between the automatic metric and ACU scores on different system pairs grouped by their ACU score differences on the CNNDM dataset, into six equal-sized buckets. We use the recall score of the automatic metrics when available.",
                "content": "<table/>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF12": {
                "text": "The system-level Kendall's correlation between the automatic metric and different human evaluation protocols on CNNDM dataset. We use the F1 score of the automatic metrics when available.",
                "content": "<table/>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF15": {
                "text": "",
                "content": "<table><tr><td>: Human evaluation protocol comparison. We</td></tr><tr><td>categorize the different protocols based on if they (1)</td></tr><tr><td>require the input document (w/ Doc), (2) require the</td></tr><tr><td>reference summary (w/ Ref), and (3) are fine-grained.</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF16": {
                "text": "Summary-level  Pearson correlations between different protocols on the fine-tuned models. nACU is the normalized ACU score. Len. is the Summary length.",
                "content": "<table><tr><td/><td colspan=\"4\">Prior Ref-free Ref-based nACU</td></tr><tr><td>Prior</td><td>-</td><td>0.526</td><td>0.056</td><td>0.082</td></tr><tr><td>Ref-free</td><td colspan=\"2\">0.526 -</td><td>0.070</td><td>0.075</td></tr><tr><td colspan=\"3\">Ref-based 0.056 0.070</td><td>-</td><td>0.355</td></tr><tr><td>nACU</td><td colspan=\"2\">0.082 0.075</td><td>0.355</td><td>-</td></tr><tr><td>Len.</td><td colspan=\"2\">0.431 0.545</td><td>-0.107</td><td>-0.007</td></tr><tr><td colspan=\"4\">we use the Python implementation. 14</td><td/></tr><tr><td colspan=\"5\">BLEU (Papineni et al., 2002) is a corpus-level</td></tr><tr><td colspan=\"5\">precision-focused metric that calculates n-gram</td></tr><tr><td colspan=\"4\">overlap and includes a brevity penalty.</td><td/></tr><tr><td colspan=\"5\">CIDEr (Vedantam et al., 2015) computes {1-4}-</td></tr><tr><td colspan=\"5\">gram co-occurrences, down-weighting common n-</td></tr><tr><td colspan=\"5\">grams and calculating cosine similarity between</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF18": {
                "text": "Survey of human evaluation practices in recent text summarization research.study in Fig.10, and note that around 93% of them are less or equal to 200. Released Human Evaluation Data: Report \"yes\", if the authors release the human evaluation data.",
                "content": "<table><tr><td>Best Practice &amp; Implementation</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF19": {
                "text": "The system-level Pearson's r, Spearman's \u03c1, and Kendall's \u03c4 correlation coefficients between the automatic metric scores and un-normalized ACU scores of system outputs on CNNDM, XSum and SamSum datasets.",
                "content": "<table><tr><td/><td/><td>CNNDM</td><td/><td/><td>XSum</td><td/><td/><td>SamSum</td><td/></tr><tr><td/><td>r</td><td>\u03c1</td><td>\u03c4</td><td>r</td><td>\u03c1</td><td>\u03c4</td><td>r</td><td>\u03c1</td><td>\u03c4</td></tr><tr><td>BARTScore-f1-cnndm</td><td>0.353</td><td>0.329</td><td>0.264</td><td>0.261</td><td>0.238</td><td>0.202</td><td>0.434</td><td>0.401</td><td>0.340</td></tr><tr><td colspan=\"2\">BARTScore-f1-parabank 0.417</td><td>0.386</td><td>0.311</td><td>0.309</td><td>0.279</td><td>0.239</td><td>0.430</td><td>0.396</td><td>0.340</td></tr><tr><td>BARTScore-p-cnndm</td><td>0.178</td><td>0.161</td><td>0.128</td><td>0.188</td><td>0.166</td><td>0.140</td><td>0.282</td><td>0.263</td><td>0.224</td></tr><tr><td>BARTScore-p-parabank</td><td>0.237</td><td>0.216</td><td>0.170</td><td>0.235</td><td>0.220</td><td>0.187</td><td>0.269</td><td>0.249</td><td>0.212</td></tr><tr><td>BARTScore-r-cnndm</td><td>0.567</td><td>0.530</td><td>0.435</td><td>0.325</td><td>0.300</td><td>0.260</td><td>0.546</td><td>0.508</td><td>0.438</td></tr><tr><td>BARTScore-r-parabank</td><td>0.582</td><td>0.548</td><td>0.453</td><td>0.353</td><td>0.326</td><td>0.282</td><td>0.531</td><td>0.500</td><td>0.430</td></tr><tr><td>BERTScore-f1-deberta</td><td>0.441</td><td>0.413</td><td>0.334</td><td>0.290</td><td>0.280</td><td>0.241</td><td>0.401</td><td>0.377</td><td>0.326</td></tr><tr><td>BERTScore-f1-roberta</td><td>0.432</td><td>0.397</td><td>0.320</td><td>0.305</td><td>0.285</td><td>0.244</td><td>0.415</td><td>0.388</td><td>0.335</td></tr><tr><td>BERTScore-p-deberta</td><td>0.255</td><td>0.239</td><td>0.191</td><td>0.209</td><td>0.211</td><td>0.180</td><td>0.208</td><td>0.204</td><td>0.173</td></tr><tr><td>BERTScore-p-roberta</td><td>0.218</td><td>0.200</td><td>0.160</td><td>0.223</td><td>0.221</td><td>0.190</td><td>0.212</td><td>0.209</td><td>0.178</td></tr><tr><td>BERTScore-r-deberta</td><td>0.544</td><td>0.516</td><td>0.424</td><td>0.327</td><td>0.305</td><td>0.262</td><td>0.507</td><td>0.476</td><td>0.409</td></tr><tr><td>BERTScore-r-roberta</td><td>0.571</td><td>0.542</td><td>0.448</td><td>0.348</td><td>0.320</td><td>0.277</td><td>0.516</td><td>0.481</td><td>0.417</td></tr><tr><td>BLANC</td><td>0.238</td><td>0.220</td><td colspan=\"5\">0.175 -0.018 -0.022 -0.020 0.167</td><td>0.156</td><td>0.136</td></tr><tr><td>BLEU</td><td>0.337</td><td>0.306</td><td>0.246</td><td>0.275</td><td>0.259</td><td>0.227</td><td>0.373</td><td>0.356</td><td>0.306</td></tr><tr><td>CHRF</td><td>0.564</td><td>0.528</td><td>0.436</td><td>0.353</td><td>0.315</td><td>0.275</td><td>0.486</td><td>0.459</td><td>0.396</td></tr><tr><td>Compression</td><td colspan=\"9\">-0.309 -0.296 -0.238 -0.088 -0.080 -0.071 -0.312 -0.307 -0.269</td></tr><tr><td>Coverage</td><td>0.012</td><td>0.005</td><td colspan=\"5\">0.003 -0.045 -0.044 -0.037 0.056</td><td>0.044</td><td>0.037</td></tr><tr><td>CTC</td><td>0.453</td><td>0.431</td><td>0.348</td><td>0.270</td><td>0.249</td><td>0.215</td><td>0.476</td><td>0.442</td><td>0.382</td></tr><tr><td>Density</td><td>0.078</td><td>0.070</td><td colspan=\"5\">0.054 -0.054 -0.052 -0.044 0.119</td><td>0.109</td><td>0.091</td></tr><tr><td>Lite 3 Pyramid-l2c</td><td>0.537</td><td>0.523</td><td>0.466</td><td>0.219</td><td>0.219</td><td>0.207</td><td>0.524</td><td>0.519</td><td>0.494</td></tr><tr><td>Lite 3 Pyramid-l3c</td><td>0.532</td><td>0.521</td><td>0.466</td><td>0.217</td><td>0.214</td><td>0.204</td><td>0.540</td><td>0.535</td><td>0.509</td></tr><tr><td>Lite 3 Pyramid-p2c</td><td>0.582</td><td>0.546</td><td>0.452</td><td>0.303</td><td>0.284</td><td>0.245</td><td>0.599</td><td>0.539</td><td>0.467</td></tr><tr><td>Lite 3 Pyramid-p3c</td><td>0.584</td><td>0.543</td><td>0.448</td><td>0.310</td><td>0.285</td><td>0.246</td><td>0.615</td><td>0.549</td><td>0.475</td></tr><tr><td>Meteor</td><td>0.537</td><td>0.496</td><td>0.407</td><td>0.327</td><td>0.308</td><td>0.268</td><td>0.471</td><td>0.430</td><td>0.373</td></tr><tr><td>MoverScore</td><td>0.388</td><td>0.364</td><td>0.292</td><td>0.320</td><td>0.296</td><td>0.252</td><td>0.398</td><td>0.375</td><td>0.320</td></tr><tr><td>Novel-1gram</td><td colspan=\"4\">-0.008 -0.005 -0.003 0.051</td><td>0.048</td><td colspan=\"4\">0.041 -0.070 -0.066 -0.056</td></tr><tr><td>Novel-2gram</td><td colspan=\"4\">-0.026 -0.035 -0.028 0.057</td><td>0.057</td><td colspan=\"4\">0.050 -0.112 -0.103 -0.087</td></tr><tr><td>Repeated-1gram</td><td>0.071</td><td>0.067</td><td>0.052</td><td>0.010</td><td>0.006</td><td>0.005</td><td>0.172</td><td>0.172</td><td>0.152</td></tr><tr><td>Repeated-2gram</td><td>0.061</td><td>0.060</td><td>0.048</td><td>0.010</td><td>0.006</td><td>0.005</td><td>0.059</td><td>0.057</td><td>0.052</td></tr><tr><td>QAEval-em</td><td>0.350</td><td>0.334</td><td>0.296</td><td>0.159</td><td>0.156</td><td>0.149</td><td>0.383</td><td>0.377</td><td>0.352</td></tr><tr><td>QAEval-f1</td><td>0.454</td><td>0.427</td><td>0.358</td><td>0.226</td><td>0.215</td><td>0.198</td><td>0.437</td><td>0.421</td><td>0.384</td></tr><tr><td>ROUGE1</td><td>0.457</td><td>0.430</td><td>0.348</td><td>0.302</td><td>0.292</td><td>0.253</td><td>0.416</td><td>0.398</td><td>0.345</td></tr><tr><td>ROUGE1p</td><td>0.190</td><td>0.175</td><td>0.140</td><td>0.227</td><td>0.224</td><td>0.194</td><td>0.113</td><td>0.119</td><td>0.103</td></tr><tr><td>ROUGE1r</td><td>0.579</td><td>0.552</td><td>0.468</td><td>0.328</td><td>0.322</td><td>0.293</td><td>0.503</td><td>0.485</td><td>0.439</td></tr><tr><td>ROUGE2</td><td>0.444</td><td>0.407</td><td>0.329</td><td>0.277</td><td>0.255</td><td>0.222</td><td>0.380</td><td>0.350</td><td>0.301</td></tr><tr><td>ROUGE2p</td><td>0.307</td><td>0.287</td><td>0.229</td><td>0.241</td><td>0.229</td><td>0.200</td><td>0.214</td><td>0.210</td><td>0.181</td></tr><tr><td>ROUGE2r</td><td>0.552</td><td>0.529</td><td>0.453</td><td>0.301</td><td>0.291</td><td>0.266</td><td>0.456</td><td>0.436</td><td>0.395</td></tr><tr><td>ROUGEL</td><td>0.430</td><td>0.399</td><td>0.321</td><td>0.266</td><td>0.249</td><td>0.215</td><td>0.395</td><td>0.372</td><td>0.323</td></tr><tr><td>ROUGELp</td><td>0.192</td><td>0.179</td><td>0.143</td><td>0.214</td><td>0.208</td><td>0.180</td><td>0.121</td><td>0.120</td><td>0.103</td></tr><tr><td>ROUGELr</td><td>0.561</td><td>0.537</td><td>0.454</td><td>0.297</td><td>0.285</td><td>0.258</td><td>0.480</td><td>0.460</td><td>0.415</td></tr><tr><td>SimCSE</td><td>0.461</td><td>0.429</td><td>0.346</td><td>0.308</td><td>0.290</td><td>0.248</td><td>0.450</td><td>0.420</td><td>0.360</td></tr><tr><td>SummaQA</td><td>0.165</td><td>0.153</td><td>0.121</td><td>0.022</td><td>0.015</td><td>0.013</td><td>0.045</td><td>0.049</td><td>0.039</td></tr><tr><td>SummaQA-prob</td><td>0.155</td><td>0.150</td><td>0.119</td><td>0.026</td><td>0.023</td><td>0.019</td><td>0.131</td><td>0.120</td><td>0.102</td></tr><tr><td>Summary-length</td><td>0.315</td><td>0.296</td><td>0.238</td><td>0.081</td><td>0.075</td><td>0.067</td><td>0.314</td><td>0.307</td><td>0.268</td></tr><tr><td>SUPERT</td><td>0.211</td><td>0.206</td><td>0.165</td><td>0.047</td><td>0.049</td><td>0.042</td><td>0.191</td><td>0.168</td><td>0.141</td></tr><tr><td>UniEval-coherence</td><td>0.098</td><td>0.127</td><td>0.100</td><td>0.017</td><td>0.011</td><td>0.012</td><td>0.186</td><td>0.197</td><td>0.167</td></tr><tr><td>UniEval-consistency</td><td>0.007</td><td>0.015</td><td>0.010</td><td>0.017</td><td>0.013</td><td>0.013</td><td>0.044</td><td>0.037</td><td>0.031</td></tr><tr><td>UniEval-fluency</td><td colspan=\"9\">-0.008 -0.022 -0.015 -0.031 -0.040 -0.034 -0.006 -0.035 -0.028</td></tr><tr><td>UniEval-overall</td><td>0.111</td><td>0.132</td><td>0.104</td><td>0.089</td><td>0.078</td><td>0.067</td><td>0.171</td><td>0.187</td><td>0.157</td></tr><tr><td>UniEval-relevance</td><td>0.129</td><td>0.154</td><td>0.121</td><td>0.180</td><td>0.181</td><td>0.152</td><td>0.201</td><td>0.235</td><td>0.197</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF20": {
                "text": "The summary-level Pearson's r, Spearman's \u03c1, and Kendall's \u03c4 correlation coefficients between the automatic metric scores and un-normalized ACU scores of system outputs on CNNDM, XSum and SamSum.",
                "content": "<table><tr><td/><td/><td>CNNDM</td><td/><td/><td>XSum</td><td/><td/><td>SamSum</td><td/></tr><tr><td/><td>r</td><td>\u03c1</td><td>\u03c4</td><td>r</td><td>\u03c1</td><td>\u03c4</td><td>r</td><td>\u03c1</td><td>\u03c4</td></tr><tr><td>BARTScore-f1-cnndm</td><td>0.539</td><td>0.706</td><td colspan=\"3\">0.455 -0.148 0.095</td><td>0.071</td><td>0.920</td><td>0.786</td><td>0.643</td></tr><tr><td colspan=\"2\">BARTScore-f1-parabank 0.692</td><td>0.706</td><td>0.455</td><td>0.478</td><td>0.429</td><td>0.429</td><td>0.907</td><td>0.714</td><td>0.571</td></tr><tr><td>BARTScore-p-cnndm</td><td>0.430</td><td>0.524</td><td colspan=\"5\">0.333 -0.622 -0.571 -0.429 0.945</td><td>0.738</td><td>0.643</td></tr><tr><td>BARTScore-p-parabank</td><td>0.421</td><td>0.413</td><td colspan=\"5\">0.242 -0.341 -0.452 -0.286 0.915</td><td>0.691</td><td>0.500</td></tr><tr><td>BARTScore-r-cnndm</td><td>0.461</td><td>0.364</td><td>0.273</td><td>0.893</td><td>0.786</td><td>0.643</td><td>0.774</td><td>0.738</td><td>0.571</td></tr><tr><td>BARTScore-r-parabank</td><td>0.756</td><td>0.615</td><td>0.455</td><td>0.932</td><td>0.881</td><td>0.714</td><td>0.786</td><td>0.738</td><td>0.571</td></tr><tr><td>BERTScore-f1-deberta</td><td>0.643</td><td>0.783</td><td>0.576</td><td>0.593</td><td>0.429</td><td>0.429</td><td>0.985</td><td>0.952</td><td>0.857</td></tr><tr><td>BERTScore-f1-roberta</td><td>0.733</td><td>0.755</td><td>0.545</td><td>0.639</td><td>0.429</td><td>0.429</td><td>0.955</td><td>0.976</td><td>0.929</td></tr><tr><td>BERTScore-p-deberta</td><td>0.335</td><td>0.378</td><td>0.242</td><td>0.191</td><td>0.405</td><td>0.357</td><td>0.756</td><td>0.619</td><td>0.571</td></tr><tr><td>BERTScore-p-roberta</td><td>0.432</td><td>0.420</td><td>0.242</td><td>0.233</td><td>0.333</td><td>0.286</td><td>0.779</td><td>0.667</td><td>0.571</td></tr><tr><td>BERTScore-r-deberta</td><td>0.664</td><td>0.489</td><td>0.333</td><td>0.863</td><td>0.762</td><td>0.571</td><td>0.845</td><td>0.714</td><td>0.500</td></tr><tr><td>BERTScore-r-roberta</td><td>0.725</td><td>0.552</td><td>0.364</td><td>0.909</td><td>0.786</td><td>0.571</td><td>0.802</td><td>0.714</td><td>0.500</td></tr><tr><td>BLANC</td><td colspan=\"6\">-0.122 -0.126 -0.061 0.020 -0.024 0.071</td><td>0.506</td><td>0.452</td><td>0.357</td></tr><tr><td>BLEU</td><td>0.442</td><td>0.482</td><td>0.303</td><td>0.676</td><td>0.595</td><td>0.571</td><td>0.906</td><td>0.905</td><td>0.786</td></tr><tr><td>CHRF</td><td>0.701</td><td>0.601</td><td>0.424</td><td>0.869</td><td>0.762</td><td>0.571</td><td>0.818</td><td>0.714</td><td>0.500</td></tr><tr><td>Compression</td><td colspan=\"5\">-0.099 -0.077 -0.091 -0.128 0.071</td><td colspan=\"4\">0.000 -0.361 -0.357 -0.214</td></tr><tr><td>Coverage</td><td colspan=\"7\">-0.599 -0.797 -0.576 -0.603 -0.571 -0.429 0.606</td><td>0.381</td><td>0.286</td></tr><tr><td>CTC</td><td colspan=\"4\">-0.074 -0.035 -0.030 0.350</td><td>0.405</td><td>0.214</td><td>0.792</td><td>0.738</td><td>0.571</td></tr><tr><td>Density</td><td colspan=\"7\">-0.366 -0.685 -0.485 -0.427 -0.381 -0.286 0.574</td><td>0.286</td><td>0.214</td></tr><tr><td>Lite 3 Pyramid-l2c</td><td>0.501</td><td>0.462</td><td>0.273</td><td>0.903</td><td>0.809</td><td>0.643</td><td>0.856</td><td>0.786</td><td>0.643</td></tr><tr><td>Lite 3 Pyramid-l3c</td><td>0.486</td><td>0.448</td><td>0.273</td><td>0.908</td><td>0.809</td><td>0.643</td><td>0.845</td><td>0.786</td><td>0.643</td></tr><tr><td>Lite 3 Pyramid-p2c</td><td>0.510</td><td>0.462</td><td>0.273</td><td>0.915</td><td>0.833</td><td>0.714</td><td>0.847</td><td>0.786</td><td>0.643</td></tr><tr><td>Lite 3 Pyramid-p3c</td><td>0.498</td><td>0.503</td><td>0.303</td><td>0.921</td><td>0.809</td><td>0.643</td><td>0.840</td><td>0.786</td><td>0.643</td></tr><tr><td>Meteor</td><td>0.744</td><td>0.601</td><td>0.424</td><td>0.901</td><td>0.762</td><td>0.571</td><td>0.796</td><td>0.714</td><td>0.500</td></tr><tr><td>MoverScore</td><td>0.540</td><td>0.594</td><td>0.394</td><td>0.718</td><td>0.571</td><td>0.500</td><td>0.879</td><td>0.905</td><td>0.786</td></tr><tr><td>Novel-1gram</td><td>0.637</td><td>0.811</td><td>0.636</td><td>0.635</td><td>0.452</td><td colspan=\"4\">0.357 -0.594 -0.381 -0.286</td></tr><tr><td>Novel-2gram</td><td>0.593</td><td>0.769</td><td>0.576</td><td>0.612</td><td>0.619</td><td colspan=\"4\">0.429 -0.609 -0.429 -0.286</td></tr><tr><td>Repeated-1gram</td><td>0.357</td><td>0.224</td><td>0.182</td><td>0.119</td><td>0.095</td><td>0.143</td><td>0.102</td><td>0.000</td><td>0.000</td></tr><tr><td>Repeated-2gram</td><td>0.382</td><td>0.252</td><td>0.182</td><td>0.243</td><td>0.119</td><td colspan=\"4\">0.143 -0.211 -0.095 -0.071</td></tr><tr><td>QAEval-em</td><td>0.408</td><td>0.350</td><td>0.242</td><td>0.489</td><td>0.452</td><td>0.357</td><td>0.909</td><td>0.786</td><td>0.643</td></tr><tr><td>QAEval-f1</td><td>0.602</td><td>0.489</td><td>0.333</td><td>0.588</td><td>0.500</td><td>0.429</td><td>0.894</td><td>0.714</td><td>0.571</td></tr><tr><td>ROUGE1</td><td>0.915</td><td>0.881</td><td>0.788</td><td>0.704</td><td>0.476</td><td>0.357</td><td>0.909</td><td>0.786</td><td>0.643</td></tr><tr><td>ROUGE1p</td><td>0.272</td><td>0.329</td><td>0.182</td><td>0.257</td><td>0.405</td><td>0.357</td><td>0.548</td><td>0.524</td><td>0.429</td></tr><tr><td>ROUGE1r</td><td>0.516</td><td>0.413</td><td>0.273</td><td>0.737</td><td>0.857</td><td>0.714</td><td>0.644</td><td>0.738</td><td>0.571</td></tr><tr><td>ROUGE2</td><td>0.770</td><td>0.699</td><td>0.515</td><td>0.665</td><td>0.500</td><td>0.429</td><td>0.961</td><td>0.881</td><td>0.714</td></tr><tr><td>ROUGE2p</td><td>0.317</td><td>0.406</td><td>0.242</td><td>0.382</td><td>0.429</td><td>0.429</td><td>0.839</td><td>0.833</td><td>0.714</td></tr><tr><td>ROUGE2r</td><td>0.651</td><td>0.510</td><td>0.364</td><td>0.893</td><td>0.786</td><td>0.643</td><td>0.842</td><td>0.786</td><td>0.643</td></tr><tr><td>ROUGEL</td><td>0.811</td><td>0.790</td><td>0.606</td><td>0.620</td><td>0.429</td><td>0.429</td><td>0.897</td><td>0.857</td><td>0.714</td></tr><tr><td>ROUGELp</td><td>0.275</td><td>0.280</td><td>0.151</td><td>0.243</td><td>0.357</td><td>0.286</td><td>0.660</td><td>0.667</td><td>0.571</td></tr><tr><td>ROUGELr</td><td>0.600</td><td>0.524</td><td>0.364</td><td>0.815</td><td>0.809</td><td>0.643</td><td>0.688</td><td>0.738</td><td>0.571</td></tr><tr><td>SimCSE</td><td>0.801</td><td>0.685</td><td>0.545</td><td>0.876</td><td>0.809</td><td>0.571</td><td>0.847</td><td>0.786</td><td>0.643</td></tr><tr><td>SummaQA</td><td>0.196</td><td>0.133</td><td colspan=\"4\">0.061 -0.239 -0.119 0.000</td><td>0.334</td><td>0.071</td><td>0.071</td></tr><tr><td>SummaQA-prob</td><td>0.591</td><td>0.503</td><td>0.212</td><td>0.251</td><td>0.214</td><td>0.143</td><td>0.416</td><td>0.357</td><td>0.286</td></tr><tr><td>Summary-length</td><td>0.087</td><td>0.042</td><td>0.030</td><td colspan=\"3\">0.166 -0.071 0.000</td><td>0.329</td><td>0.357</td><td>0.214</td></tr><tr><td>SUPERT</td><td colspan=\"7\">-0.233 -0.329 -0.212 -0.057 -0.095 -0.071 0.293</td><td>0.238</td><td>0.071</td></tr><tr><td>UniEval-coherence</td><td colspan=\"4\">-0.620 -0.357 -0.273 0.062</td><td>0.095</td><td>0.071</td><td>0.481</td><td>0.333</td><td>0.286</td></tr><tr><td>UniEval-consistency</td><td colspan=\"4\">-0.534 -0.748 -0.515 0.023</td><td>0.071</td><td>0.071</td><td>0.606</td><td>0.214</td><td>0.000</td></tr><tr><td>UniEval-fluency</td><td>0.286</td><td>0.189</td><td colspan=\"5\">0.121 -0.681 -0.643 -0.500 0.667</td><td>0.500</td><td>0.357</td></tr><tr><td>UniEval-overall</td><td>0.178</td><td>0.126</td><td colspan=\"3\">0.121 -0.072 0.048</td><td>0.071</td><td>0.734</td><td>0.381</td><td>0.286</td></tr><tr><td>UniEval-relevance</td><td>0.365</td><td>0.762</td><td>0.545</td><td>0.064</td><td>0.333</td><td>0.214</td><td>0.717</td><td>0.548</td><td>0.429</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF21": {
                "text": "The system-level Pearson's r, Spearman's \u03c1, and Kendall's \u03c4 correlation coefficients between the automatic metric scores and normalized ACU scores of system outputs on CNNDM, XSum and SamSum datasets.",
                "content": "<table><tr><td/><td/><td>CNNDM</td><td/><td/><td>XSum</td><td/><td/><td>SamSum</td><td/></tr><tr><td/><td>r</td><td>\u03c1</td><td>\u03c4</td><td>r</td><td>\u03c1</td><td>\u03c4</td><td>r</td><td>\u03c1</td><td>\u03c4</td></tr><tr><td>BARTScore-f1-cnndm</td><td>0.398</td><td>0.384</td><td>0.296</td><td>0.276</td><td>0.274</td><td>0.228</td><td>0.445</td><td>0.413</td><td>0.341</td></tr><tr><td colspan=\"2\">BARTScore-f1-parabank 0.465</td><td>0.441</td><td>0.342</td><td>0.327</td><td>0.310</td><td>0.260</td><td>0.483</td><td>0.442</td><td>0.371</td></tr><tr><td>BARTScore-p-cnndm</td><td>0.284</td><td>0.273</td><td>0.209</td><td>0.205</td><td>0.208</td><td>0.172</td><td>0.389</td><td>0.355</td><td>0.295</td></tr><tr><td>BARTScore-p-parabank</td><td>0.355</td><td>0.338</td><td>0.259</td><td>0.257</td><td>0.262</td><td>0.216</td><td>0.411</td><td>0.367</td><td>0.307</td></tr><tr><td>BARTScore-r-cnndm</td><td>0.485</td><td>0.435</td><td>0.334</td><td>0.329</td><td>0.303</td><td>0.257</td><td>0.439</td><td>0.411</td><td>0.345</td></tr><tr><td>BARTScore-r-parabank</td><td>0.507</td><td>0.462</td><td>0.357</td><td>0.361</td><td>0.329</td><td>0.277</td><td>0.462</td><td>0.444</td><td>0.372</td></tr><tr><td>BERTScore-f1-deberta</td><td>0.518</td><td>0.491</td><td>0.386</td><td>0.317</td><td>0.323</td><td>0.274</td><td>0.517</td><td>0.472</td><td>0.401</td></tr><tr><td>BERTScore-f1-roberta</td><td>0.515</td><td>0.486</td><td>0.386</td><td>0.333</td><td>0.330</td><td>0.280</td><td>0.512</td><td>0.470</td><td>0.401</td></tr><tr><td>BERTScore-p-deberta</td><td>0.423</td><td>0.411</td><td>0.320</td><td>0.248</td><td>0.294</td><td>0.248</td><td>0.413</td><td>0.382</td><td>0.323</td></tr><tr><td>BERTScore-p-roberta</td><td>0.389</td><td>0.378</td><td>0.296</td><td>0.261</td><td>0.296</td><td>0.250</td><td>0.411</td><td>0.377</td><td>0.319</td></tr><tr><td>BERTScore-r-deberta</td><td>0.491</td><td>0.454</td><td>0.354</td><td>0.329</td><td>0.291</td><td>0.246</td><td>0.474</td><td>0.441</td><td>0.372</td></tr><tr><td>BERTScore-r-roberta</td><td>0.515</td><td>0.476</td><td>0.371</td><td>0.349</td><td>0.306</td><td>0.260</td><td>0.470</td><td>0.442</td><td>0.375</td></tr><tr><td>BLANC</td><td>0.045</td><td>0.020</td><td colspan=\"5\">0.014 -0.031 -0.041 -0.036 0.034</td><td>0.038</td><td>0.035</td></tr><tr><td>BLEU</td><td>0.441</td><td>0.414</td><td>0.328</td><td>0.294</td><td>0.300</td><td>0.260</td><td>0.469</td><td>0.432</td><td>0.368</td></tr><tr><td>CHRF</td><td>0.527</td><td>0.479</td><td>0.379</td><td>0.351</td><td>0.301</td><td>0.256</td><td>0.438</td><td>0.412</td><td>0.351</td></tr><tr><td>Compression</td><td colspan=\"5\">-0.053 -0.002 0.021 -0.037 0.069</td><td colspan=\"4\">0.071 -0.037 -0.021 0.001</td></tr><tr><td>Coverage</td><td colspan=\"7\">-0.016 -0.020 -0.019 -0.051 -0.049 -0.040 0.055</td><td>0.045</td><td>0.037</td></tr><tr><td>CTC</td><td>0.349</td><td>0.317</td><td>0.237</td><td>0.274</td><td>0.231</td><td>0.194</td><td>0.414</td><td>0.385</td><td>0.326</td></tr><tr><td>Density</td><td colspan=\"7\">-0.031 -0.040 -0.032 -0.059 -0.067 -0.055 0.050</td><td>0.051</td><td>0.046</td></tr><tr><td>Lite 3 Pyramid-l2c</td><td>0.452</td><td>0.424</td><td>0.355</td><td>0.212</td><td>0.197</td><td>0.181</td><td>0.410</td><td>0.404</td><td>0.374</td></tr><tr><td>Lite 3 Pyramid-l3c</td><td>0.449</td><td>0.427</td><td>0.358</td><td>0.213</td><td>0.197</td><td>0.180</td><td>0.418</td><td>0.417</td><td>0.385</td></tr><tr><td>Lite 3 Pyramid-p2c</td><td>0.482</td><td>0.420</td><td>0.321</td><td>0.294</td><td>0.259</td><td>0.216</td><td>0.462</td><td>0.419</td><td>0.353</td></tr><tr><td>Lite 3 Pyramid-p3c</td><td>0.489</td><td>0.430</td><td>0.330</td><td>0.298</td><td>0.253</td><td>0.211</td><td>0.469</td><td>0.419</td><td>0.354</td></tr><tr><td>Meteor</td><td>0.484</td><td>0.435</td><td>0.337</td><td>0.329</td><td>0.303</td><td>0.260</td><td>0.427</td><td>0.391</td><td>0.335</td></tr><tr><td>MoverScore</td><td>0.509</td><td>0.483</td><td>0.380</td><td>0.341</td><td>0.329</td><td>0.275</td><td>0.513</td><td>0.459</td><td>0.386</td></tr><tr><td>Novel-1gram</td><td>0.017</td><td>0.020</td><td>0.018</td><td>0.054</td><td>0.045</td><td colspan=\"4\">0.037 -0.066 -0.059 -0.049</td></tr><tr><td>Novel-2gram</td><td>0.053</td><td>0.049</td><td>0.037</td><td>0.063</td><td>0.068</td><td colspan=\"4\">0.055 -0.060 -0.058 -0.052</td></tr><tr><td>Repeated-1gram</td><td colspan=\"8\">-0.029 -0.044 -0.033 -0.015 -0.030 -0.022 -0.004 0.007</td><td>0.011</td></tr><tr><td>Repeated-2gram</td><td colspan=\"9\">-0.012 -0.016 -0.007 0.004 -0.009 -0.007 -0.036 -0.032 -0.027</td></tr><tr><td>QAEval-em</td><td>0.322</td><td>0.302</td><td>0.253</td><td>0.161</td><td>0.155</td><td>0.144</td><td>0.328</td><td>0.321</td><td>0.289</td></tr><tr><td>QAEval-f1</td><td>0.412</td><td>0.379</td><td>0.301</td><td>0.227</td><td>0.207</td><td>0.188</td><td>0.367</td><td>0.349</td><td>0.307</td></tr><tr><td>ROUGE1</td><td>0.541</td><td>0.510</td><td>0.403</td><td>0.324</td><td>0.324</td><td>0.278</td><td>0.494</td><td>0.464</td><td>0.399</td></tr><tr><td>ROUGE1p</td><td>0.386</td><td>0.380</td><td>0.298</td><td>0.263</td><td>0.303</td><td>0.262</td><td>0.320</td><td>0.308</td><td>0.269</td></tr><tr><td>ROUGE1r</td><td>0.425</td><td>0.374</td><td>0.290</td><td>0.317</td><td>0.276</td><td>0.244</td><td>0.349</td><td>0.329</td><td>0.286</td></tr><tr><td>ROUGE2</td><td>0.504</td><td>0.473</td><td>0.375</td><td>0.296</td><td>0.292</td><td>0.253</td><td>0.432</td><td>0.402</td><td>0.343</td></tr><tr><td>ROUGE2p</td><td>0.439</td><td>0.424</td><td>0.335</td><td>0.270</td><td>0.291</td><td>0.253</td><td>0.346</td><td>0.330</td><td>0.283</td></tr><tr><td>ROUGE2r</td><td>0.474</td><td>0.433</td><td>0.347</td><td>0.298</td><td>0.274</td><td>0.243</td><td>0.379</td><td>0.359</td><td>0.317</td></tr><tr><td>ROUGEL</td><td>0.512</td><td>0.481</td><td>0.378</td><td>0.288</td><td>0.293</td><td>0.252</td><td>0.462</td><td>0.427</td><td>0.365</td></tr><tr><td>ROUGELp</td><td>0.380</td><td>0.375</td><td>0.294</td><td>0.249</td><td>0.287</td><td>0.248</td><td>0.313</td><td>0.293</td><td>0.254</td></tr><tr><td>ROUGELr</td><td>0.424</td><td>0.378</td><td>0.293</td><td>0.292</td><td>0.258</td><td>0.227</td><td>0.338</td><td>0.321</td><td>0.279</td></tr><tr><td>SimCSE</td><td>0.437</td><td>0.393</td><td>0.300</td><td>0.321</td><td>0.305</td><td>0.255</td><td>0.454</td><td>0.422</td><td>0.356</td></tr><tr><td>SummaQA</td><td>0.059</td><td>0.044</td><td>0.031</td><td>0.018</td><td>0.001</td><td>0.002</td><td>0.010</td><td>0.022</td><td>0.014</td></tr><tr><td>SummaQA-prob</td><td>0.076</td><td>0.069</td><td>0.050</td><td colspan=\"4\">0.019 -0.003 -0.003 0.050</td><td>0.054</td><td>0.047</td></tr><tr><td>Summary-length</td><td>0.040</td><td colspan=\"6\">0.002 -0.021 0.027 -0.075 -0.079 0.010</td><td colspan=\"2\">0.021 -0.001</td></tr><tr><td>SUPERT</td><td>0.062</td><td>0.045</td><td>0.030</td><td>0.031</td><td>0.013</td><td>0.008</td><td>0.064</td><td>0.059</td><td>0.044</td></tr><tr><td>UniEval-coherence</td><td>0.083</td><td>0.087</td><td>0.060</td><td colspan=\"4\">0.011 -0.018 -0.014 0.108</td><td>0.112</td><td>0.090</td></tr><tr><td>UniEval-consistency</td><td colspan=\"7\">0.001 -0.006 -0.009 0.008 -0.015 -0.010 0.087</td><td>0.075</td><td>0.060</td></tr><tr><td>UniEval-fluency</td><td>0.017</td><td>0.001</td><td colspan=\"5\">0.001 -0.031 -0.040 -0.032 0.035</td><td>0.014</td><td>0.009</td></tr><tr><td>UniEval-overall</td><td>0.130</td><td>0.146</td><td>0.107</td><td>0.088</td><td>0.062</td><td>0.052</td><td>0.204</td><td>0.208</td><td>0.168</td></tr><tr><td>UniEval-relevance</td><td>0.150</td><td>0.173</td><td>0.127</td><td>0.187</td><td>0.192</td><td>0.158</td><td>0.236</td><td>0.252</td><td>0.203</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF22": {
                "text": "The summary-level Pearson's r, Spearman's \u03c1, and Kendall's \u03c4 correlation coefficients between the automatic metric scores and normalized ACU scores of system outputs on CNNDM, XSum and SamSum datasets.",
                "content": "<table/>",
                "html": null,
                "num": null,
                "type_str": "table"
            }
        }
    }
}
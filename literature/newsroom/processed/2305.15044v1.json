{
    "paper_id": "2305",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2025-02-14T13:47:02.482238Z"
    },
    "title": "Is Summary Useful or Not? An Extrinsic Human Evaluation of Text Summaries on Downstream Tasks",
    "authors": [
        {
            "first": "Xiao",
            "middle": [],
            "last": "Pu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Peking University",
                "location": {}
            },
            "email": "puxiao@stu.pku.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Research on automated text summarization relies heavily on human and automatic evaluation. While recent work on human evaluation mainly adopted intrinsic evaluation methods, judging the generic quality of text summaries, e.g. informativeness and coherence, our work focuses on evaluating the usefulness of text summaries with extrinsic methods. We carefully design three different downstream tasks for extrinsic human evaluation of summaries, i.e., question answering, text classification and text similarity assessment. We carry out experiments using system rankings and user behavior data to evaluate the performance of different summarization models. We find summaries are particularly useful in tasks that rely on an overall judgment of the text, while being less effective for question answering tasks. The results show that summaries generated by fine-tuned models lead to higher consistency in usefulness across all three tasks, as rankings of fine-tuned summarization systems are close across downstream tasks according to the proposed extrinsic metrics. Summaries generated by models in the zero-shot setting, however, are found to be biased towards the text classification and similarity assessment tasks, due to its general and less detailed summary style.\nWe further evaluate the correlation of 14 intrinsic automatic metrics with human criteria and show that intrinsic automatic metrics perform well in evaluating the usefulness of summaries in the question-answering task, but are less effective in the other two tasks. This highlights the limitations of relying solely on intrinsic automatic metrics in evaluating the performance and usefulness of summaries.",
    "pdf_parse": {
        "paper_id": "2305",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Research on automated text summarization relies heavily on human and automatic evaluation. While recent work on human evaluation mainly adopted intrinsic evaluation methods, judging the generic quality of text summaries, e.g. informativeness and coherence, our work focuses on evaluating the usefulness of text summaries with extrinsic methods. We carefully design three different downstream tasks for extrinsic human evaluation of summaries, i.e., question answering, text classification and text similarity assessment. We carry out experiments using system rankings and user behavior data to evaluate the performance of different summarization models. We find summaries are particularly useful in tasks that rely on an overall judgment of the text, while being less effective for question answering tasks. The results show that summaries generated by fine-tuned models lead to higher consistency in usefulness across all three tasks, as rankings of fine-tuned summarization systems are close across downstream tasks according to the proposed extrinsic metrics. Summaries generated by models in the zero-shot setting, however, are found to be biased towards the text classification and similarity assessment tasks, due to its general and less detailed summary style.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "We further evaluate the correlation of 14 intrinsic automatic metrics with human criteria and show that intrinsic automatic metrics perform well in evaluating the usefulness of summaries in the question-answering task, but are less effective in the other two tasks. This highlights the limitations of relying solely on intrinsic automatic metrics in evaluating the performance and usefulness of summaries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "conveys the main points. The recent decade has witnessed the rapid development of automated text summarization models. One major challenge for the application of text summarization is how to evaluate whether such summaries generated by models are actually fluent, accurate, and useful.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Text summary evaluation methods can be divided into two categories: using automatic evaluation metrics or human judgments. Automatic evaluation metrics make it possible to evaluate the quality of generated text summaries in a much cheaper and quicker way, and existing popular automatic metrics are intrinsic evaluation metrics as they usually compare generated summaries with reference summaries or source documents to reflect the generic quality of summaries. Since current intrinsic automatic evaluation metrics can sometimes lead to erroneous conclusions [12] , however, there is still no perfect substitute for human annotation. Human evaluation is usually used to evaluate the performance of text summarization models more reliably, or used as an oracle to evaluate the reliability of automated evaluation metrics.",
                "cite_spans": [
                    {
                        "start": 559,
                        "end": 563,
                        "text": "[12]",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "There are two types of human evaluation: intrinsic evaluation and extrinsic evaluation. While intrinsic evaluation of text summarization focuses on the requirements of the task per se, e.g. coherence, fluency, and informativeness [2, 8] , extrinsic evaluation, also known as task-based evaluation, assesses the usefulness or helpfulness of text summaries in other tasks [6] . It is more objective and spontaneous because it evaluates human performance in a realistic usage scenario and is less demanding on the annotators. [10] The prior works on extrinsic evaluation of summarization models have employed methods such as cross-comprehension tests [16] , relevance judgment [6] , and question answering [14] . These studies are dated more than a decade ago. In recent years, neural summarization systems, especially those based on pre-trained language models have made great strides in intrinsic evaluation [2, 8] . However, to the best of our knowledge, no work has investigated the usefulness of these approaches from the perspective of extrinsic evaluation. Furthermore, these studies rely on a single method of extrinsic evaluation or are limited by the small scale of human experiments [15] . In light of these limitations, our work aims to propose a more comprehensive extrinsic evaluation method and conduct experiments on a larger scale, to systematically evaluate the usefulness of text summarization, including the summarization methods proposed recently. An attempt is also made to construct a trustworthy human-evaluated corpus, including subsets on three downstream tasks.",
                "cite_spans": [
                    {
                        "start": 230,
                        "end": 233,
                        "text": "[2,",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 234,
                        "end": 236,
                        "text": "8]",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 370,
                        "end": 373,
                        "text": "[6]",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 523,
                        "end": 527,
                        "text": "[10]",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 648,
                        "end": 652,
                        "text": "[16]",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 674,
                        "end": 677,
                        "text": "[6]",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 703,
                        "end": 707,
                        "text": "[14]",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 907,
                        "end": 910,
                        "text": "[2,",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 911,
                        "end": 913,
                        "text": "8]",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 1191,
                        "end": 1195,
                        "text": "[15]",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Based on the proposed evaluation method in this study, we want to investigate the following research questions:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "\u2022 How useful are text summaries compared to the source articles?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "\u2022 In which tasks are summaries more useful in general?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "\u2022 What kind of summaries are more useful than others?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "\u2022 Which intrinsic automatic metrics for text summarization correlate well with our human judgments?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The contributions of our work are summarized as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "\u2022 We introduce an extrinsic evaluation framework for systematically assessing the usefulness of text summarization. We also present seven extrinsic metrics in three downstream tasks. \u2022 We annotate and construct a reliable human extrinsic evaluation dataset of 4,000 texts, including 400 source texts, 400 human summaries, and 3,200 summaries generated by eight different text summarization systems.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "\u2022 We analyze the usefulness of various types of text summaries and discover that they are more useful in the classification task and the similarity assessment task. \u2022 We re-evaluate 14 intrinsic automatic metrics through our proposed criteria and discover that most of them fail to reflect the extrinsic metrics in classification and similarity tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The rest of this paper will be organized as follows: Section 2 introduces related work. Section 3 outlines the research methodology adopted in this study. Section 4 provides some preliminaries, including the datasets, summarization systems, and intrinsic automatic metrics utilized. The experimental setup is described in Section 5. The results of our analysis are presented in Section 6. Finally, significant conclusions are drawn in Section 7.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Past works that have assessed the quality of summaries through intrinsic evaluation methods can be classified into two main categories: intrinsic automatic metrics and intrinsic human evaluation. Early works evaluate summaries by computing the n-gram word overlap between reference summaries and generated summaries, such as BLEU [34] and ROUGE [21] , which have proven to be relatively effective over time. With the development of representation learning, researchers have proposed new intrinsic automatic metrics based on word embeddings, such as Greedy Matching [37] and SMS [5] , which compute the similarity of word embeddings between reference summaries and generated summaries. Additionally, automatic metrics based on question-answering [40] and entailment classification [17] have also been proposed. Human evaluation, on the other hand, is considered the gold standard for evaluating generated summaries. The Pyramid method [30] serves as a viable framework for human evaluation, which has been further improved into a crowdsourcing method [41] .",
                "cite_spans": [
                    {
                        "start": 330,
                        "end": 334,
                        "text": "[34]",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 345,
                        "end": 349,
                        "text": "[21]",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 565,
                        "end": 569,
                        "text": "[37]",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 578,
                        "end": 581,
                        "text": "[5]",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 745,
                        "end": 749,
                        "text": "[40]",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 780,
                        "end": 784,
                        "text": "[17]",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 934,
                        "end": 938,
                        "text": "[30]",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 1050,
                        "end": 1054,
                        "text": "[41]",
                        "ref_id": "BIBREF40"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RELATED WORK 2.1 Intrinsic Evaluation for Summarization",
                "sec_num": "2"
            },
            {
                "text": "Previous research has also investigated the relationship between intrinsic automatic metrics and intrinsic human judgments in the field of text summarization. A common approach to conduct metaevaluation is to have annotators score the quality of summaries by Pyramid method [2] or on multiple dimensions [8] such as coherence, consistency, relevance, and fluency, and compute the correlation coefficient between the output scores of automatic evaluation metrics and human judgments. Prior research has shown significant differences in the performance of experts and non-experts in scoring summaries [10] . Recent work has examined the consistency between intrinsic automatic metrics and human preferences for different types of summaries and found that intrinsic automatic metrics cannot reliably evaluate summaries generated by models in the zero-shot setting. In contrast, our work investigates the correlation between intrinsic automatic metrics and extrinsic human judgments [11] .",
                "cite_spans": [
                    {
                        "start": 274,
                        "end": 277,
                        "text": "[2]",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 304,
                        "end": 307,
                        "text": "[8]",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 599,
                        "end": 603,
                        "text": "[10]",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 979,
                        "end": 983,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RELATED WORK 2.1 Intrinsic Evaluation for Summarization",
                "sec_num": "2"
            },
            {
                "text": "Previous work has acknowledged the human's subjectivity in evaluating summaries, and has attempted to alleviate this through the use of cross-comprehension tests [16] . The usefulness of summaries has also been evaluated through a single extrinsic task, i.e. relevance judgment [6] and question answering [14] . While some researchers have proposed a set of tasks to measure the information content of full text and summaries, including a Shannon Game, a Question Game, and a Classification Game, finding that different extrinsic evaluation methods rate summaries differently, the scale of the experiments was too small to draw statistically significant conclusions [15] . Our work designs three distinct extrinsic evaluation tasks with a larger scale of human judgments and evaluates the summaries generated by the recently proposed summarization approaches.",
                "cite_spans": [
                    {
                        "start": 162,
                        "end": 166,
                        "text": "[16]",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 278,
                        "end": 281,
                        "text": "[6]",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 305,
                        "end": 309,
                        "text": "[14]",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 666,
                        "end": 670,
                        "text": "[15]",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extrinsic Evaluation for Summarization",
                "sec_num": "2.2"
            },
            {
                "text": "Summarization models can be broadly categorized into two groups: extractive and abstractive. Extractive models directly identify and extract the most important sentences or words from the source text as the summary. Non-neural models, such as graph-based models, fuzzy logic-based models, and latent semantic analysis have been proposed and investigated [7, 18, 25, 26, 33, 42] . Additionally, researchers have also explored extractive summarization based on neural network models [22, 27, 29, 44] . On the other hand, abstractive models generate a summary text that is not necessarily a direct extraction of the source text. In recent years, abstractive summarization models based on neural networks have been advancing and become dominant in the summarization field. A common paradigm is pre-training and fine-tuning [20, 23, 45] . Additionally, some prompt-based approaches have been proposed [3, 39] , enabling summarization models to learn from specific task instructions.",
                "cite_spans": [
                    {
                        "start": 354,
                        "end": 357,
                        "text": "[7,",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 358,
                        "end": 361,
                        "text": "18,",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 362,
                        "end": 365,
                        "text": "25,",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 366,
                        "end": 369,
                        "text": "26,",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 370,
                        "end": 373,
                        "text": "33,",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 374,
                        "end": 377,
                        "text": "42]",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 481,
                        "end": 485,
                        "text": "[22,",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 486,
                        "end": 489,
                        "text": "27,",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 490,
                        "end": 493,
                        "text": "29,",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 494,
                        "end": 497,
                        "text": "44]",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 819,
                        "end": 823,
                        "text": "[20,",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 824,
                        "end": 827,
                        "text": "23,",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 828,
                        "end": 831,
                        "text": "45]",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 896,
                        "end": 899,
                        "text": "[3,",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 900,
                        "end": 903,
                        "text": "39]",
                        "ref_id": "BIBREF38"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summarization Models",
                "sec_num": "2.3"
            },
            {
                "text": "The purpose of this study is to provide a comprehensive assessment of the usefulness of summaries in real-world usage scenarios. Participants are asked to complete three tasks using source articles and summaries, and their performance is measured to determine the usefulness of summaries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RESEARCH METHODOLOGY",
                "sec_num": "3"
            },
            {
                "text": "Measures of usefulness In our study, we consider a summary to be useful (or helpful) if it is able to facilitate users to complete a task. A useful summary should help users save time by being shorter than the source text, while also providing them with the important information they need to complete the task. Therefore, to assess the usefulness of the summaries, we decide to compare on two dimensions: time and correctness. Time refers to the amount of time it takes the participant to complete the task using either the source text or the summary. Correctness refers to the accuracy of the participant's response and is measured using different metrics for each task. A web-based platform is developed and deployed for this study, to automatically record the completion time and submitted answers by participants for each task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RESEARCH METHODOLOGY",
                "sec_num": "3"
            },
            {
                "text": "The three downstream tasks that we designed in this study are: Question answering task: In this task, participants are asked to answer questions based on the information provided in the source text or the summary. To evaluate the participant's accuracy, we use two commonly used evaluation metrics in QA systems to calculate the overlap between the answers submitted by the participant and the ground true answers. Additionally, we also propose a distinguished metric to reflect on the probability of the participants' answer attempts. By evaluating their performance in the QA task, we are able to determine the amount of useful information contained in the summary.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RESEARCH METHODOLOGY",
                "sec_num": "3"
            },
            {
                "text": "Classification task: In this task, participants are asked to select one or more tags based on the article or summary they see. The accuracy of their choices is calculated as a way of determining whether different types of summaries are useful in helping people make an overall judgment about the article.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RESEARCH METHODOLOGY",
                "sec_num": "3"
            },
            {
                "text": "Similarity assessment task: Participants are presented with a pair of news articles or summaries in this task. They are asked to take into account various factors such as the topic, event field, writing style, tone, etc. of the two articles to make a comprehensive judgment, and then score the similarity of the two articles or summaries on a scale of 1 to 4. By calculating how similar their scores are to the ground truth scores, we can determine how useful the summaries are for similarity judgments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RESEARCH METHODOLOGY",
                "sec_num": "3"
            },
            {
                "text": "We use three datasets for different downstream tasks respectively in our study:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PRELIMINARIES 4.1 Datasets",
                "sec_num": "4"
            },
            {
                "text": "CNN/DailyMail [13, 28] is a widely used benchmark for text summarization, which includes a collection of news articles and their corresponding reference summaries that are typically 3-4 sentences in length. This dataset is used for extrinsic evaluation on the question answering task.",
                "cite_spans": [
                    {
                        "start": 14,
                        "end": 18,
                        "text": "[13,",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 19,
                        "end": 22,
                        "text": "28]",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PRELIMINARIES 4.1 Datasets",
                "sec_num": "4"
            },
            {
                "text": "New York Times Annotated Corpus [38] contains a set of news articles along with human-written summaries. Each article is also associated with multiple tags or labels. This dataset is used for extrinsic evaluation on the text classification task.",
                "cite_spans": [
                    {
                        "start": 32,
                        "end": 36,
                        "text": "[38]",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PRELIMINARIES 4.1 Datasets",
                "sec_num": "4"
            },
            {
                "text": "The SemEval-2022 Task 8 dataset [4] is a multilingual collection of the URLs of news articles that have been paired and annotated for their similarity level. The dataset includes nearly 1,000 article pairs from 18 different languages. This dataset is used for extrinsic evaluation on the text similarity assessment task.",
                "cite_spans": [
                    {
                        "start": 32,
                        "end": 35,
                        "text": "[4]",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PRELIMINARIES 4.1 Datasets",
                "sec_num": "4"
            },
            {
                "text": "We need to select a few publicly available systems to generate summaries on the three datasets and then studying the usefulness of the summaries. As neural abstractive summarization methods with pretraining have achieved great success in recent years, we mainly focus on these summarization models. A total of six representative neural models are chosen as the abstractive systems, including:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Representative Summarization Systems",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 BART [20] : a sequence-to-sequence model trained as a denoising autoencoder, which is applicable to various natural language generation tasks. It is fine-tuned on CNN/DailyMail. \u2022 Pegasus [45] : a model pre-trained with self-supervised gapsentence-generation objective designed for abstractive summarization. We use the version fine-tuned on CNN/DailyMail. \u2022 BRIO [24] : a model with a new training paradigm that assigns candidate outputs probability mass according to their quality using contrastive learning. It is also fine-tuned on CNN/DailyMail. \u2022 T5 [36] : a text-to-text transfer learning framework that is pretrained with several unsupervised and supervised objectives, including summarization. \u2022 T0 [39] : a prompt-based model, which is fine-tuned on standard summarization datasets including CNN/DailyMail. \u2022 GPT3 [3] : a prompt-based language model that achieves strong performance in the few-shot setting. In this work, we use OpenAI's text-davinci-002 [32] . We also include two simple extractive systems for comparison:",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 11,
                        "text": "[20]",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 190,
                        "end": 194,
                        "text": "[45]",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 366,
                        "end": 370,
                        "text": "[24]",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 558,
                        "end": 562,
                        "text": "[36]",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 710,
                        "end": 714,
                        "text": "[39]",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 826,
                        "end": 829,
                        "text": "[3]",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 967,
                        "end": 971,
                        "text": "[32]",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Representative Summarization Systems",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 Lead-n: Lead-3 is a simple but commonly used summarization baseline that selects the first three sentences of an article as the summary. we modify the Lead-3 setting and refer to it as the Lead-n model. Lead-n selects the first several sentences that are closest to the summary length we set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Representative Summarization Systems",
                "sec_num": "4.2"
            },
            {
                "text": "\u2022 Lexrank [7] : a graph-based text summarization model that calculates the importance of sentences by determining the cosine similarity between them, and the sentences with highest scores are selected as the summary.",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 13,
                        "text": "[7]",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Representative Summarization Systems",
                "sec_num": "4.2"
            },
            {
                "text": "We employ a set of 14 automatic evaluation metrics to intrinsically assess the summaries. These metrics include n-gram overlap-based measures such as ROUGE-1, ROUGE-2, ROUGE-L [21] , BLEU [34] , METEOR [1] , CIDEr [43] and CHRF [35] . For metrics based on word embeddings, we report BERTScore [46] , MoverScore [47] , Rouge-we [31] , Embedding average [19] , Vector extrema [9] , Greedy matching [37] . Furthermore, we also include a model-based metric SummaQA [40] in our evaluation. All scores are reported in the range of 0-1. These scores will be compared with our extrinsic human evaluation results.",
                "cite_spans": [
                    {
                        "start": 176,
                        "end": 180,
                        "text": "[21]",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 188,
                        "end": 192,
                        "text": "[34]",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 202,
                        "end": 205,
                        "text": "[1]",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 214,
                        "end": 218,
                        "text": "[43]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 228,
                        "end": 232,
                        "text": "[35]",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 293,
                        "end": 297,
                        "text": "[46]",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 311,
                        "end": 315,
                        "text": "[47]",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 327,
                        "end": 331,
                        "text": "[31]",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 352,
                        "end": 356,
                        "text": "[19]",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 374,
                        "end": 377,
                        "text": "[9]",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 396,
                        "end": 400,
                        "text": "[37]",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 461,
                        "end": 465,
                        "text": "[40]",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Intrinsic Automatic Metrics",
                "sec_num": "4.3"
            },
            {
                "text": "In this section, we present the construction and annotation of three datasets for use and the design of our user study for extrinsic evaluation. Specifically, we focus on three downstream tasks: question answering (QA), text classification, and text similarity assessment.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "EXPERIMENTAL SETTINGS",
                "sec_num": "5"
            },
            {
                "text": "We then propose extrinsic metrics based on these tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "EXPERIMENTAL SETTINGS",
                "sec_num": "5"
            },
            {
                "text": "Processing and annotating datasets. We reprocess and manually annotate three existing datasets for use in our user study. The datasets for the downstream tasks are constructed in the following steps.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Preparation Process",
                "sec_num": "5.1"
            },
            {
                "text": "For the QA task, we randomly select 100 pairs of source text and reference summary from the CNN/DailyMail test set. We then construct two datasets for the QA task, namely QA-ref and QA-source.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Preparation Process",
                "sec_num": "5.1"
            },
            {
                "text": "For QA-ref, we formulate four questions and their corresponding answers for each reference summary. For QA-source, we read the longer source text, identify the important points within the news, and formulate four questions accordingly. For each question, we search for all corresponding content within the source text as the correct answer. In both datasets, a question may have multiple correct answers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Preparation Process",
                "sec_num": "5.1"
            },
            {
                "text": "For the classification task, we randomly sample 100 news articles from the New York Times Annotated Corpus test set and obtain 19 tags. We analyze these tags and identify those that are vague in meaning and difficult to identify from the article, such as 'Front Page', and those that were redundant and dependent on other tags, such as 'Travel', 'Theater', 'Dining and Wine', and 'Movies', which always appear alongside 'Art'. We remove these tags and retain a total of 11 tags, at least one for each news article.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Preparation Process",
                "sec_num": "5.1"
            },
            {
                "text": "For the similarity task, we utilize the Semeval2022 task8 dataset and construct a dataset for use consisting of 100 pairs of news articles, together with reference summaries and corresponding similarity scores through the following steps: First, we randomly crawl 300 pairs of news pages through the corresponding URLs and extract the title, description, and body parts of each article. The next step is data cleaning, where we remove pairs with empty or too short titles/descriptions/bodies and those whose descriptions are directly sourced from the beginning of the source article with incomplete sentences. Then, we splice headlines and descriptions to form summaries. After manual review, we finally retain 100 pairs of news articles for the similarity task, comprising 200 news articles.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Preparation Process",
                "sec_num": "5.1"
            },
            {
                "text": "Generating summaries of similar length. In order to eliminate any potential bias that may have resulted from variations in text length, we keep the length of summaries within a defined range. This range is determined based on the average length of the human summaries in each task. To achieve this, we employ a two-step process: First, we set a range for the number of tokens generated for abstractive models and a range for the number of sentences generated for extractive models, during the process of generating the summaries with the model. Secondly, all summaries are truncated to the established range. Figure 6 in the appendix shows the length of summaries in the three tasks.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 616,
                        "end": 617,
                        "text": "6",
                        "ref_id": "FIGREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Data Preparation Process",
                "sec_num": "5.1"
            },
            {
                "text": "We implement a web-based platform (as shown in Figure 1 ) to facilitate users' participation in the tasks and the acquisition of experiment data, which includes responses and completion time for each question. To guarantee impartiality, the platform is designed to prohibit the utilization of the copy-paste/search functionality. Furthermore, the website offers guidance information and exemplar answers to assist participants to fully understand the tasks.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 54,
                        "end": 55,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Web-based Platform for Experiments",
                "sec_num": "5.2"
            },
            {
                "text": "We initially recruit ten individuals to participate in the QA-ref, classification and similarity tasks. For the QA-source task, we conduct a separate recruitment process and select another ten individuals. The purpose of this design was to ensure that participants had no prior memory of the text or question content. By having different individuals perform each task, we aim to minimize the influence of previously seen summaries on their responses to the original text questions. In total, we collect 1,000 responses for each task, resulting in a dataset of 10,000 annotations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Details",
                "sec_num": "5.3"
            },
            {
                "text": "To maintain the quality of annotations, all participants are recruited from the university campus, they are all graduate students aged between 22 and 26. All participants had the same native language and are proficient in English as their second language. They have obtained excellent scores in internationally recognized English exams, indicating their suitability for successfully completing the experimental tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Details",
                "sec_num": "5.3"
            },
            {
                "text": "To ensure that the participants' responses are only based on the content of the text currently being viewed and to minimize the influence of individual differences, a method for distributing the texts is devised. The following considerations are taken into account: 1) To prevent people from having an advantage due to prior exposure to a similar text, each person is allowed to see only one text (either source text or summary) from the same source. 2) To ensure fairness and remove the influence of individual differences, each person must be exposed to the same number of texts from each system, regardless of their proficiency level.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Details",
                "sec_num": "5.3"
            },
            {
                "text": "The distribution method is as follows: One source text is associated with nine summaries (including reference summary), resulting in ten texts (including source text) originating from the same source text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Details",
                "sec_num": "5.3"
            },
            {
                "text": "First, all summaries are aligned with the source text, then different systems are arranged in the following order: [Source, Human, BART, Pegasus, Lexrank, Lead-n, BRIO, T5, T0, GPT3]. After that, all texts are numbered, with text_id (0-999) as their unique identifier. Therefore, the hundreds place indicates the system corresponding to the text, and the tens place and the individual place indicate the corresponding source text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Details",
                "sec_num": "5.3"
            },
            {
                "text": "The texts are assigned to different participants according to the system it belongs to and the corresponding source text. Each participant is assigned to a user_id and the correspondence between texts and participants is established by the following formula:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Details",
                "sec_num": "5.3"
            },
            {
                "text": "\ud835\udc66 = \u230a \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61_\ud835\udc56\ud835\udc51 -\u230a \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61 _\ud835\udc56\ud835\udc51 100 \u230b \u00d7 100 10 \u230b -\u230a \ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61_\ud835\udc56\ud835\udc51 100 \u230b \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f _\ud835\udc56\ud835\udc51 (\ud835\udc66) =",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Details",
                "sec_num": "5.3"
            },
            {
                "text": "\ud835\udc66, \ud835\udc66 \u2265 0 10 + \ud835\udc66, \ud835\udc66 < 0",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Details",
                "sec_num": "5.3"
            },
            {
                "text": "Based on the three downstream tasks, we propose the following extrinsic metrics to evaluate the usefulness of the summaries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Proposed Extrinsic Metrics",
                "sec_num": "5.4"
            },
            {
                "text": "For the QA task, let \ud835\udc66 \ud835\udc58 \ud835\udc5b denote the participant's answer to the k-th question of n-th article. All the correct answers to a question are ordered and \u0177\ud835\udc58\ud835\udc56 \ud835\udc5b denote the i-th key answer to the k-th question of n-th article. \ud835\udc41 represents the number of summaries of each system, which equals 100, and \ud835\udc3e represents the number of questions for each article, which equals 4 in this case, and the three metrics are calculated as follows.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Proposed Extrinsic Metrics",
                "sec_num": "5.4"
            },
            {
                "text": "\u2022 Answerable measures the proportion of questions that can be answered according to the text. \u2022 Exact Match Ratio (EM), which counts the overall accuracy rate of the answers. EM of each system is calculated as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Proposed Extrinsic Metrics",
                "sec_num": "5.4"
            },
            {
                "text": "\ud835\udc38\ud835\udc40 = 1 \ud835\udc41 \ud835\udc3e \ud835\udc41 \u2211\ufe01 \ud835\udc5b=1 \ud835\udc3e \u2211\ufe01 \ud835\udc58=1 \ud835\udc40\ud835\udc34\ud835\udc4b \ud835\udc56 (\ud835\udc3c (\ud835\udc66 \ud835\udc58 \ud835\udc5b == \u0177\ud835\udc58\ud835\udc56 \ud835\udc5b )) \ud835\udc64\ud835\udc56\ud835\udc61\u210e \ud835\udc3c (\ud835\udc66 \ud835\udc58 \ud835\udc5b == \u0177\ud835\udc5b \ud835\udc58\ud835\udc56 ) = 1, \ud835\udc66 \ud835\udc58 \ud835\udc5b = \u0177\ud835\udc5b \ud835\udc58\ud835\udc56 0, \ud835\udc66 \ud835\udc58 \ud835\udc5b \u2260 \u0177\ud835\udc5b \ud835\udc58\ud835\udc56",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Proposed Extrinsic Metrics",
                "sec_num": "5.4"
            },
            {
                "text": "\u2022 F1 is a looser measure of the average overlap between the prediction and ground truth answer. When calculating F1, both \ud835\udc66 \ud835\udc58 \ud835\udc5b and \u0177\ud835\udc5b \ud835\udc58 are tokenized into sets of words. F1 is calculated as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Proposed Extrinsic Metrics",
                "sec_num": "5.4"
            },
            {
                "text": "\ud835\udc39 1 = 1 \ud835\udc41 \ud835\udc3e \ud835\udc41 \u2211\ufe01 \ud835\udc5b=1 \ud835\udc3e \u2211\ufe01 \ud835\udc58=1 \ud835\udc40\ud835\udc34\ud835\udc4b \ud835\udc56 2|\ud835\udc66 \ud835\udc58 \ud835\udc5b \u2229 \u0177\ud835\udc5b \ud835\udc58\ud835\udc56 | |\ud835\udc66 \ud835\udc58 \ud835\udc5b | + | \u0177\ud835\udc5b \ud835\udc58\ud835\udc56 |",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Proposed Extrinsic Metrics",
                "sec_num": "5.4"
            },
            {
                "text": "For the classification task, we use EM and F1, two metrics that are commonly used in multiclass classification tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Proposed Extrinsic Metrics",
                "sec_num": "5.4"
            },
            {
                "text": "For the similarity task, we use the following metrics:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Proposed Extrinsic Metrics",
                "sec_num": "5.4"
            },
            {
                "text": "\u2022 Mean Squared Error (MSE), which indicates the extent to which the participant's answer deviates from the standard answer. \u2022 Spearman's \ud835\udf0c, a measure of the correlation between the participant's judgment and the true similarity. It can only be used for system-level analysis because it cannot be calculated using separate texts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Proposed Extrinsic Metrics",
                "sec_num": "5.4"
            },
            {
                "text": "In this section, we study the relationship between our proposed extrinsic metrics. We compute system-level correlations of all the extrinsic metrics (as shown in Figure 2 ). According to the Pearson's r, extrinsic metrics of the same downstream task are highly correlated, ranging from 0.8 to 1. QA-ref and QA-source are highly correlated at system level, with Pearson's r above 0.8 and Kendall's \ud835\udf0f above 0.69. This suggests that there is little difference in the relative performance of the systems on QA-ref and QA-source, although they differ in the way the dataset is constructed. Comparing the metrics of the different downstream tasks, we find that the QA task and the classification task are poorly correlated, with Pearson's r ranging from -0.2 to 0.2. Whereas the similarity task is moderately correlated with both the other two tasks, with Pearson's r ranging from 0.4 to 0.7. Overall, moderate to weak correlations illustrate that our experiment involves three tasks of different perspectives to measure the usefulness of the summary.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 169,
                        "end": 170,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "RESULTS AND ANALYSIS 6.1 Analyzing Our Extrinsic Metrics",
                "sec_num": "6"
            },
            {
                "text": "In this section, we compare the performance of different summarization systems by means of the proposed extrinsic evaluation method (as shown in Table 1 ) and try to answer some questions regarding the usefulness of summaries.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 151,
                        "end": 152,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Usefulness of Summaries",
                "sec_num": "6.2"
            },
            {
                "text": "How useful are text summaries compared to source articles?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Usefulness of Summaries",
                "sec_num": "6.2"
            },
            {
                "text": "Results from three downstream tasks demonstrate that the use of summaries significantly reduces the time required for task completion. Specifically, compared to the source articles, the average time participants spent using summaries to complete QA tasks drops by 61-62% (as shown in Table 2 ). Similar results can also be observed in the classification and similarity tasks, with the timesaving percentages of 59% and 42%, respectively (as shown in Table 3 ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 290,
                        "end": 291,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 456,
                        "end": 457,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Usefulness of Summaries",
                "sec_num": "6.2"
            },
            {
                "text": "We also find that summaries are particularly useful in classification and similarity tasks. In the QA task, source texts outperform summaries on average, while in the classification and similarity tasks, participants spend less time as well as perform better with summaries. This may be due to the fact that making an overall judgment about the text, such as classification or similarity assessment, does not require as much information as answering specific questions. As a result, the excess information in the long source text may not aid in decision-making and even interfere with human judgments. This is supported by observed people's tendencies in the classification task, where they tend to assign more tags to longer source articles, potentially leading to a higher recall but lower precision in comparison to the human summaries. 3 : Summaries compared to source texts in the classification and similarity tasks. It shows that summary serves about the same function as the source text in these two tasks, and even helps participants to do tasks better.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 840,
                        "end": 841,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Usefulness of Summaries",
                "sec_num": "6.2"
            },
            {
                "text": "Difference between QA-ref and QA-source In the QA-source task, where questions and answers are constructed from the source text, source articles excel in all three metrics (answerable, EM, and F1). In the QA-ref task, where questions and answers are constructed from the reference summary, although the answerable metric is similar for source articles and reference summaries, in terms of the other two metrics, i.e. EM and F1, reference summaries are approximately 50% better than the source text. This is because the information in the reference summary is only a subset of the source text. Therefore in some cases, although people find some questions in QA-ref answerable by looking at the source text, their answers may be counted as incorrect because they do not appear in the reference summary (even though they may be correct according to the source text).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Usefulness of Summaries",
                "sec_num": "6.2"
            },
            {
                "text": "What kind of summaries are more useful? We divide all the automated summaries into three categories based on the model used to generate them: fine-tuned, prompt-based, and simple extractive. A question we want to know is, how stable or consistent is the usefulness level of summaries across different downstream tasks? By analyzing rankings of the source text and summaries in the three tasks, as is shown in Figure 3 , we find that: The summaries generated by fine-tuned models have higher consistency in usefulness across different tasks, such as those generated by BART, Pegasus, and BRIO, with a stable ranking similar to that of the human summaries. This suggests that summaries generated by fine-tuned models are insensitive to differences between tasks.The summaries generated by simple extractive models and models in the zero-shot setting exhibit a varying ranking across tasks. For example, both zero-shot GPT3 summaries and simple extractive Lexrank summaries show high or above average rankings in the classification task, medium rankings in the similarity task, and very low rankings in the QA task.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 416,
                        "end": 417,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Usefulness of Summaries",
                "sec_num": "6.2"
            },
            {
                "text": "We also identify differences in the style of the summaries generated by the different models and a case study in Table 4 illustrates this point. The Summaries generated by fine-tuned models tend to be more informative and specific, including more factual details such as times, places, and numbers. 1 Due to this trait, summaries generated by fine-tuned models are found to be more useful for detail-oriented QA tasks, compared to their counterparts. The top six in all systems except the source text and reference summary are fine-tuned models, including task-specific fine-tuned T0 system. Summaries generated by models in the zero-shot setting are more abstractive and general than that of fine-tuned models, and therefore they are found to be more suitable for tasks that require overall judgment, such as classification and similarity tasks. As is shown in Figure 3 , zero-shot GPT3 summaries rank second in the classification task but only second to last in the QA task.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 119,
                        "end": 120,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 869,
                        "end": 870,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Usefulness of Summaries",
                "sec_num": "6.2"
            },
            {
                "text": "Compared to them, simple extractive summaries are more coarsegrained and less useful. According to the case study, they contain relatively less important information in a limited space. These two models were developed in the early years of natural language processing, and after nearly two decades of advancements in the field, their usefulness has been surpassed by more recent models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Usefulness of Summaries",
                "sec_num": "6.2"
            },
            {
                "text": "We perform a meta-evaluation using Pearson's r and Kendall's \ud835\udf0f to compare various intrinsic automatic metrics with our extrinsic metrics. Summary-level correlation (shown in Figure 4 ) is shown to be much lower than system-level correlation (shown in Table 5 ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 181,
                        "end": 182,
                        "text": "4",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 257,
                        "end": 258,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating Intrinsic Automatic Metrics",
                "sec_num": "6.3"
            },
            {
                "text": "Our analysis reveals that there is a high correlation between extrinsic metrics in the QA task and intrinsic automatic metrics, with Pearson's r values ranging between 0.7 and 1. Additionally, we find that there is little difference between the performance of different intrinsic automatic metrics, indicating that they are able to evaluate the QA task relatively well. models. When referring to summaries generated by fine-tuned models, it should only be understood as those fine-tuned on the CNN/DailyMail dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating Intrinsic Automatic Metrics",
                "sec_num": "6.3"
            },
            {
                "text": "A heartbroken pensioner is believed to have killed himself six days after his wife 's death by jumping from a bridge at their ' special place ' where they used to take romantic walks together. [...] Today officers confirmed a body pulled from the River Trent on April 15 by a specialist underwater search unit was sadly that of the missing pensioner. [...] June tragically died on March 31 , eight hours after collapsing suddenly from what doctors at the Queen 's Medical Centre in Nottingham described as a ' catastrophic bleed ' to the brain. [...] GPT3 summary: A man is believed to have killed himself by jumping from a bridge at a picturesque spot where he and his wife used to take romantic walks together, six days after she died from a brain hemorrhage.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Source text:",
                "sec_num": null
            },
            {
                "text": "John Lord , 86 , went missing from his home on April 6 less than a week after his beloved wife June , 81, died from a ' catastrophic bleed ' to the brain. The body of the pensioner was recovered from the River Trent on April 15. His family believe he may have jumped from a bridge at the picturesque beauty [...] T0 summary: John Lord, 86, went missing from his home on April 6. His wife June, 81, died from a 'catastrophic bleed' to the brain. Family feared the worst after finding a note describing how much he missed her. Mr Lord's body was pulled from the River Trent on April 15. Table 4 : A case study to illustrate the difference of summary style. By looking at the source text and the summaries generated with different models, we find that the zero-shot GPT3 summary tends to paraphrase the news in a more general way, making it easier for readers to capture the main point, but often omitting detailed information. Instead, summaries of fine-tuned BRIO and T0 models contain more detailed information, making it more suitable for QA tasks. The coherence between sentences in the extractive Lexrank summary is poor, causing difficulty in reading.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 591,
                        "end": 592,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "BRIO summary:",
                "sec_num": null
            },
            {
                "text": "On the other hand, we observe that extrinsic metrics in classification and similarity tasks have low to moderate correlation with most intrinsic automatic metrics. The Embedding Average metric is found to be strongly correlated with the extrinsic metrics for the classification task (statistically significant at p <0.01) and show a moderate correlation for the similarity task. Other word embedding-based metrics such as Greedy Matching, Rouge-we, BERTScore and MOVERScore also show moderate correlation with extrinsic metrics in classification and similarity tasks. In terms of the best and worst intrinsic automatic metrics, we find that no single metric consistently performs the best across all tasks. However, two intrinsic automatic metrics that are closest to the extrinsic metrics are Rouge-1 (better in the QA task) and Embedding Average (better in the similarity and classification tasks). On the other hand, CIDEr is found to be least correlated with the extrinsic metrics, and show little relevance for the similarity and classification tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "BRIO summary:",
                "sec_num": null
            },
            {
                "text": "We further evaluate the reliability of intrinsic automatic metrics in quantifying differences between systems with competitive performance,i.e., top-\ud835\udc58 system analysis. As illustrated in Figure 5 , \ud835\udc58 systems are ranked based on different extrinsic metrics. We observe that for the QA-ref answerable metric and QA-source F1 and answerable metrics, the correlation between automatic and extrinsic metrics decreases slightly as the number of systems increases from 3, then increases when the number of systems reaches 5. A similar trend is also observed in the plot of the F1 indicator for the classification task, but with more noticeable fluctuations. However, we find a significant decline in the correlation between extrinsic and intrinsic automatic metrics of the similarity task as \ud835\udc58 increased, which suggests that intrinsic automatic metrics should not be used to compare systems with substantial differences in usefulness in this task. While the correlation between the QA-ref answerable metric and intrinsic automatic metrics remains stable at a high level even as \ud835\udc58 changed, we find that most intrinsic automatic metrics may not consistently and reliably quantify differences of usefulness between systems.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 193,
                        "end": 194,
                        "text": "5",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "BRIO summary:",
                "sec_num": null
            },
            {
                "text": "In this work, we conduct a user study for extrinsic evaluation of the usefulness of text summaries in different downstream tasks. Our key findings are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CONCLUSIONS",
                "sec_num": "7"
            },
            {
                "text": "(1) The usefulness of summaries is demonstrated through the dual factors of time-saving and performance. While summaries notably decrease task completion time, they may also lead to a decrease in task performance in some cases. However, the overall benefit of summaries is still apparent when considering the balance between time saved and reduction in accuracy. (2) Summaries are particularly useful for classification and similarity tasks while being less effective for question answering tasks. This is because classification and similarity tasks rely on overall judgments of the text and do not require as much detailed information as question answering. (3) Summaries generated by fine-tuned models exhibit consistent utility across various tasks, as they are insensitive to task differences and have a stable ranking that resembles human summaries. Conversely, zero-shot and simple extractive summaries demonstrate varying rankings across tasks. (4) Summaries generated by fine-tuned models tend to perform better on QA tasks, while summaries generated by models in the zero-shot setting are more suitable for classification and similarity tasks. This is due to the fact that summaries generated by fine-tuned models are extractive and specific, including details such as times, places, and numbers, while summaries generated by models in the zero-shot setting are more general. (5) Intrinsic automatic metrics are suitable for assessing usefulness of summaries in QA tasks, but their utility may be limited when it comes to tasks where people are required to make an overall judgment about the text, such as classification and similarity tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CONCLUSIONS",
                "sec_num": "7"
            },
            {
                "text": "The length of the summary can affect the information contained in the text. Therefore, in order to ensure fairness in comparing summaries across different systems, we set a range for the number of words in the generated summary based on the length of the reference summary, so that as shown in figure 6 , the summaries of all systems fall within a similar length interval.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 301,
                        "end": 302,
                        "text": "6",
                        "ref_id": "FIGREF7"
                    }
                ],
                "eq_spans": [],
                "section": "A LENGTH OF SUMMARIES FROM DIFFERENT SYSTEMS",
                "sec_num": null
            },
            {
                "text": "Here we report summary-level correlations between proposed extrinsic metrics with Kendall's \ud835\udf0f and Pearson's r (shown in Figure 8 ) and summary-level correlations between proposed extrinsic metrics with Pearson's r (shown in Figure 7 ). ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 127,
                        "end": 128,
                        "text": "8",
                        "ref_id": "FIGREF9"
                    },
                    {
                        "start": 231,
                        "end": 232,
                        "text": "7",
                        "ref_id": "FIGREF8"
                    }
                ],
                "eq_spans": [],
                "section": "B CORRELATION BETWEEN EXTRINSIC METRICS",
                "sec_num": null
            },
            {
                "text": "It's important to note that this observation is only based on the summaries fine-tuned on the CNN/DailyMail dataset. Fine-tuning on other datasets may produce different results and therefore cannot be generalized as all summaries generated by fine-tuned",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
                "authors": [
                    {
                        "first": "Satanjeev",
                        "middle": [],
                        "last": "Banerjee",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization",
                "volume": "",
                "issue": "",
                "pages": "65--72",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 65-72.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Atabak Ashfaq, Pengfei Liu, and Graham Neubig",
                "authors": [
                    {
                        "first": "Manik",
                        "middle": [],
                        "last": "Bhandari",
                        "suffix": ""
                    },
                    {
                        "first": "Pranav",
                        "middle": [],
                        "last": "Gour",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Re-evaluating evaluation in text summarization",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2010.07100"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neu- big. 2020. Re-evaluating evaluation in text summarization. arXiv preprint arXiv:2010.07100 (2020).",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Language models are few-shot learners",
                "authors": [
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Mann",
                        "suffix": ""
                    },
                    {
                        "first": "Nick",
                        "middle": [],
                        "last": "Ryder",
                        "suffix": ""
                    },
                    {
                        "first": "Melanie",
                        "middle": [],
                        "last": "Subbiah",
                        "suffix": ""
                    },
                    {
                        "first": "Jared",
                        "middle": [
                            "D"
                        ],
                        "last": "Kaplan",
                        "suffix": ""
                    },
                    {
                        "first": "Prafulla",
                        "middle": [],
                        "last": "Dhariwal",
                        "suffix": ""
                    },
                    {
                        "first": "Arvind",
                        "middle": [],
                        "last": "Neelakantan",
                        "suffix": ""
                    },
                    {
                        "first": "Pranav",
                        "middle": [],
                        "last": "Shyam",
                        "suffix": ""
                    },
                    {
                        "first": "Girish",
                        "middle": [],
                        "last": "Sastry",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Advances in neural information processing systems",
                "volume": "33",
                "issue": "",
                "pages": "1877--1901",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "SemEval-2022 Task 8: Multilingual news article similarity",
                "authors": [
                    {
                        "first": "Xi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Zeynali",
                        "suffix": ""
                    },
                    {
                        "first": "Chico",
                        "middle": [],
                        "last": "Camargo",
                        "suffix": ""
                    },
                    {
                        "first": "Fabian",
                        "middle": [],
                        "last": "Fl\u00f6ck",
                        "suffix": ""
                    },
                    {
                        "first": "Devin",
                        "middle": [],
                        "last": "Gaffney",
                        "suffix": ""
                    },
                    {
                        "first": "Przemyslaw",
                        "middle": [],
                        "last": "Grabowicz",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Hale",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Jurgens",
                        "suffix": ""
                    },
                    {
                        "first": "Mattia",
                        "middle": [],
                        "last": "Samory",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)",
                "volume": "",
                "issue": "",
                "pages": "1094--1106",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2022.semeval-1.155"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xi Chen, Ali Zeynali, Chico Camargo, Fabian Fl\u00f6ck, Devin Gaffney, Przemyslaw Grabowicz, Scott Hale, David Jurgens, and Mattia Samory. 2022. SemEval-2022 Task 8: Multilingual news article similarity. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022). Association for Computational Linguistics, Seattle, United States, 1094-1106. https://doi.org/10.18653/v1/2022. semeval-1.155",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Sentence mover's similarity: Automatic evaluation for multi-sentence texts",
                "authors": [
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Asli",
                        "middle": [],
                        "last": "Celikyilmaz",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "2748--2760",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Elizabeth Clark, Asli Celikyilmaz, and Noah A Smith. 2019. Sentence mover's similarity: Automatic evaluation for multi-sentence texts. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2748-2760.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "A methodology for extrinsic evaluation of text summarization: does ROUGE correlate?",
                "authors": [
                    {
                        "first": "Bonnie",
                        "middle": [],
                        "last": "Dorr",
                        "suffix": ""
                    },
                    {
                        "first": "Christof",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Schwartz",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Zajic",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
                "volume": "",
                "issue": "",
                "pages": "1--8",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bonnie Dorr, Christof Monz, Richard Schwartz, and David Zajic. 2005. A method- ology for extrinsic evaluation of text summarization: does ROUGE correlate?. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. 1-8.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
                "authors": [
                    {
                        "first": "G\u00fcnes",
                        "middle": [],
                        "last": "Erkan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dragomir R Radev",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Journal of artificial intelligence research",
                "volume": "22",
                "issue": "",
                "pages": "457--479",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G\u00fcnes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal of artificial intelligence research 22 (2004), 457-479.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Summeval: Re-evaluating summarization evaluation",
                "authors": [
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Alexander R Fabbri",
                        "suffix": ""
                    },
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Kry\u015bci\u0144ski",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Mccann",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Dragomir",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Radev",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "9",
                "issue": "",
                "pages": "391--409",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexander R Fabbri, Wojciech Kry\u015bci\u0144ski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summa- rization evaluation. Transactions of the Association for Computational Linguistics 9 (2021), 391-409.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Bootstrapping dialog systems with word embeddings",
                "authors": [
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Forgues",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    },
                    {
                        "first": "Jean-Marie",
                        "middle": [],
                        "last": "Larchev\u00eaque",
                        "suffix": ""
                    },
                    {
                        "first": "R\u00e9al",
                        "middle": [],
                        "last": "Tremblay",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Nips, modern machine learning and natural language processing workshop",
                "volume": "2",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gabriel Forgues, Joelle Pineau, Jean-Marie Larchev\u00eaque, and R\u00e9al Tremblay. 2014. Bootstrapping dialog systems with word embeddings. In Nips, modern machine learning and natural language processing workshop, Vol. 2. 168.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Non-expert evaluation of summarization systems is risky",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Gillick",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk",
                "volume": "",
                "issue": "",
                "pages": "148--151",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Gillick and Yang Liu. 2010. Non-expert evaluation of summarization systems is risky. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk. 148-151.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "News summarization and evaluation in the era of gpt-3",
                "authors": [
                    {
                        "first": "Tanya",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Junyi",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Jessy",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Durrett",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2209.12356"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356 (2022).",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
                "authors": [
                    {
                        "first": "Tianxing",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jingyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Tianle",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Sachin",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Glass",
                        "suffix": ""
                    },
                    {
                        "first": "Yulia",
                        "middle": [],
                        "last": "Tsvetkov",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2212.10020"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, and Yulia Tsvetkov. 2022. On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. arXiv preprint arXiv:2212.10020 (2022).",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Teaching machines to read and comprehend",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Moritz Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Kocisky",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Lasse",
                        "middle": [],
                        "last": "Espeholt",
                        "suffix": ""
                    },
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Kay",
                        "suffix": ""
                    },
                    {
                        "first": "Mustafa",
                        "middle": [],
                        "last": "Suleyman",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in neural information processing systems",
                "volume": "28",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems 28 (2015).",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "An extrinsic evaluation for question-biased text summarization on QA tasks",
                "authors": [
                    {
                        "first": "Tsutomu",
                        "middle": [],
                        "last": "Hirao",
                        "suffix": ""
                    },
                    {
                        "first": "Yutaka",
                        "middle": [],
                        "last": "Sasaki",
                        "suffix": ""
                    },
                    {
                        "first": "Hideki",
                        "middle": [],
                        "last": "Isozaki",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proc. of the NAACL 2001 Workshop on Automatic Summarization",
                "volume": "",
                "issue": "",
                "pages": "61--68",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tsutomu Hirao, Yutaka Sasaki, and Hideki Isozaki. 2001. An extrinsic evaluation for question-biased text summarization on QA tasks. In Proc. of the NAACL 2001 Workshop on Automatic Summarization. 61-68.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Automated text summarization and the SUMMARIST system",
                "authors": [
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    },
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eduard Hovy and Chin-Yew Lin. 1998. Automated text summarization and the SUMMARIST system. Technical Report. UNIVERSITY OF SOUTHERN CALIFOR- NIA MARINA DEL REY INFORMATION SCIENCES INST.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "On the Subjectivity of Human Authored Summaries",
                "authors": [
                    {
                        "first": "Balakrishna",
                        "middle": [],
                        "last": "Kolluru",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshihiko",
                        "middle": [],
                        "last": "Gotoh",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
                "volume": "",
                "issue": "",
                "pages": "9--16",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "BalaKrishna Kolluru and Yoshihiko Gotoh. 2005. On the Subjectivity of Human Authored Summaries. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. 9-16.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Evaluating the Factual Consistency of Abstractive Text Summarization",
                "authors": [
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Kryscinski",
                        "suffix": ""
                    },
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Mccann",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "9332--9346",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.emnlp-main.750"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the Factual Consistency of Abstractive Text Summarization. In Pro- ceedings of the 2020 Conference on Empirical Methods in Natural Language Pro- cessing (EMNLP). Association for Computational Linguistics, Online, 9332-9346. https://doi.org/10.18653/v1/2020.emnlp-main.750",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Optimizing text summarization based on fuzzy logic",
                "authors": [
                    {
                        "first": "Farshad",
                        "middle": [],
                        "last": "Kyoomarsi",
                        "suffix": ""
                    },
                    {
                        "first": "Hamid",
                        "middle": [],
                        "last": "Khosravi",
                        "suffix": ""
                    },
                    {
                        "first": "Esfandiar",
                        "middle": [],
                        "last": "Eslami",
                        "suffix": ""
                    },
                    {
                        "first": "Pooya",
                        "middle": [],
                        "last": "Khosravyan Dehkordy",
                        "suffix": ""
                    },
                    {
                        "first": "Asghar",
                        "middle": [],
                        "last": "Tajoddin",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Seventh IEEE/ACIS International Conference on Computer and Information Science",
                "volume": "",
                "issue": "",
                "pages": "347--352",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Farshad Kyoomarsi, Hamid Khosravi, Esfandiar Eslami, Pooya Khosravyan Dehkordy, and Asghar Tajoddin. 2008. Optimizing text summarization based on fuzzy logic. In Seventh IEEE/ACIS International Conference on Computer and Information Science (icis 2008). IEEE, 347-352.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Thomas",
                        "suffix": ""
                    },
                    {
                        "first": "Susan",
                        "middle": [
                            "T"
                        ],
                        "last": "Landauer",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dumais",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Psychological review",
                "volume": "104",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thomas K Landauer and Susan T Dumais. 1997. A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review 104, 2 (1997), 211.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Marjan",
                        "middle": [],
                        "last": "Ghazvininejad",
                        "suffix": ""
                    },
                    {
                        "first": "Abdelrahman",
                        "middle": [],
                        "last": "Mohamed",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Ves",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1910.13461"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 (2019).",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Rouge: A package for automatic evaluation of summaries",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Text summarization branches out",
                "volume": "",
                "issue": "",
                "pages": "74--81",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. 74-81.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Fine-tune BERT for extractive summarization",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1903.10318"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu. 2019. Fine-tune BERT for extractive summarization. arXiv preprint arXiv:1903.10318 (2019).",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Text Summarization with Pretrained Encoders",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "3730--3740",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1387"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu and Mirella Lapata. 2019. Text Summarization with Pretrained Encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 3730-3740. https://doi.org/10.18653/v1/D19-1387",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "BRIO: Bringing order to abstractive summarization",
                "authors": [
                    {
                        "first": "Yixin",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Dragomir",
                        "middle": [],
                        "last": "Radev",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2203.16804"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. 2022. BRIO: Bringing order to abstractive summarization. arXiv preprint arXiv:2203.16804 (2022).",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Automatic text summarization using latent semantic analysis",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "I"
                        ],
                        "last": "Igor V Mashechkin",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Petrovskiy",
                        "suffix": ""
                    },
                    {
                        "first": "Dmitry",
                        "middle": [
                            "V"
                        ],
                        "last": "Popov",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Tsarev",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Programming and Computer Software",
                "volume": "37",
                "issue": "",
                "pages": "299--305",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Igor V Mashechkin, MI Petrovskiy, DS Popov, and Dmitry V Tsarev. 2011. Au- tomatic text summarization using latent semantic analysis. Programming and Computer Software 37 (2011), 299-305.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Textrank: Bringing order into text",
                "authors": [
                    {
                        "first": "Rada",
                        "middle": [],
                        "last": "Mihalcea",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Tarau",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 2004 conference on empirical methods in natural language processing",
                "volume": "",
                "issue": "",
                "pages": "404--411",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Pro- ceedings of the 2004 conference on empirical methods in natural language processing. 404-411.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
                "authors": [
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Nallapati",
                        "suffix": ""
                    },
                    {
                        "first": "Feifei",
                        "middle": [],
                        "last": "Zhai",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the AAAI conference on artificial intelligence",
                "volume": "31",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. In Proceedings of the AAAI conference on artificial intelligence, Vol. 31.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Abstractive text summarization using sequence-to-sequence rnns and beyond",
                "authors": [
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Nallapati",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Caglar",
                        "middle": [],
                        "last": "Gulcehre",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1602.06023"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. 2016. Ab- stractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023 (2016).",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Ranking sentences for extractive summarization with reinforcement learning",
                "authors": [
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Shay",
                        "middle": [
                            "B"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1802.08636"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Ranking sentences for extractive summarization with reinforcement learning. arXiv preprint arXiv:1802.08636 (2018).",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Evaluating content selection in summarization: The pyramid method",
                "authors": [
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    },
                    {
                        "first": "Rebecca",
                        "middle": [
                            "J"
                        ],
                        "last": "Passonneau",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the human language technology conference of the north american chapter of the association for computational linguistics: Hlt-naacl",
                "volume": "",
                "issue": "",
                "pages": "145--152",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ani Nenkova and Rebecca J Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of the human language tech- nology conference of the north american chapter of the association for computational linguistics: Hlt-naacl 2004. 145-152.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Better summarization evaluation with word embeddings for ROUGE",
                "authors": [
                    {
                        "first": "Jun-Ping",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Viktoria",
                        "middle": [],
                        "last": "Abrecht",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1508.06034"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jun-Ping Ng and Viktoria Abrecht. 2015. Better summarization evaluation with word embeddings for ROUGE. arXiv preprint arXiv:1508.06034 (2015).",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Training language models to follow instructions with human feedback",
                "authors": [
                    {
                        "first": "Long",
                        "middle": [],
                        "last": "Ouyang",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Diogo",
                        "middle": [],
                        "last": "Almeida",
                        "suffix": ""
                    },
                    {
                        "first": "Carroll",
                        "middle": [
                            "L"
                        ],
                        "last": "Wainwright",
                        "suffix": ""
                    },
                    {
                        "first": "Pamela",
                        "middle": [],
                        "last": "Mishkin",
                        "suffix": ""
                    },
                    {
                        "first": "Chong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Sandhini",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Katarina",
                        "middle": [],
                        "last": "Slama",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Ray",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Schulman",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Hilton",
                        "suffix": ""
                    },
                    {
                        "first": "Fraser",
                        "middle": [],
                        "last": "Kelton",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "Maddie",
                        "middle": [],
                        "last": "Simens",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Welinder",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Christiano",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Leike",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul- man, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. (2022).",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Text summarization using latent semantic analysis",
                "authors": [
                    {
                        "first": "Ferda",
                        "middle": [],
                        "last": "Makbule Gulcin Ozsoy",
                        "suffix": ""
                    },
                    {
                        "first": "Ilyas",
                        "middle": [],
                        "last": "Nur Alpaslan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Cicekli",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Journal of Information Science",
                "volume": "37",
                "issue": "",
                "pages": "405--417",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Makbule Gulcin Ozsoy, Ferda Nur Alpaslan, and Ilyas Cicekli. 2011. Text sum- marization using latent semantic analysis. Journal of Information Science 37, 4 (2011), 405-417.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311-318.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "chrF++: words helping character n-grams",
                "authors": [
                    {
                        "first": "Maja",
                        "middle": [],
                        "last": "Popovi\u0107",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Second Conference on Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "612--618",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W17-4770"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maja Popovi\u0107. 2017. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation. Association for Computational Linguistics, Copenhagen, Denmark, 612-618. https://doi.org/10.18653/v1/W17- 4770",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sharan",
                        "middle": [],
                        "last": "Narang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Matena",
                        "suffix": ""
                    },
                    {
                        "first": "Yanqi",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "The Journal of Machine Learning Research",
                "volume": "21",
                "issue": "1",
                "pages": "5485--5551",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485-5551.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "A Comparison of Greedy and Optimal Assessment of Natural Language Student Input Using Word-to-Word Similarity Metrics",
                "authors": [
                    {
                        "first": "Vasile",
                        "middle": [],
                        "last": "Rus",
                        "suffix": ""
                    },
                    {
                        "first": "Mihai",
                        "middle": [],
                        "last": "Lintean",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP",
                "volume": "",
                "issue": "",
                "pages": "157--162",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vasile Rus and Mihai Lintean. 2012. A Comparison of Greedy and Optimal As- sessment of Natural Language Student Input Using Word-to-Word Similarity Metrics. In Proceedings of the Seventh Workshop on Building Educational Applica- tions Using NLP. Association for Computational Linguistics, Montr\u00e9al, Canada, 157-162. https://aclanthology.org/W12-2018",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "The New York Times Annotated Corpus",
                "authors": [
                    {
                        "first": "Evan",
                        "middle": [],
                        "last": "Sandhaus",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.35111/77ba-9x74"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Evan Sandhaus. 2008. The New York Times Annotated Corpus. (2008). https: //doi.org/10.35111/77ba-9x74",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Multitask prompted training enables zero-shot task generalization",
                "authors": [
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Sanh",
                        "suffix": ""
                    },
                    {
                        "first": "Albert",
                        "middle": [],
                        "last": "Webson",
                        "suffix": ""
                    },
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [
                            "H"
                        ],
                        "last": "Bach",
                        "suffix": ""
                    },
                    {
                        "first": "Lintang",
                        "middle": [],
                        "last": "Sutawika",
                        "suffix": ""
                    },
                    {
                        "first": "Zaid",
                        "middle": [],
                        "last": "Alyafeai",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Chaffin",
                        "suffix": ""
                    },
                    {
                        "first": "Arnaud",
                        "middle": [],
                        "last": "Stiegler",
                        "suffix": ""
                    },
                    {
                        "first": "Teven",
                        "middle": [],
                        "last": "Le Scao",
                        "suffix": ""
                    },
                    {
                        "first": "Arun",
                        "middle": [],
                        "last": "Raja",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2110.08207"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207 (2021).",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Answers unite! unsupervised metrics for reinforced summarization models",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Scialom",
                        "suffix": ""
                    },
                    {
                        "first": "Sylvain",
                        "middle": [],
                        "last": "Lamprier",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Piwowarski",
                        "suffix": ""
                    },
                    {
                        "first": "Jacopo",
                        "middle": [],
                        "last": "Staiano",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.01610"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2019. Answers unite! unsupervised metrics for reinforced summarization models. arXiv preprint arXiv:1909.01610 (2019).",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Crowdsourcing lightweight pyramids for manual summary evaluation",
                "authors": [
                    {
                        "first": "Ori",
                        "middle": [],
                        "last": "Shapira",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Gabay",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Hadar",
                        "middle": [],
                        "last": "Ronen",
                        "suffix": ""
                    },
                    {
                        "first": "Ramakanth",
                        "middle": [],
                        "last": "Pasunuru",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    },
                    {
                        "first": "Yael",
                        "middle": [],
                        "last": "Amsterdamer",
                        "suffix": ""
                    },
                    {
                        "first": "Ido",
                        "middle": [],
                        "last": "Dagan",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.05929"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ramakanth Pasunuru, Mohit Bansal, Yael Amsterdamer, and Ido Dagan. 2019. Crowdsourcing lightweight pyramids for manual summary evaluation. arXiv preprint arXiv:1904.05929 (2019).",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Sentence features fusion for text summarization using fuzzy logic",
                "authors": [
                    {
                        "first": "Ladda",
                        "middle": [],
                        "last": "Suanmali",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammed",
                        "middle": [],
                        "last": "Salem Binwahlan",
                        "suffix": ""
                    },
                    {
                        "first": "Naomie",
                        "middle": [],
                        "last": "Salim",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "2009 Ninth International Conference on Hybrid Intelligent Systems",
                "volume": "1",
                "issue": "",
                "pages": "142--146",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ladda Suanmali, Mohammed Salem Binwahlan, and Naomie Salim. 2009. Sen- tence features fusion for text summarization using fuzzy logic. In 2009 Ninth International Conference on Hybrid Intelligent Systems, Vol. 1. IEEE, 142-146.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Cider: Consensus-based image description evaluation",
                "authors": [
                    {
                        "first": "Ramakrishna",
                        "middle": [],
                        "last": "Vedantam",
                        "suffix": ""
                    },
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Zitnick",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "4566--4575",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE confer- ence on computer vision and pattern recognition. 4566-4575.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Extractive summarization using deep learning",
                "authors": [
                    {
                        "first": "Sukriti",
                        "middle": [],
                        "last": "Verma",
                        "suffix": ""
                    },
                    {
                        "first": "Vagisha",
                        "middle": [],
                        "last": "Nidhi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1708.04439"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sukriti Verma and Vagisha Nidhi. 2017. Extractive summarization using deep learning. arXiv preprint arXiv:1708.04439 (2017).",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
                "authors": [
                    {
                        "first": "Jingqing",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yao",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Saleh",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "11328--11339",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning. PMLR, 11328-11339.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Bertscore: Evaluating text generation with bert",
                "authors": [
                    {
                        "first": "Tianyi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Varsha",
                        "middle": [],
                        "last": "Kishore",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [
                            "Q"
                        ],
                        "last": "Weinberger",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.09675"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019).",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Maxime",
                        "middle": [],
                        "last": "Peyrard",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [
                            "M"
                        ],
                        "last": "Meyer",
                        "suffix": ""
                    },
                    {
                        "first": "Steffen",
                        "middle": [],
                        "last": "Eger",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.02622"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Stef- fen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. arXiv preprint arXiv:1909.02622 (2019).",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "uris": null,
                "fig_num": "1",
                "text": "Figure 1: A screenshot of the answer page for the QA task. The user information on the platform has been anonymized.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF1": {
                "uris": null,
                "fig_num": "2",
                "text": "Figure 2: System-level Pearson correlation of all extrinsic metrics. The result of Kendall correlation is shown in Figure 7 in the Appendix.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF2": {
                "uris": null,
                "fig_num": "3",
                "text": "Figure 3: Average ranking of different systems on three different tasks. Each ranking is calculated by averaging the rankings over extrinsic metrics for the same task.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF3": {
                "uris": null,
                "fig_num": null,
                "text": "Lexrank summary: Mr Lord , 86 , went missing from his home in St Ann 's on Monday, 6 April . ' A Nottinghamshire Police spokesperson said : ' The body of a man found in the River Trent on April 15 , 2015 , has been confirmed as that of missing John Lord . ' Message :Mr Lord 's daughter Alison said her father was grieving and had left a heartbreaking note signed [...]",
                "type_str": "figure",
                "num": null
            },
            "FIGREF4": {
                "uris": null,
                "fig_num": "365",
                "text": "64* 0.88** 0.84** 0.92** 0.93** 0.85** 0.71* 0.83* 0.71* 0.83* 0.71* 0.21 0.21 0.28 0.21 -0.01 0.14 -0.08 0.21 METEOR 0.93** 0.64* 0.88** 0.84** 0.94** 0.79** 0.91** 0.86** 0.87** 0.71* 0.89** 0.71* 0.49 0.50 0.54 0.50 0.31 0.36 0.24 0.29 CHRF 0.95** 0.64* 0.90** 0.84** 0.96** 0.93** 0.91** 0.71* 0.88** 0.71* 0.89** 0.71* 0.48 0.50 0.52 0.50 0.31 0.29 0.23 0.36 CIDEe 0.75* 0.50 0.83** 0.69* 0.85** 0.79** 0.82* 0.71* 00.00 0.20 0.00 -0.03 0.07 -0.09 0.00 BERTScore 0.94** 0.71* 0.87** 0.62* 0.93** 0.71* 0.89** 0.93** 0.85** 0.79** 0.86** 0.79** 0.54 0.43 0.59 0.43 0.54 0.43 0.48 0.36 MOVERScore 0.97** 0.79** 0.93** 0.69* 0.97** 0.79** 0.93** 0.86** 0.87** 0.71* 0.88** 0.71* 0.55 0.50 0.60 0.50 0.46 0.43 0.39 0.36 ROUGE-we 0.95** 0.71* 0.94** 0.76** 0.98** 0.86** 0.95** 0.79** 0.90** 0.64* 0.91** 0.64* 0.50 0.50 0.55 0.50 0.45 0.43 0.38 0.36 EmbeddingAverage 0.79* 0.50 0.82* 0.69* 0.86** 0.79** 0.87** 0.71* 0.85** 0.57 0.86** 0.57 0.71* 0.57 0.75 0.57 0.56 0.50 0.51 0.43 VectorExtrema 0.80* 0.57 0.80* 0.76** 0.86** 0.86** 0.82* 0.64* 0.84** 0.64* 0.84** 0.64* 0.37 0.21 0.42 0.21 0.40 0.36 0.33 0.29 GreedyMatching 0.89** 0.64* 0.80* 0.69* 0.88** 0.79** 0.85** 0.71* 0.85** 0.71* 0.86** 0.71* 0.60 0.50 0.64 0.50 0.43 0.50 0.36 0.43 SummaQA 0.87** 0.57 0.85** 0.62* 0.91** 0.71* 0.93** 0.79** 0.87** 0.64* 0.89** 0.64* 0.24 0.21 0.30 0.21 0.43 0.43 0.35 0.Pearson's r and Kendall's \ud835\udf0f between intrinsic automatic metrics and extrinsic Criteria. Significance is indicated by * for p-values less than or equal to 0.05 and ** for p-values less than or equal to 0.01.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF5": {
                "uris": null,
                "fig_num": "4",
                "text": "Figure 4: Summary-level correlation between intrinsic automatic metrics and extrinsic criteria.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF6": {
                "uris": null,
                "fig_num": "5",
                "text": "Figure 5: System-level Pearson correlations between intrinsic automatic metrics and proposed extrinsic metrics on top-k systems.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF7": {
                "uris": null,
                "fig_num": "6",
                "text": "Figure 6: Length of summaries from different systems in three tasks.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF8": {
                "uris": null,
                "fig_num": "7",
                "text": "Figure 7: System-level Kendall correlations of all extrinsic metrics.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF9": {
                "uris": null,
                "fig_num": "8",
                "text": "Figure 8: Summary-level Kendall(left) and Pearson(right) correlations of extrinsic metrics in the QA task.",
                "type_str": "figure",
                "num": null
            },
            "TABREF0": {
                "text": "Usefulness of different systems on downstream tasks, including the average time taken by participants to complete tasks with different system outputs and results of extrinsic metrics based on user performance.",
                "content": "<table><tr><td>system</td><td/><td colspan=\"2\">QA (ref-based)</td><td/><td/><td/><td colspan=\"3\">QA (source-based)</td><td/><td colspan=\"3\">Classification</td><td>Similarity</td></tr><tr><td/><td>answerable</td><td>EM</td><td>F1</td><td colspan=\"3\">time(seconds) answerable</td><td>EM</td><td>F1</td><td colspan=\"2\">time(seconds)</td><td>EM</td><td>F1</td><td>time(seconds)</td><td>MSE</td><td>\ud835\udf0c</td><td>time(seconds)</td></tr><tr><td>source</td><td>0.8550</td><td colspan=\"2\">0.3225 0.5077</td><td colspan=\"2\">280.04</td><td>0.8875</td><td colspan=\"2\">0.5050 0.6796</td><td>211.64</td><td colspan=\"3\">0.8827 0.8951</td><td>72.97</td><td>0.9136 0.6184</td><td>37.74</td></tr><tr><td>reference</td><td>0.8875</td><td colspan=\"2\">0.5400 0.7535</td><td colspan=\"2\">93.94</td><td>0.5375</td><td colspan=\"2\">0.2725 0.3746</td><td>83.3</td><td colspan=\"3\">0.9127 0.9156</td><td>34.37</td><td>0.7736 0.7060</td><td>19.92</td></tr><tr><td>bart</td><td>0.4975</td><td colspan=\"2\">0.2400 0.3240</td><td colspan=\"2\">108.37</td><td>0.4900</td><td colspan=\"2\">0.2325 0.3197</td><td>83.05</td><td colspan=\"3\">0.8964 0.9015</td><td>25.43</td><td>0.9803 0.6085</td><td>21.94</td></tr><tr><td>pegasus</td><td>0.5475</td><td colspan=\"2\">0.2100 0.3222</td><td colspan=\"2\">112.55</td><td>0.5125</td><td colspan=\"2\">0.2825 0.3662</td><td>89.66</td><td colspan=\"3\">0.8900 0.8942</td><td>29.88</td><td>0.9836 0.6014</td><td>23.93</td></tr><tr><td>lexrank</td><td>0.3625</td><td colspan=\"2\">0.0900 0.1631</td><td colspan=\"2\">111.78</td><td>0.3775</td><td colspan=\"2\">0.1500 0.2291</td><td>92.01</td><td colspan=\"3\">0.9000 0.9017</td><td>29.88</td><td>1.2403 0.5323</td><td>23.77</td></tr><tr><td>Lead-n</td><td>0.4175</td><td colspan=\"2\">0.1600 0.2483</td><td colspan=\"2\">110.78</td><td>0.4775</td><td colspan=\"2\">0.2475 0.3342</td><td>84.33</td><td colspan=\"3\">0.8773 0.8792</td><td>31.29</td><td>1.4336 0.4536</td><td>23.42</td></tr><tr><td>BRIO</td><td>0.5825</td><td colspan=\"2\">0.2350 0.3598</td><td colspan=\"2\">104.21</td><td>0.5425</td><td colspan=\"2\">0.3075 0.4040</td><td>90.15</td><td colspan=\"3\">0.9000 0.9036</td><td>25.9</td><td>0.7569 0.6998</td><td>21.07</td></tr><tr><td>t5</td><td>0.4400</td><td colspan=\"2\">0.1600 0.2416</td><td colspan=\"2\">106.07</td><td>0.4375</td><td colspan=\"2\">0.2075 0.2861</td><td>86.57</td><td colspan=\"3\">0.8791 0.8814</td><td>34.86</td><td>1.3736 0.4699</td><td>20.17</td></tr><tr><td>t0</td><td>0.5350</td><td colspan=\"2\">0.1875 0.3003</td><td colspan=\"2\">107.21</td><td>0.5100</td><td colspan=\"2\">0.2600 0.3530</td><td>98.6</td><td colspan=\"3\">0.8864 0.8889</td><td>28.57</td><td>0.7669 0.7087</td><td>20.96</td></tr><tr><td>gpt3</td><td>0.4200</td><td colspan=\"2\">0.1575 0.2338</td><td colspan=\"2\">100.02</td><td>0.4500</td><td colspan=\"2\">0.1975 0.2855</td><td>83.74</td><td colspan=\"3\">0.9036 0.9068</td><td>29.11</td><td>0.8469 0.6741</td><td>20.66</td></tr><tr><td/><td/><td/><td/><td/><td colspan=\"3\">QA (ref-based)</td><td/><td/><td/><td/><td/><td>QA (source-based)</td></tr><tr><td/><td/><td colspan=\"3\">Answerable</td><td>EM</td><td/><td>F1</td><td colspan=\"4\">Time(seconds) Answerable</td><td/><td>EM</td><td>F1</td><td>Time(seconds)</td></tr><tr><td>Source</td><td/><td>0.86</td><td/><td>0.32</td><td/><td>0.51</td><td/><td>280</td><td/><td>0.89</td><td/><td colspan=\"2\">0.51</td><td>0.7</td><td>212</td></tr><tr><td colspan=\"4\">Reference Summaries 0.89 +4%</td><td colspan=\"5\">0.54 +67% 0.75 +48% 94</td><td>-66%</td><td colspan=\"4\">0.54 -39% 0.27 -46% 0.4 -45% 88</td><td>-58%</td></tr><tr><td colspan=\"2\">All Summaries</td><td colspan=\"8\">0.52 -39% 0.22 -32% 0.33 -36% 106 -62%</td><td colspan=\"4\">0.52 -41% 0.24 -53% 0.3 -52% 83</td><td>-61%</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF1": {
                "text": "Summaries compared to source texts in the QA tasks. The red percentages indicate that summaries are better compared to the source text, i.e. participants take less time or perform better in completing the task. The green ones indicate the opposite. Although the summaries represent a significant time saving, participants perform worse in QA tasks using the summaries compared to source texts.",
                "content": "<table><tr><td/><td/><td colspan=\"3\">Classification</td><td/><td/><td/><td>Similarity</td></tr><tr><td/><td>EM</td><td>F1</td><td/><td colspan=\"2\">Time(seconds)</td><td>MSE</td><td colspan=\"3\">Spearman's \ud835\udf0c Time(seconds)</td></tr><tr><td>Source</td><td>0.88</td><td>0.90</td><td/><td>73</td><td/><td>0.91</td><td>0.6</td><td/><td>38</td></tr><tr><td colspan=\"5\">Reference Summaries 0.91 +3% 0.92 +2% 34</td><td>-53%</td><td colspan=\"2\">0.77 -15% 0.7</td><td>+14%</td><td>20</td><td>-47%</td></tr><tr><td>All Summaries</td><td colspan=\"2\">0.89 +1% 0.90</td><td>-</td><td>30</td><td>-59%</td><td colspan=\"2\">1.02 +11% 0.6</td><td>-</td><td>22</td><td>-42%</td></tr><tr><td>Table</td><td/><td/><td/><td/><td/><td/><td/><td/></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF2": {
                "text": "* 0.71* 0.94** 0.76** 0.98** 0.86** 0.95** 0.79** 0.89** 0.64* 0.91** 0.64* 0.51 0.50 0.56 0.50 0.48 0.43 0.40 0.36 ROUGE-2 0.97** 0.79** 0.94** 0.91** 0.98** 0.93** 0.92** 0.71* 0.89** 0.71* 0.89** 0.71* 0.23 0.21 0.29 0.21 0.18 0.29 0.10 0.36 ROUGE-L 0.99** 0.93** 0.93** 0.76** 0.97** 0.79** 0.91** 0.71* 0.87** 0.71* 0.87** 0.71* 0.33 0.43 0.40 0.43 0.29 0.29 0.22 0.36 BLEU 0.89** 0.",
                "content": "<table><tr><td>Extrinsic Criteria</td><td/><td/><td colspan=\"2\">QA (ref-based)</td><td/><td/><td/><td/><td colspan=\"2\">QA (source-based)</td><td/><td/><td colspan=\"3\">Classification</td><td/><td colspan=\"3\">Similarity</td><td/></tr><tr><td/><td colspan=\"2\">answerable</td><td>EM</td><td/><td>F1</td><td/><td colspan=\"2\">answerable</td><td>EM</td><td/><td>F1</td><td/><td>EM</td><td/><td>F1</td><td/><td>MSE</td><td/><td>\ud835\udf0c</td><td/></tr><tr><td>Automatic Metrics</td><td>r</td><td>\ud835\udf0f</td><td>r</td><td>\ud835\udf0f</td><td>r</td><td>\ud835\udf0f</td><td>r</td><td>\ud835\udf0f</td><td>r</td><td>\ud835\udf0f</td><td>r</td><td>\ud835\udf0f</td><td>r</td><td>\ud835\udf0f</td><td>r</td><td>\ud835\udf0f</td><td>r</td><td>\ud835\udf0f</td><td>r</td><td>\ud835\udf0f</td></tr><tr><td>ROUGE-1</td><td>0.95*</td><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            }
        }
    }
}
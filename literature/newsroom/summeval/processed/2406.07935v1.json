{
    "paper_id": "2406",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2025-02-14T13:47:37.086044Z"
    },
    "title": "Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation",
    "authors": [
        {
            "first": "Jie",
            "middle": [],
            "last": "Ruan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Peking University",
                "location": {}
            },
            "email": "ruanjie@stu.pku.edu"
        },
        {
            "first": "Wenqing",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Peking University",
                "location": {}
            },
            "email": "wangwenqing@stu.pku.edu"
        },
        {
            "first": "Xiaojun",
            "middle": [],
            "last": "Wan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Peking University",
                "location": {}
            },
            "email": "wanxiaojun@pku.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Human evaluation serves as the gold standard for assessing the quality of Natural Language Generation (NLG) systems. Nevertheless, the evaluation guideline, as a pivotal element ensuring reliable and reproducible human assessment, has received limited attention. Our investigation revealed that only 29.84% of recent papers involving human evaluation at top conferences release their evaluation guidelines, with vulnerabilities identified in 77.09% of these guidelines. Unreliable evaluation guidelines can yield inaccurate assessment outcomes, potentially impeding the advancement of NLG in the right direction. To address these challenges, we take an initial step towards reliable evaluation guidelines and propose the first human evaluation guideline dataset by collecting annotations of guidelines extracted from existing papers as well as generated via Large Language Models (LLMs). We then introduce a taxonomy of eight vulnerabilities and formulate a principle for composing evaluation guidelines. Furthermore, a method for detecting guideline vulnerabilities has been explored using LLMs, and we offer a set of recommendations to enhance reliability in human evaluation. The annotated human evaluation guideline dataset and code for the vulnerability detection method are publicly available online. 1 * Equal contribution.",
    "pdf_parse": {
        "paper_id": "2406",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Human evaluation serves as the gold standard for assessing the quality of Natural Language Generation (NLG) systems. Nevertheless, the evaluation guideline, as a pivotal element ensuring reliable and reproducible human assessment, has received limited attention. Our investigation revealed that only 29.84% of recent papers involving human evaluation at top conferences release their evaluation guidelines, with vulnerabilities identified in 77.09% of these guidelines. Unreliable evaluation guidelines can yield inaccurate assessment outcomes, potentially impeding the advancement of NLG in the right direction. To address these challenges, we take an initial step towards reliable evaluation guidelines and propose the first human evaluation guideline dataset by collecting annotations of guidelines extracted from existing papers as well as generated via Large Language Models (LLMs). We then introduce a taxonomy of eight vulnerabilities and formulate a principle for composing evaluation guidelines. Furthermore, a method for detecting guideline vulnerabilities has been explored using LLMs, and we offer a set of recommendations to enhance reliability in human evaluation. The annotated human evaluation guideline dataset and code for the vulnerability detection method are publicly available online. 1 * Equal contribution.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Natural Language Generation (NLG) has found extensive applications across diverse domains. Nevertheless, evaluating the quality of generated outputs has posed a longstanding and formidable challenge due to the inherent diversity of expressions capable of conveying the same meaning (Howcroft et al., 2020; Zhou et al., 2022) . This abundance of possible variations complicates the development of automated evaluation methods (Novikova et al., 2017b; Reiter and Belz, 2009a) , thus necessitating the reliance on human evaluation as the gold standard and regarding it as a more reliable evaluation method in NLG (Celikyilmaz et al., 2020; Gatt and Krahmer, 2018; Gkatzia and Mahamood, 2015b; Mellish and Dale, 1998; van der Lee et al., 2018) .",
                "cite_spans": [
                    {
                        "start": 282,
                        "end": 305,
                        "text": "(Howcroft et al., 2020;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 306,
                        "end": 324,
                        "text": "Zhou et al., 2022)",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 425,
                        "end": 449,
                        "text": "(Novikova et al., 2017b;",
                        "ref_id": null
                    },
                    {
                        "start": 450,
                        "end": 473,
                        "text": "Reiter and Belz, 2009a)",
                        "ref_id": null
                    },
                    {
                        "start": 610,
                        "end": 636,
                        "text": "(Celikyilmaz et al., 2020;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 637,
                        "end": 660,
                        "text": "Gatt and Krahmer, 2018;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 661,
                        "end": 689,
                        "text": "Gkatzia and Mahamood, 2015b;",
                        "ref_id": null
                    },
                    {
                        "start": 690,
                        "end": 713,
                        "text": "Mellish and Dale, 1998;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 714,
                        "end": 739,
                        "text": "van der Lee et al., 2018)",
                        "ref_id": "BIBREF52"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "However, the evaluation guidelines, which play a crucial role in ensuring the reliability of human evaluation, have not received adequate emphasis. The transparency issues inherent in human evaluation guidelines raise concerns regarding the validity and reproducibility of the evaluation results (Schoch et al., 2020) . To investigate this issue, we conducted a study based on 3,233 papers that we crawled from ACL, EMNLP, and NAACL conferences in the last three years. Surprisingly, we indicate that only 29.84% of the papers involving human evaluation release their human evaluation guidelines. Human evaluation guidelines are crucially important for ensuring that human assessments are conducted reliably. However, when papers fail to release the evaluation guidelines, there is no guarantee of the reliability and reproducibility of their evaluation results. Moreover, our analysis of the guidelines released by these papers uncovered a significant concern: a striking 77.09% of the released guidelines exhibited noticeable vulnerabilitiesfoot_0 , which could potentially have a detrimental impact on the correctness of human evaluation outcomes (Schoch et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 296,
                        "end": 317,
                        "text": "(Schoch et al., 2020)",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 1150,
                        "end": 1171,
                        "text": "(Schoch et al., 2020)",
                        "ref_id": "BIBREF46"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The ultimate goal of establishing reliable human evaluation guidelines comprises several essential steps, which include detecting potential vulnerabilities in the guidelines, identifying specific vulnerability types, marking the precise segments with vulnerabilities, providing modification suggestions, and finally correcting identified vulnerabilities in the guidelines. In this paper, we conduct a preliminary study on defining and detecting vulnerabilities in human evaluation guidelines, marking an initial step towards reliable guidelines. Specifically, we first constructed a human evaluation guideline dataset by collecting annotations of guidelines extracted from existing papers as well as generated via LLMs. Based on the analysis of the guidelines, we identified eight main categories of vulnerabilities including Ethical Issues, Unconscious Bias, Ambiguous Definition, Unclear Rating, Edge Cases, Prior Knowledge, Inflexible Instructions, and Others. The guidelines with vulnerabilities can result in issues such as annotators being unclear about task requirements, misunderstanding specific scoring standards, or erroneously directing annotators to assign higher scores to particular systems, which leads to incorrect and irreproducible evaluation results. To detect these vulnerabilities, we explored several prompt strategies to evoke the capability of current LLMs in vulnerability detection for human evaluation guidelines, and recommend an LLM-based method employing Chain of Thought (CoT) strategies.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The main contribution of this paper is as follows: 1) We are the first to study vulnerabilities in human evaluation guidelines and release the first human evaluation guideline dataset with annotated vulnerabilities for advancing reliable human evaluation; 2) We analyze the existing human evaluation guidelines and introduce a taxonomy of eight vulnerabilities for evaluation guidelines; Furthermore, we establish a principle for writing a reliable human evaluation guideline; 3) We explore an LLM-based method for detecting guideline vulnerabilities. We recommend employing this method to assess the reliability of the guidelines before conducting human evaluations; 4) We present a set of recommendations designed to elevate the reliability of human evaluation by offering guidance on writing robust guidelines and identifying potential vulnerabilities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Due to the lack of existing work related to human evaluation guideline assessment, we construct the first human evaluation guideline dataset through two methods: extracting from existing papers and generating from GPT3.5, referred to as the authentic guidelines and synthetic guidelines, respectively. Note that we collect and analyze synthetic guidelines because LLMs has been proved as pow-erful tools for synthetic data generation (Agrawal et al., 2022; Liu et al., 2022; Bitton et al., 2023) .",
                "cite_spans": [
                    {
                        "start": 434,
                        "end": 456,
                        "text": "(Agrawal et al., 2022;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 457,
                        "end": 474,
                        "text": "Liu et al., 2022;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 475,
                        "end": 495,
                        "text": "Bitton et al., 2023)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Raw Guideline Dataset",
                "sec_num": "2"
            },
            {
                "text": "The construction of authentic guidelines involves a three-step process: First, we crawled papersfoot_1 on ACL, EMNLP, and NAACL conferences from 2020 to 2022 and obtained 3,233 raw data. Then, we filter the papers using two groups of keywords, and narrow down the paper set to 319. Human evaluation and manual assessment constitute the first group, using them to focus solely on the 677 papers related to the evaluation tasks, while guideline, instruction, questionnaire, interface, and screenshot are employed as keywords to identify papers potentially containing guideline sections. Finally, we manually filtered out papers specifically related to NLG tasks and extract 227 guidelines from ACL (111), EMNLP (62) and NAACL (54). Any guidelines presented as figures or charts were converted into textual formats. More Details of the collected data can be found in Appendix A.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Authentic Guidelines",
                "sec_num": "2.1"
            },
            {
                "text": "Constructing effective prompts for language models to perform NLP tasks is currently a highly researched topic (Schick and Sch\u00fctze, 2021; Le Scao and Rush, 2021; Tam et al., 2021; Logan IV et al., 2022; Reynolds and McDonell, 2021) . Inspired by Mishra et al. (2022) , we design 5 prompts to guide GPT-3.5-Turbo in generating diverse guidelines, including raw prompt, raw prompt with evaluation aspects, structured prompt, structured prompt with evaluation aspects and structured prompt with evaluation aspects and constraints, as shown in Appendix B. For each prompt, we expand the dataset by incorporating 12 NLG tasks and 2 evaluation settings, along with alternately utilizing the keywords instruction and guideline. Consequently, we generated a total of 48 guidelines for each prompt (12 tasks \u00d7 2 settings \u00d7 2 keywords).",
                "cite_spans": [
                    {
                        "start": 111,
                        "end": 137,
                        "text": "(Schick and Sch\u00fctze, 2021;",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 138,
                        "end": 161,
                        "text": "Le Scao and Rush, 2021;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 162,
                        "end": 179,
                        "text": "Tam et al., 2021;",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 180,
                        "end": 202,
                        "text": "Logan IV et al., 2022;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 203,
                        "end": 231,
                        "text": "Reynolds and McDonell, 2021)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 246,
                        "end": 266,
                        "text": "Mishra et al. (2022)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Synthetic Guidelines",
                "sec_num": "2.2"
            },
            {
                "text": "Finally, we obtained 227 authentic guidelines extracted from existing papers, alongside 239 4 synthetic guidelines generated by GPT-3.5-Turbo with average lengths of 247.64 words and 237.05 words, respectively. In total, our dataset comprises 466 human evaluation guidelines. It is worth noting that out of the 677 papers related to human evaluation, only 202 (29.84%) of them openly released their evaluation guidelines after considering cases where multiple guidelines were included in a single paper, indicating the insufficient attention given to the evaluation guidelines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Statistics",
                "sec_num": "2.3"
            },
            {
                "text": "We define a typology consisting of eight guideline vulnerabilities by analyzing the guidelines extracted from existing papers and generated by LLMs. An illustration for each type is shown in Table 1 , which is designed for illustrative purposes and does not originate from the actual dataset. More examples can be found in Appendix C. Ethical Issues (Mieskes, 2017) : instructions do not consider potential ethical implications related to the evaluation process, like privacy, cultural sensitivity, accessibility, or the potential misuse of the evaluation results. Unconscious Bias (Schoch et al., 2020) : instructions unconsciously favors or disadvantages certain results. Ambiguous Definition (Jurgens, 2014) : instructions for task definition are unclear, vague, or imprecise that can be interpreted in multiple ways. Unclear Rating (Amidei et al., 2019) : instructions that lack standardized criteria for evaluating aspects or definition of each point on a rating scale, resulting in potential inconsistency in ratings. Edge Cases (Ruggeri et al., 2023) : instructions do not specify how to handle edge cases or exceptional situations that don't neatly fit into the usual categories or criteria. Prior Knowledge (Sun et al., 2020) : instructions assume that evaluators have certain background knowledge or familiarity with a specific subject matter, tool, or principle. Inflexible Instructions: instructions are unnecessarily complex or rigid, making it hard for evaluators to follow and incapable of adjusting to variations in data or task requirements, which contradicts Sabou et al. (2014) 's conclusion that a simpler instruction tends to yield better results.",
                "cite_spans": [
                    {
                        "start": 350,
                        "end": 365,
                        "text": "(Mieskes, 2017)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 582,
                        "end": 603,
                        "text": "(Schoch et al., 2020)",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 695,
                        "end": 710,
                        "text": "(Jurgens, 2014)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 836,
                        "end": 857,
                        "text": "(Amidei et al., 2019)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1035,
                        "end": 1057,
                        "text": "(Ruggeri et al., 2023)",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 1216,
                        "end": 1234,
                        "text": "(Sun et al., 2020)",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 1577,
                        "end": 1596,
                        "text": "Sabou et al. (2014)",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 197,
                        "end": 198,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Taxonomy of Guideline Vulnerability",
                "sec_num": "3.1"
            },
            {
                "text": "Finally, we add the additional type Others to ensure the completeness of the typology. This covers any vulnerabilities that do not fall into the above categories.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Taxonomy of Guideline Vulnerability",
                "sec_num": "3.1"
            },
            {
                "text": "We recruit four college students who possess English qualification certificates. Firstly, they were provided with an annotation guideline, which can be found in Appendix C. Each evaluator went through a training process (details in Appendix D) to enhance their understanding in the annotation process. Before annotation, we also designed a qualification test consisting of 10 guidelines, only annotators who passed the test were considered qualified and allowed to continue annotation. To ensure the annotation quality, we divided the dataset into batches and assigned the specific number of daily tasks to each annotator. Upon receiving the daily annotations, we reviewed the results and required the specific annotator to reannotate the batch of data assigned for that day if there was a low accuracy (less than 80%).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Annotation",
                "sec_num": "3.2"
            },
            {
                "text": "In the annotation interface, the authentic guidelines and synthetic guidelines are randomly presented on the left side so as to prevent bias, while the eight vulnerability types are displayed on the right. Annotators were instructed to assign the specific vulnerability types based on the predefined typology, or indicate \"None\" for guidelines where the vulnerability type is absent. Each sample was annotated by two distinct annotators and a third annotator made the final decision if they are in disagreement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Annotation",
                "sec_num": "3.2"
            },
            {
                "text": "We utilized Cohen's kappa (Cohen, 1960) to measure the inter-annotator agreement and computed on a per-label basis so as to gain labelspecific insights. Ultimately, we calculated the mean values across all labels to assess the overall agreement. The annotation process lasted approximately two weeks, culminating in a substantial inter-annotator agreement of Cohen's kappa with \u03ba=0.722 on authentic guidelines and \u03ba=0.737 on the synthetic guidelines. More annotation details can be found in Appendix D.",
                "cite_spans": [
                    {
                        "start": 26,
                        "end": 39,
                        "text": "(Cohen, 1960)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Annotation",
                "sec_num": "3.2"
            },
            {
                "text": "Figure 1 reports the annotation results on both authentic and synthetic guidelines. While LLMs have shown impressive results in various generation tasks, its current capabilities to generate reliable evaluation guidelines is limited, with vulnerabilities over 50%. We also report the results of five prompts in Appendix B, indicating that structured instructions incorporating evaluation aspects exhibit the lowest vulnerability ratio. What is worth",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Annotation Result",
                "sec_num": "3.3"
            },
            {
                "text": "Thank you for participating in this opinion summarization quality evaluation task! Opinion summarization is the task of automatically generating summaries for a set of reviews about a specific target. In this task, we focus on movie reviews written by users from the Rotten Tomatoes website 1 . You will be presented with one human-written reference summary first along with three system summaries generated by trained neural networks respectively 2 . Please evaluate the quality of opinion summaries 3 with respect to the following four features: (1) Relevance; (2) Consistency; (3) Fluency; and (4) Coherence 4 . You should make comparisons for the summary evaluation and rank the four summaries in the order of the four evaluation aspects 5 , and the evaluation is conducted on the open-source annotation tool Doccano 6 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Guideline for Opinion Summarization Quality Evaluation",
                "sec_num": null
            },
            {
                "text": "\u2022 Thoroughly read the guideline and familiarize yourself with the task of opinion summarization quality evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "\u2022 Carefully read the source reviews as well as reference and system summaries to grasp the overall content.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "\u2022 Evaluate the overall quality of each summary based on the four designated aspects, assign a score to each dimension sentence by sentence and aggregate all the scores of each sentence to perform pairwise comparisons 7 . \u2022 If you encounter any difficulties or have questions during the annotation procedure, refer to the provided guidelines. Alternatively, feel free to contact us via email for further clarification.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "Vulnerabilities in Guideline 1. Ethical Issues: guiding in this manner disregards the personal privacy of the commenters as it fails to specify whether the comments are anonymous or obtained with user consent. An improved guideline should address ethical concerns such as \"All anonymized reviews have been previously collected with user consent and have been stripped of personally identifiable information.\" 2. Unconscious Bias: guiding in this manner specifies the sequence of the summaries, leading evaluators to have a biased perception of the reference as superior in quality. An improved guideline should be more neutral such as \"You will be presented with four summaries in a random order, including one reference summary and three system summaries generated by trained neural networks.\" 3. Ambiguous Definition: guiding in this manner fails to clarify whether the task is to evaluate four summaries based on the source review or to evaluate three system-generated summaries based on the reference. An improved guideline should provide a more explicit task definition such as \"Please evaluate the quality of both the reference and three system-generated opinion summaries given the corresponding source review.\" 4. Unclear Rating: guiding in this manner lacks a clear explanation of the evaluation aspect, which leads to multiple interpretations for different evaluators, resulting in inconsistent ratings. Given that this task involves pairwise comparison, an improved guideline doesn't have to provide a rating scale, yet it should explicit the evaluation criteria such as: \"(1) Relevance: measures how well the summary captures the key points of the source review;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "(2) Consistency: measures whether the facts in the summary are consistent with the facts in the source review;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "(3) Fluency: measures the quality of individual sentences, are they well-written and grammatically correct; (4) Coherence: measures the quality of all sentences collectively, to the fit together and sound naturally.\" 5. Edge Cases: guiding in this manner fails to provide directions for addressing edge cases where both summaries have equal quality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "An improved guideline should comprehensively consider exceptional situations such as: \"In the case of two summaries are of equal quality, place them side by side in the same ranking.\" 6. Prior Knowledge: guiding in this manner assumes evaluators have annotation experience without explaining how to use the professional tool Doccano. An improved guideline should offer training or detailed explanations for professional tools and principles such as \"The evaluation is conducted on the open-source annotation tool Doccan, and subsequently, training will be provided on how to use it for annotation. If you are interested, you can visit this website in advance for more information: https://doccano.github.io/doccano.\" 7. Inflexible Instructions: guiding in this manner makes the task unnecessarily complex by aggregating individual sentences for overall quality evaluation. Furthermore, it doesn't align with certain aspects, such as coherence, which require an evaluation that considers all sentences collectively. An improved guideline should be more flexible and reasonable such as \"Evaluate the overall quality of each summary and make comparisons based on the four designated aspects.\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "Table 1 : An illustration of the taxonomy on guideline vulnerability types.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "noting is that the quality of the authentic guidelines extracted from existing papers is much poorer, and the vulnerability ratio is 77.09%, which undermines the reliability of evaluation tasks. This aligns with the conclusions of Sabou et al. (2014) , who demonstrated that the crowdsourcing community still lacks a set of best-practice guidelines, resulting in low-quality annotations. We make a call for future researchers to be aware of the issue and emphasize the need for thorough refinement and investigation to develop a robust guideline.",
                "cite_spans": [
                    {
                        "start": 231,
                        "end": 250,
                        "text": "Sabou et al. (2014)",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "Regarding the vulnerability type, both the authentic and synthetic guidelines are in a similar distribution that Ambiguous Definition and Unclear Rating occur most frequently. The vulnerability \"Others\" appears in scenarios such as when guidelines generated by LLMs are incomplete. Table 2 shows the authentic guideline for the machine-inthe-loop writing of image caption task extracted from Padmakumar and He (2022) . The guideline lacks the definition of the machine-in-the-loop writing task and fails to specify the evaluation criterion, leaving uncertainty about the annotation process. As a result, the reliability and validity of the evaluation process will be compromised. Apart from the two types, authentic guidelines exhibit more vulnerabilities of bias, prior knowledge and ethical issues. Additionally, authentic guidelines are Instructions for crowdworkers evaluating the captions \u2022 Choose the better (more descriptive and/or figurative) caption for the image.",
                "cite_spans": [
                    {
                        "start": 392,
                        "end": 416,
                        "text": "Padmakumar and He (2022)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 288,
                        "end": 289,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "\u2022 A better caption is your subjective judgement, the rubrics to make the choice are that the caption is descriptive and/or figurative in its interpretation of the image (Refer the examples for further clarification).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "\u2022 The explanation asked is supposed to be very brief. A single word of if you like it for being descriptive or interpretive will do.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "\u2022 Relevance of the caption to the image is your subjective choice whether the caption appropriately represents what is in the image and is not just a catchy piece of text unrelated to the image.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "\u2022 A caption that you deem irrelevant should never be the better caption, unless both are irrelevant. more likely to suffer from neglecting edge cases, whereas the LLM is more prone to generate excessively rigid and complex guidelines, resulting in more vulnerabilities of Inflexible Instructions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "As such, resorting to the LLMs to fill in the gaps proves to be a promising approach. However, it is important to note that the current LLM can only generate preliminary drafts of guidelines and needs more effective strategies to enhance reasoning ability for improving the reliability of guidelines. A future direction is to enhance LLM's reasoning ability to improve its capability in generating reliable guidelines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IMPORTANT:",
                "sec_num": null
            },
            {
                "text": "In this section, we investigate utilizing LLMs to detect the specific vulnerability types in each eval-uation guideline, which is taken as a multi-label vulnerability type classification task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "We perform our experiments utilizing both opensource and closed-source LLMs. For open-source models, we fine-tuned LLaMA-7B, an efficient and popular foundation language model with LoRAfoot_3 . Additionally, we also experimented with Flan-T5-XXLfoot_4 , Flan-Alpaca-Lfoot_5 , and Falcon-7Bfoot_6 , respectively. For closed-source models, we select two widely accessible large language models: TEXT-DAVINCI-003foot_7 and GPT-3.5-turbofoot_8 . TEXT-DAVINCI-003 is developed using a combination of supervised instruction tuning and Reinforcement Learning from Human Feedback methodologies. GPT-3.5-Turbo is an enhanced version of the GPT-3 language model with instruction fine-tuning.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Large Language Models",
                "sec_num": "4.1"
            },
            {
                "text": "Our exploration involves designing prompts for both zero-shot and few-shot scenarios, encompassing four distinct prompt templates (\"Basic\", \"VDesc\", \"CoT-Basic\" and \"CoT-VDesc\") under each scenario, thus yielding a total of eight prompts. Basic prompt offers only the name of the vulnerability type, whereas VDesc prompt expands on this by including definition for each type. Additionally, we investigate the Chain-of-Thought (CoT) prompting technique on both prompt templates. Detailed prompting design and the full prompts are detailed in Appendix E.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompting Strategies",
                "sec_num": "4.2"
            },
            {
                "text": "We further implement and finetune three Transformer-based classifiers as baselines: BERT (Devlin et al., 2019) along with its successors XLNet (Yang et al., 2019) and ALBERT (Lan et al., 2019) , which have shown excellent performance on classification tasks. They are all deep pretrained models that first encodes a guideline into vector space by capturing contextual information bidirectionally and then outputs the probability for each label independently. We finetune all the models on the base version and the hyper-parameters can be found in Appendix F.",
                "cite_spans": [
                    {
                        "start": 89,
                        "end": 110,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 174,
                        "end": 192,
                        "text": "(Lan et al., 2019)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.3"
            },
            {
                "text": "Human Evaluation Guideline LLM LLM Answer Read the following pairs of texts (source and candidate) and provide ratings between 0 to 100 based on the degree of similarity in meaning and preservation of grammar for each candidate text. Drag or click on the appropriate portion of the slider to provide a rating for each candidate text. Please refer to the scoring example before starting the first task. Note: There will be a candidate text in each HIT which contains several random words appearing out of context unrelated to the text. Please make sure to score this candidate text in each HIT with 10-20 points. Apart from this, please refer to the ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "4.3"
            },
            {
                "text": "We initially divide the dataset into five parts, with four parts designated for training (80%) and one for testing (20%). The training set is used for supervised fine-tuning of pretrained baselines and is subsequently divided into train/validation sets in a 4:1 ratio. Further, each of these five parts is used as an individual testing set, while the remaining four parts serve as training sets. As such, we evaluate the performance of the baselines and LLMs across the entire dataset, treating each part as a test set in rotation, so as to mitigate random fluctuations due to the relatively small size of the dataset and obtain a more accurate performance estimate.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Splits",
                "sec_num": "4.4"
            },
            {
                "text": "Following Chen et al. ( 2017), we adopt macro-Precision(macro-P), macro-Recall (macro-R), and macro-F1 scores (Vechtomova, 2009) , which assess the overall performance of a classifier by taking the average of Precision, Recall, and F1-scores across all individual labels for each class (including \"None\"). Considering the unequal proportions of different vulnerability types in the dataset, as shown in Figure 1 , macro metrics can provide a more balanced view of the model's performance across all classes, as opposed to micro-averaging (Vechtomova, 2009) , which gives more weight to the larger classes. Furthermore, we follow Ganda and Buch (2018) and utilize Accuracy (ACC) to assess the average accuracy of all individual types. We also follow Wu and Zhou (2017) and use the instance-AUC (AUC) metric. Hamming Loss (Schapire and Singer, 1998) is also incorporated, which evaluates the fraction of misclassified instance-label pairs, accounting for both missed relevant labels and predicted irrelevant labels.",
                "cite_spans": [
                    {
                        "start": 110,
                        "end": 128,
                        "text": "(Vechtomova, 2009)",
                        "ref_id": "BIBREF53"
                    },
                    {
                        "start": 538,
                        "end": 556,
                        "text": "(Vechtomova, 2009)",
                        "ref_id": "BIBREF53"
                    },
                    {
                        "start": 629,
                        "end": 650,
                        "text": "Ganda and Buch (2018)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 749,
                        "end": 767,
                        "text": "Wu and Zhou (2017)",
                        "ref_id": "BIBREF55"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 410,
                        "end": 411,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "4.5"
            },
            {
                "text": "5 Results and Analysis",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "4.5"
            },
            {
                "text": "We first show the case study of a sample from the test set in Table 3 , in which the authentic guideline is drawn from Kim et al. (2021) and suffers from vulnerabilities of Ambiguous Definition and Unclear Rating. The answers are generated by LLMs under few-shot CoT prompting. We can find that TEXT-DAVINCI-003 not only generates completely correct answers, but also narrows down the scope of vulnerabilities in its reasoning, facilitating the correction of identified vulnerabilities in the guidelines. Nevertheless, GPT-3.5-Turbo appears to have misconstrued the definition of the Edge Cases, since the handling of cases \"where the candidate text contains random words unrelated to the text\" has already been provided. The four open-source models, on the other hand, don't gen-erate the answer as instructed. Instead, LLaMA extracts keywords directly as the output, Flan and Flan-Alpaca yields nonsensical results, and Falcon consistently outputs \"None\" for all of the data, revealing the inefficiency of open-source models in vulnerability detection. We speculate the reason is due to the limited training data and the excessive length of the instructions.",
                "cite_spans": [
                    {
                        "start": 119,
                        "end": 136,
                        "text": "Kim et al. (2021)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 68,
                        "end": 69,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Qualitative Analysis",
                "sec_num": "5.1"
            },
            {
                "text": "Given that the results generated by open-source LLMs are invalid, as demonstrated in Table 3 , quantitative evaluation becomes unfeasible. Therefore, we focus on GPT models and pre-trained baselines for quantitative analysis. Table 4 shows the experiment results for guideline vulnerability detection on both authentic and synthetic guidelines along with the entire dataset. We also report the results of each vulnerability type in Appendix G.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 91,
                        "end": 92,
                        "text": "3",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 232,
                        "end": 233,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Quantitative Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "We first explored the effects of different prompt strategies, including Basic, Vdesc, and the use of CoT. Subsequently, we explored the detection performance of LLMs in zero-shot and fewshot settings. Additionally, we investigated the performance of different LLMs, namely TEXT-DAVINCI-003, GPT-3.5-Turbo, and pre-trained models including BERT, XLNet, and ALBERT. Finally, we analyzed the varying performance between authentic guidelines and synthetic guidelines. Through this exploration of different prompt strategies, models, and settings, we conclude that TEXT-DAVINCI-003 demonstrates superior performance with few-shot prompting and CoT strategies. Our analysis of experimental results exploring different prompt strategies, models, and settings is based on a comprehensive consideration of all evaluation metrics. When drawing conclusions from specific metrics, we specify the particular metrics that serve as the basis for our conclusions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Quantitative Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "Regarding the Basic and VDesc prompt templates, they exhibit comparable capabilities. The reason is that the incorporation of vulnerability descriptions might potentially disrupt the reasoning process of LLMs, although they might provide detailed vulnerability descriptions for LLMs. According to results on All guidelines, we can also find that CoT generally improves model performance in all prompt strategies of zero-shot setting and Vdesc prompt strategy of few-shot setting. The reason why CoT doesn't consistently enhance model performance in the Basic prompt strategy may stem from the insufficiency of vulnerability information provided by the Basic prompt for effective reasoning. For few-shot and zero-shot settings, we can conclude from the results on All guidelines that LLMs generally exhibit enhanced performance in few-shot scenarios.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Quantitative Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "In the analysis of various LLMs and pretrained models, the experimental results indicate that TEXT-DAVINCI-003 and GPT-3.5-Turbo exhibit comparable performance, consistently outperforming pretrained models across the majority of prompt strategies. However, the pretrained models still serve as robust baselines, showing specific advantages over TEXT-DAVINCI-003 without CoT strategies under zero-shot scenarios. A noteworthy observation is that Recall values generally surpass Precision in most cases, indicating a tendency for the models to classify guidelines as positive, i.e., no vulnerability is detected. Furthermore, observing the results of each vulnerability type (detailed experimental results are shown in Appendix G), it is found that the model's ability to detect different vulnerabilities varies significantly. All these gaps suggest that the models still have room for improvement in guideline vulnerability detection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Quantitative Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "We also compare the model's performance on the two categories of guidelines: Authentic Guidelines and Synthetic Guidelines. Experimental results in Table 4 indicate that LLMs exhibit a stronger ability to detect vulnerabilities in synthetic guidelines compared to authentic guidelines. Moreover, TEXT-DAVINCI-003 with CoT-Vdesc strategy demonstrates superior detection capabilities in detecting authentic guidelines, while GPT-3.5-Turbo with CoT-Vdesc strategy exhibits enhanced detection proficiency for synthetic guidelines. Overall, the experimental results show TEXT-DAVINCI-003 exhibits superior detection capabilities in detecting all guidelines.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 154,
                        "end": 155,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Quantitative Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "Based on the outcomes of a thorough exploration involving various prompt strategies, models, and settings, our conclusion is that TEXT-DAVINCI-003 demonstrates superior performance with fewshot prompting and CoT strategies. Overall, TEXT-DAVINCI-003 with CoT-Vdesc prompt strategy in the few-shot scenario has the best performance for all guidelines and is recommended as the method for guideline vulnerability detection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Quantitative Analysis",
                "sec_num": "5.2"
            },
            {
                "text": "We summarize the key findings from this work and provide practical recommendations for reliable 4 : Guideline vulnerability detection results on \"Authentic Guidelines (Aut)\", \"Synthetic Guidelines (Syn)\" and the whole dataset (All). Upper, middle and lower parts show results of LLMs under zero-shot and few-shot scenarios as well as baseline models, respectively. The best values of each column are bolded. \u2193 indicates that the lower value indicates the better performance. human evaluation.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 96,
                        "end": 97,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Practical Recommendations",
                "sec_num": "6"
            },
            {
                "text": "LLMs. Our research has found that the proportion of vulnerabilities in guidelines generated by LLMs is lower than those written by humans. We suggest directly instructing LLMs about the requirements for evaluation and utilizing them to generate human evaluation guidelines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Writing human evaluation guidelines using",
                "sec_num": "1."
            },
            {
                "text": "Appendix H). We analyze human evaluation guidelines and summarize principles to compose a robust evaluation guidelines. We recommend referencing these principles when crafting the guidelines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modify the evaluation guideline draft written by LLMs based on the proposed principles for human evaluation guidelines (shown in",
                "sec_num": "2."
            },
            {
                "text": "3. Utilize TEXT-DAVINCI-003 with CoT-VDesc strategy to identify vulnerabilities. It has been proven to be an efficient, convenient, and cost-effective method, and detecting a guideline only requires approximately $0.02 11 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modify the evaluation guideline draft written by LLMs based on the proposed principles for human evaluation guidelines (shown in",
                "sec_num": "2."
            },
            {
                "text": "11 The prompt consists of 909 tokens, with the inclusion of 4. Conduct human evaluation in strict accordance with the human evaluation guidelines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modify the evaluation guideline draft written by LLMs based on the proposed principles for human evaluation guidelines (shown in",
                "sec_num": "2."
            },
            {
                "text": "5. Publicly release the human evaluation guideline. This can contribute to the transparency of human evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modify the evaluation guideline draft written by LLMs based on the proposed principles for human evaluation guidelines (shown in",
                "sec_num": "2."
            },
            {
                "text": "7 Related Work have been designed to explore the issue of vulnerability detection in human evaluation guidelines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modify the evaluation guideline draft written by LLMs based on the proposed principles for human evaluation guidelines (shown in",
                "sec_num": "2."
            },
            {
                "text": "Previous studies have frequently relied on automatic metrics like BLEU (Papineni et al., 2002) , METEOR (Banerjee and Lavie, 2005) , ROUGE (Lin, 2004) , BERT-SCORE (Zhang et al., 2019) , MOVER-SCORE (Zhao et al., 2019) and BART-SCORE (Yuan et al., 2021) to evaluate the quality of generated text, primarily due to their costeffectiveness, quickness, and repeatability (Reiter and Belz, 2009b) . Nevertheless, these metrics have been criticized for their limited interpretability (van der Lee et al., 2019) and low correlation with human judgements (Belz and Reiter, 2006; Liu et al., 2016; Reiter and Belz, 2009b; Novikova et al., 2017a) . Human evaluation is widely recognized as the gold standard for evaluating NLG systems (Mellish and Dale, 1998; Gkatzia and Mahamood, 2015a; van der Lee et al., 2018) . However, it has the potential to be unreliable due to cognitive biases (Schoch et al., 2020) and the lack of standardized evaluation methodologies (van der Lee et al., 2019) . Shimorina and Belz (2021) contributed to transparency in the human evaluation process by documenting it, while Belz et al. (2023) explored reproducibility in NLP human evaluation. Ruan et al. (2024) proposed CASF to solve the sampling problem in human evaluation. However, there is currently no comprehensive work addressing the reliability of human evaluation guidelines, a pivotal element ensuring reliable and reproducible human assessment. With the increasing interest in LLMs, recent studies have been conducted to examine their suitability for assessing generation tasks (Gao et al., 2024) , like summarization (Luo et al., 2023; Gao et al., 2023) , machine translation (Kocmi and Federmann, 2023) , etc. In this work, we focus on both human evaluation and large language model evaluation, which is the first to utilize LLMs for assessing guidelines in human evaluation.",
                "cite_spans": [
                    {
                        "start": 71,
                        "end": 94,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 104,
                        "end": 130,
                        "text": "(Banerjee and Lavie, 2005)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 139,
                        "end": 150,
                        "text": "(Lin, 2004)",
                        "ref_id": null
                    },
                    {
                        "start": 164,
                        "end": 184,
                        "text": "(Zhang et al., 2019)",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 199,
                        "end": 218,
                        "text": "(Zhao et al., 2019)",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 234,
                        "end": 253,
                        "text": "(Yuan et al., 2021)",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 368,
                        "end": 392,
                        "text": "(Reiter and Belz, 2009b)",
                        "ref_id": null
                    },
                    {
                        "start": 548,
                        "end": 571,
                        "text": "(Belz and Reiter, 2006;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 572,
                        "end": 589,
                        "text": "Liu et al., 2016;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 590,
                        "end": 613,
                        "text": "Reiter and Belz, 2009b;",
                        "ref_id": null
                    },
                    {
                        "start": 614,
                        "end": 637,
                        "text": "Novikova et al., 2017a)",
                        "ref_id": null
                    },
                    {
                        "start": 726,
                        "end": 750,
                        "text": "(Mellish and Dale, 1998;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 751,
                        "end": 779,
                        "text": "Gkatzia and Mahamood, 2015a;",
                        "ref_id": null
                    },
                    {
                        "start": 780,
                        "end": 805,
                        "text": "van der Lee et al., 2018)",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 879,
                        "end": 900,
                        "text": "(Schoch et al., 2020)",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 964,
                        "end": 981,
                        "text": "Lee et al., 2019)",
                        "ref_id": null
                    },
                    {
                        "start": 984,
                        "end": 1009,
                        "text": "Shimorina and Belz (2021)",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 1095,
                        "end": 1113,
                        "text": "Belz et al. (2023)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 1164,
                        "end": 1182,
                        "text": "Ruan et al. (2024)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 1561,
                        "end": 1579,
                        "text": "(Gao et al., 2024)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 1601,
                        "end": 1619,
                        "text": "(Luo et al., 2023;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 1620,
                        "end": 1637,
                        "text": "Gao et al., 2023)",
                        "ref_id": null
                    },
                    {
                        "start": 1660,
                        "end": 1687,
                        "text": "(Kocmi and Federmann, 2023)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Natural Language Generation Evaluation",
                "sec_num": "7.2"
            },
            {
                "text": "In this paper, we propose and analyze significant issues in the evaluation guidelines of gold-standard human assessments. We conduct a preliminary study on defining and detecting vulnerabilities in guidelines to advancing reliable human evaluation. By proposing a taxonomy of guideline vulnerabili-ties, we constructed the first annotated human evaluation guideline dataset. We then explored LLMs with Few-Shot prompting and CoT strategies for automatic vulnerability detection. Recommendations include employing LLMs to assist in writing human evaluation guidelines and modifying them based on the proposed principles. Utilizing the proposed LLM-based vulnerability detection method is suggested for assessing the reliability of the guidelines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "8"
            },
            {
                "text": "In future work, we will delve into the precise annotation of spans containing vulnerabilities in guidelines, providing correction suggestions, automatically correcting identified vulnerabilities in the guidelines, and generating reliable guidelines by AI models. This advancement aims to contribute towards the ultimate goal of establishing dependable gold-standard human evaluation guidelines, thereby enhancing the reliability of NLG assessments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "8"
            },
            {
                "text": "This study serves as a preliminary exploration towards establishing reliable evaluation guidelines. We proposed and analyzed significant issues in gold-standard human assessments, specifically focusing on identifying vulnerabilities in guidelines. Our preliminary study employed LLMs to detect guideline vulnerabilities and provided recommendations for improving reliability in human evaluation. However, the ultimate goal of achieving dependable gold-standard human evaluation guidelines requires further investigation. Future work can delve into precise annotation of spans containing vulnerabilities, automatic correction of identified issues, and the generation of reliable guidelines using AI models. These advancements aim to contribute to establishing dependable guidelines, thereby enhancing the reliability of NLG assessments. It is important to note that due to cost considerations, experiments with the proposed method were not conducted on GPT-4. Implementing the proposed method on GPT-4 may further enhance its effectiveness, a consideration for future research.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations",
                "sec_num": null
            },
            {
                "text": "We recruit annotators from a college campus. They are completely free to decide whether or not to participate in our annotation. The payment is 9 dollars per hour, higher than the local minimum wage. There is no personal information in our collected dataset. The information which may be used to identify the participants is deleted after the annotation. Moreover, the LLM-generated guidelines may contain toxic language, which can make annotators uncomfortable. We reviewed the data before annotation and found no problematic samples. We check the licenses of the artifacts used in this study and do not find conflicts. The license of the dataset we will release is CC BY-NC 4.0.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ethics Statement",
                "sec_num": null
            },
            {
                "text": "For the collected data, we focused on work related to human evaluation in NLG tasks. In descending order of frequency, specific tasks include summarization (42), dialogue generation(36), question answering (34), machine translation (26), story generation (20), image captioning (9), etc. These guidelines are collected from high-quality NLP conferences ACL, EMNLP and NAACL over the past three years (2020-2022). Apart from machine translation that cover a range of language pairs like English-French, English-Japanese, Chinese -English, English-German, English-Spanish, and English-Russian, most of the tasks primarily focus on the English language. Additionally, we have gathered information on the reported interannotator agreement, revealing a general inverse relationship between the number of identified vulnerabilities and the level of agreement. To illustrate, in the vulnerability-free guideline from Jiang et al. ( 2020), Cohen's Kappa can reach a substantial level of 0.807, whereas in the guideline from Roy et al. (2021) , with three identified vulnerabilities, Cohen's Kappa falls only within the range of 0.50 to 0.64. The list of crawled papers and the guidelines with annotations are released.",
                "cite_spans": [
                    {
                        "start": 1016,
                        "end": 1033,
                        "text": "Roy et al. (2021)",
                        "ref_id": "BIBREF40"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Authentic Guidelines Details",
                "sec_num": null
            },
            {
                "text": "To explore the LLM's ability in writing human evaluation guidelines and extend the guideline datasets, we utilize different prompt strategies for LLMs to generate diverse human evaluation guidelines. Table 6 displays the prompts that were employed for creating synthetic guidelines, which fall into two categories: raw instructions and structured instructions. Inspired by the sensitivity of language models to the framing of their instructional prompts (Mishra et al., 2022) , we explore the impact of incorporating evaluation aspects and constraints separately, with a total of five prompt variations. For each prompt, we analyze their performance across 12 NLG tasks: summarization, machine translation, dialogue generation, story generation, paraphrase generation, data to text, grammar error correction, text simplification, code generation, code summarization, question generation, and spelling correction, involve two assessment methods: direct assessment and pairwise comparison, and focus on the keywords \"guideline\" and \"instruction\". The annotation result of each prompt can be found in Table 5 . It can be seen that structured instructions, as opposed to raw instructions, generally contain fewer vulnerabilities and both can enhance generation performance after adding evaluation aspects. However, incorporating constraints into the prompt leads to a drop in generation performance, contradicting the findings of Shi et al. (2022) , who employ a fluency constraint and observed an enhancement in performance.",
                "cite_spans": [
                    {
                        "start": 454,
                        "end": 475,
                        "text": "(Mishra et al., 2022)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 1426,
                        "end": 1443,
                        "text": "Shi et al. (2022)",
                        "ref_id": "BIBREF47"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 206,
                        "end": 207,
                        "text": "6",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 1104,
                        "end": 1105,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "B Prompts for Synthetic Guideline Generation",
                "sec_num": null
            },
            {
                "text": "Ratio \u2193 raw 13.8 raw with aspect 10.5 structured 10.9 structured with aspect 9.6 structured with aspect and constraint 12.6",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompt % Vulnerability",
                "sec_num": null
            },
            {
                "text": "Table 5 : Annotation results regarding the vulnerability ratio of each prompt variation. The ratio calculation involves dividing the count of synthetic guidelines containing vulnerabilities for a specific prompt by the overall guideline count. \u2193 indicates a lower value is preferable.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Prompt % Vulnerability",
                "sec_num": null
            },
            {
                "text": "We release our full guideline provided to crowdworker participants for the manual evaluation of the vulnerability detection task in Table 7 . We advocate for more related works to share their guidelines, aiming to enhance the transparency of human evaluation and thereby contribute to the establishment of a set of best-practice guidelines for the community.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 138,
                        "end": 139,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "C Annotation Guideline",
                "sec_num": null
            },
            {
                "text": "The annotators we recruited are four college students with College English Test-6 certificates who are fluent in both English and Chinese languages, with Chinese as their mother tongue. There are 1 females and 3 males, with an average age of around 24. Then we conduct a training process. Specifically, we conducted an online meeting for annotator training, covering the interpretation of annotation guidelines, explanations and examples of various guideline vulnerabilities, clarification of relevant considerations, and a Q&A session. To confirm their proficiency, annotators underwent a pre-annotation test, and only those who passed were allowed to proceed with the formal annotation. Specifically, 10 guidelines are randomly sampled with 5 in Authentic Guidelines and 5 in Synthetic Guidelines respectively. We annotated them first. Then, we calculated the accuracy of each participant based on our annotation. Higher accuracy indicates a more consistent understanding of our guidelines. Annotators who achieve at least 80% accuracy are considered qualified to continue the annotation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Annotation Details",
                "sec_num": null
            },
            {
                "text": "We used Cohen's kappa (Cohen, 1960) to measure the inter-rater reliability. Considering that each label is independent and there are diverse label combinations for multi-label classification task, we do not require that two annotators provide completely identical label sets for each guideline. Instead, we assess the agreement between the two annotators in terms of each label they assign. Specifically, let n be the number of guidelines to be labeled by A and B two annotators. g is the number of distinct vulnerability labels, and f ij denotes the frequency of the number of subjects with the i th categorical response for annotator A and the j th categorical response for annotator B. The kappa agreement is then calculated as:",
                "cite_spans": [
                    {
                        "start": 22,
                        "end": 35,
                        "text": "(Cohen, 1960)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Annotation Details",
                "sec_num": null
            },
            {
                "text": "p 0 = 1 n g i=1 f ii , p e = 1 n 2 g i=1 f i+ f +i , \u03ba = p 0 -pe 1-pe ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Annotation Details",
                "sec_num": null
            },
            {
                "text": "where f i+ is the total for the i th row f +i and is the total for the i th column in the frequency table.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D Annotation Details",
                "sec_num": null
            },
            {
                "text": "The prompt templates used for vulnerability type detection on human evaluation guidelines are illustrated in Figure 2 Regarding the few-shot prompts, we expanded on the zero-shot method by incorporating seven pseudo-examples that encompass all vulnerability types except for \"Others\", some of which include multiple vulnerability types, so as to facilitate more appropriate model reasoning.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 116,
                        "end": 117,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "E Prompt Template for Vulnerability Detection",
                "sec_num": null
            },
            {
                "text": "Additionally, we explore the CoT prompting technique, which elicits complex multi-step reasoning through step-by-step answer examples. For zero-shot prompts, we incorporate CoT by simply incorporating the phrase \"Let's think step by step\" before each answer, without supplying any examples (Kojima et al., 2022) . It is worth noting that the phrase \"Let's think step by step\" is absent in the 7-shot prompt differing from zero-shot scenario. Instead, the phrase is integrated into the reasoning process of examples, through which we observed an improvement in performance. Inspired by Wang et al. (2022) , we integrate the results of each reasoning over three runs and select the most consistent answer as the final answer set.",
                "cite_spans": [
                    {
                        "start": 290,
                        "end": 311,
                        "text": "(Kojima et al., 2022)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 585,
                        "end": 603,
                        "text": "Wang et al. (2022)",
                        "ref_id": "BIBREF54"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E Prompt Template for Vulnerability Detection",
                "sec_num": null
            },
            {
                "text": "For TEXT-DAVINCI-003 12 and GPT-3.5-Turbo 13 , the specific hyper-parameters can be found in Table 8. For the baselines, we utilize Huggingface 14 implementations for all the deep pretrained models and the specific hyper-parameters can be found in Table 9 . We initially explored two approaches: one is to treat the multi-label classification task as a seq2seq problem, and generate a variable-length label sequence for a given text sequence. The other is to consider each neuron to be a binary classifier since the predictions for each category are independent in multi-label classification task, essentially forming a binary classification task for each label. We ultimately chose the second approach due to the limited training dataset in this task (less than 500 samples) and the increased data requirements of complex sequence models. We selected BCELoss as the loss function, which is commonly used in binary classification tasks. It calculates the individual losses for each label, quantifying the model's performance in terms of the difference between its predictions and the actual labels for each vul-12 https://platform.openai.com/docs/models/gpt-3-5 13 https://platform.openai.com/docs/models/gpt-3-5 14 https://huggingface.co/models nerability type. Besides, we utilized the sigmoid activation function in the fully connected layers.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 254,
                        "end": 255,
                        "text": "9",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "F Hyper-parameters",
                "sec_num": null
            },
            {
                "text": "Table 10 , 11 and 12 report the experimental results of each vulnerability type (including \"None\") of pre-trained baselines, TEXT-DAVINCI-003 as well as GPT-3.5-Turbo respectively. Please note that the overall accuracy and AUC in Table 4 are the averages across eight types of vulnerabilities, which may vary with the inclusion of \"None\". We can observe that the model's capacity to detect different vulnerability types exhibits some variation, showing a trend in both LLMs and the Baseline, where more frequently occurring vulnerability types yield lower results. For LLMs, this is reasonable as they tend to output \"None\", as mentioned above, making them prone to misidentify more guidelines containing high-frequency vulnerability types as positive. Nevertheless, for the Baseline, which has undergone supervised training beforehand, we speculate this could be attributed to the limited size of the dataset, resulting in the models not having acquired robust capabilities yet. Additionally, the accuracy of \"None\" reaches highest at 0.79 in TEXT-DAVINCI-003 CoT-VDesc, which can be considered as an indicator of the reliability of the human evaluation guideline.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 8,
                        "text": "10",
                        "ref_id": null
                    },
                    {
                        "start": 236,
                        "end": 237,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "G Results of Each Vulnerability Type",
                "sec_num": null
            },
            {
                "text": "A reliable human evaluation guideline is the beginning of reliable human evaluation. We provide the principal for composing a reliable human evaluation guideline in Table 13 . For writing a reliable human evaluation guideline, researchers should provide explicit task definitions for raters and avoid biased instruction and prior knowledge assumptions. Moreover, the instruction should comprehensively cover a broad range of scenarios including the edge cases. Researchers should provide clear rating scale and criteria and make the instruction simple and flexible. Additionally, the potential ethical issues should be identified and addressed. It is highly recommended to attach examples and design a good user interface. Last but not least, remind annotators to be careful while carrying out their tasks to get more accurate evaluation results.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 171,
                        "end": 173,
                        "text": "13",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "Principal for Human Evaluation Guideline (a) Explicit Task Definition: Provide a concise and precise task description along with the evaluation aspects to preclude misinterpretation or confusion. Use clear language and avoid jargon or technical terms that may not be commonly known to all evaluators.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "(b) Unbiased Instructions: Ensure that all instructions and statements are free from any unconscious bias. Avoid favoring or disadvantaging certain results or approaches. Use neutral language and present the task in a fair and objective manner.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "(c) Avoiding Prior Knowledge Assumptions: Provide sufficient explanations regarding the subject matter, tools, or principles involved. Avoid assuming that evaluators are equipped with specific background knowledge. A good guideline is to make the content easily comprehensible, even for non-expert annotators.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "(d) Comprehensive Coverage: Formulate instructions that cover a broad range of scenarios, incorporating edge cases and exceptional situations that may not fit neatly into predefined categories or criteria. Clearly specify how evaluators should handle such cases and provide guidance on making informed judgments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "(e) Clear Rating Scale and Criteria: Define a rating scale and provide a clear explanation of the evaluation criteria for each point on the scale. Ensure that evaluators comprehensively grasp the meaning and expectations associated with each rating level so as to promote consistency in ratings and minimize potential confusion.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "(f) Simplicity and Flexibility: Keep the instructions straightforward and easy to understand. Avoid unnecessary complexity or rigid requirements that may make it difficult for evaluators to follow or adapt to variations in data and task requirements. Provide room for reasonable judgment and adaptability within the evaluation process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "(g) Addressing Ethical Issues: Identify and address potential ethical issues, including guidelines and safeguards, to ensure ethical considerations are upheld throughout the evaluation process. Consider privacy concerns, cultural sensitivities, accessibility requirements, and the potential misuse of evaluation results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "(h) Attach Examples: It is highly recommended to list positive examples that contain the input presented to the worker or system, and the anticipated results, thus providing crowdworkers with a clearer comprehension of the task. Additionally, listing negative examples can effectively emphasize THINGS TO AVOID by supplying that should not be generated.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "(i) Design a good user interface: A good user interface provides a positive experience for annotators and the design needs to be tailored to the specific needs of the users. For non-expert crowdsourcing, acquisition interfaces can be developed to facilitate the execution of crowdsourcing tasks. While for those running the crowdsourcing project, management interfaces are required to monitor progress, assess quality, and manage annotators.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "(j) Emphasize precautions: In concluding the guidelines, remind annotators to be careful while carrying out their tasks and outline the annotation requirements, feedback mechanism, and quality assurance processes explicitly so that annotators can manage their time effectively and provide more accurate evaluations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "Adherence to these principles facilitates the creation of a human evaluation guideline that is clear, fair, inclusive, and capable of producing reliable and meaningful results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "Table 13 : Principal for reliable human evaluation guideline.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 8,
                        "text": "13",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "H Principal for Reliable Human Evaluation Guideline",
                "sec_num": null
            },
            {
                "text": "In this paper, \"vulnerability\" carries the same meaning as \"defect\", indicating issues within evaluation guidelines that could potentially result in unreliable evaluation outcomes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We crawled https://paperswithcode.com, an open resource website which ensures our access to the guideline data once they are publicly available.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "A piece of synthetic guideline that doesn't belong to the evaluation task has been filtered out.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://github.com/Lightning-AI/lit-llama",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://huggingface.co/google/flan-t5-xxl",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://huggingface.co/declare-lab/flan-alpaca-large",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://huggingface.co/tiiuae/falcon-7b",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://platform.openai.com/docs/models/gpt-3-5",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://platform.openai.com/docs/models/gpt-3-5",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This work was supported by Beijing Science and Technology Program (Z231100007423011), National Key R&D Program of China (2021YFF0901502), National Science Foundation of China (No. 62161160339) and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). We thank Professor Ehud Reiter for providing constructive suggestions. We appreciate the anonymous reviewers for their helpful comments. Xiaojun Wan is the corresponding author.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            },
            {
                "text": "Thank you for participating in this task! We are currently working on a project focused on crafting robust and reliable guidelines for human evaluation. You will be randomly presented with a human evaluation guideline extracted from existing papers or generated by Large Language Models (LLMs). Your job is to review the provided guidelines and identify potential vulnerabilities within the text. These vulnerabilities should fall into one or more of the eight categories outlined below.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Vulnerability Detection in Human Evaluation Guidelines Task Overview",
                "sec_num": null
            },
            {
                "text": "Ethical Issues: instructions do not consider potential ethical implications related to the evaluation process, like privacy, cultural sensitivity, accessibility, or the potential misuse of the evaluation results.\u2022 Ethical Issues: Evaluate the comments on this public social media post for sentiment analysis.\u2022 Improved: Evaluate anonymized comments provided for sentiment analysis. All comments have been previously collected with user consent and have been stripped of personally identifiable information.Unconscious Bias: instructions unconsciously favors or disadvantages certain results.\u2022 Unconscious Bias: Evaluate the two systems A and B: How many points do you think system A is higher than system B?\u2022 Improved: Evaluate the two systems A and B based on user satisfaction and score them respectively.Ambiguous Definition: instructions for task definition are unclear, vague, or imprecise that can be interpreted in multiple ways.\u2022 Ambiguous Definition:Factual consistency of summarization is defined as the accuracy and faithfulness of a summary in representing the source.\u2022 Improved: Factual consistency of summarization is defined as the accuracy and faithfulness of a summary in representing the source. The source here usually has scenarios: the first is the input document and the second is common sense. In our task, we only focus on the first situation, i.e. evaluate the summary as factually inconsistent if it contains extra information of the input document,even though it is true facts.Unclear Rating: instructions that lack standardized criteria for evaluating aspects or definition of each point on a rating scale, resulting in potential inconsistency in ratings.\u2022 Unclear Rating: Rate the quality of the website.\u2022 Improved: Rate the quality of the website based on its design, ease of navigation, and relevance of content on a scale of 1 to 5, where 1 is 'very poor' and 5 is 'excellent'. If the website was generally good but had one major flaw, consider rating it a 3or 4 depending on the severity of the flaw. If the website was poor but had one saving grace, consider rating it a 2 or 3.Edge Cases: instructions do not specify how to handle edge cases or exceptional situations that don't neatly fit into the usual categories or criteria.\u2022 Edge Cases: Evaluate the factuality error types in the summary, including: Hallucination Error, Entity Error, Particulars Error, Predicate Error, Coreference Error.\u2022 Improved: Evaluate the factuality error types in the summary, including: Hallucination Error, Entity Error, Particulars Error, Predicate Error, Coreference Error. If the summary contains multiple errors, please list them all. If the error does not correspond to any of the above types, evaluate it as \"Others\".Prior knowledge: instructions assume that evaluators have certain background knowledge or familiarity with a specific subject matter, tool, or principle.\u2022 Prior knowledge: Evaluate the use of object-oriented programming (OOP) principles in the code.\u2022 Improved: Evaluate the use of object-oriented programming (OOP) principles in the code. Check for the following aspects. If you are unfamiliar with these principles, please refer to https://baldur.gitbook.io/patters-and-best-practices/solid/oop-principles for more information.\"-Encapsulation:Object properties are hidden, and object properties need to be modified through object methods.-Inheritance:Subclasses can inherit the properties and methods of the parent class without redefining them.-Polymorphism: Polymorphism can be divided into static and dynamic. Static means that the same object can have different forms of expression, while dynamic means that a parent type can point to an instance of its subtype, making the subtype respond differently to the same method.-Abstraction:Abstraction refers to extracting the common attributes and behaviors of a class and storing them in a class, regardless of how the specific behaviors a rerealized.Inflexible Instructions: instructions are unnecessarily complex or rigid, making it hard for evaluators to follow and incapable of adjusting to variations in data or task requirements.\u2022 Inflexible Instructions:Evaluate the website's user interface design on a scale of 1 to 10, considering color aesthetics, balance between text and imagery, navigability, font choices,button placements, menu design, adherence to modern design principles, web page loading speed, and responsive design.\u2022 Improved: Evaluate the website's user interface design on a scale of 1 to 10 from the perspectives of aesthetics, navigation and functionality.Others: covers any vulnerabilities that do not fall into the above categories.(Continued from previous page)Annotation Procedure\u2022 Comprehension: Carefully read through the entire human evaluation guideline in the center of the interface to get a full understanding of the content.\u2022 Labeling: Identify and click all potential vulnerabilities within the guideline according to the eight defined categories: Definition Ambiguity, Bias, Assuming Prior Knowledge, Insufficient Coverage, Lack of Rating Scale, Lack of adaptability, Neglecting Ethical Implications and Others.\u2022 Review and Submit: Repeat this process until the entire guideline has been thoroughly reviewed and all potential vulnerabilities have been identified. Press Enter to save and submit the annotation result.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Defect Types",
                "sec_num": null
            },
            {
                "text": "\u2022 Edge Cases: Please note that a single guideline may contain more than one type of defect. In such scenarios,ensure to label all the appropriate defect types. If the defect does not fit any of the seven specific categories, classify it as \"Others\" and provide a brief explanation.\u2022 Daily Annotation Requirement: The guideline for this task will be provided in batches. You are required to annotate a setof 30 items each day. Please submit the daily annotation results before 24:00(midnight) of that day.\u2022 Quality Assurance: Each day, we will conduct a random inspection of the annotated data. If the accuracy rate falls below 80%, you will be required to re-annotate the data for that day. Please maintain high quality in your annotations.\u2022 Support and Reference: If you encounter any confusion regarding professional knowledge or context while performing this task, please feel free to reach out to us for clarification.You may also refer to Wikipedia or other reliable sources to gain further understanding.\u2022 Feedback Mechanism: Wehave set up a discussion board on the interface, where you can directly submit your queries, concerns, or suggestions through the button \"click to comment on document\". This collaborative environment will allow for shared learning and problem-solving.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Emphasis and Caution",
                "sec_num": null
            },
            {
                "text": "Guideline: Ensure you partake in a comprehensive interrogation of the quantifiable parameters that govern the efficacy of the experimental intervention under scrutiny, taking into account the numerous facets and intricate variables that contribute to the overall outcome, keeping in mind the statistical significance thresholds and the corresponding probability distributions. Your final judgement should be a synthesis of these insights, crystallized into a ranking that encapsulates the overall potency of the intervention in question.Label: Assuming Prior Knowledge, Lack of adaptability, Lack of Rating Scale Explanation:Assuming Prior Knowledge: The guideline contains terms and concepts such as\"quantifiable parameters,\" \"efficacy of the experimental intervention,\" \"statistical significance thresholds,\" and\"probability distributions\". These terms assume the annotators process a prior knowledge in statistics or experimental design, which might make them struggle to understand and apply these instructions correctly without specific training or explanation of the professional background.Lack of adaptability: The guideline requires the annotators to conduct a\"comprehensive interrogation of the quantifiable parameters\",considering \"numerous facets and intricate variables\", and also take into account \"statistical significance thresholds\" and\"probability distributions\". It's quite strict and complex, leaving little room for adaptability depending on the experimental intervention being evaluated. The rigidness of these instructions can make it challenging for evaluators to apply them across a variety of scenarios or different types of interventions.Lack of Rating Scale: The guideline suggests that the final judgement should be a\"synthesis of these insights, crystallized into a ranking\". However,it doesn't provide any clear definition or structure for this ranking system.Without knowing how many points are on the scale or what each point represents,annotators might interpret the ranking system differently, leading to inconsistency in evaluations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "POSITIVE EXAMPLES",
                "sec_num": null
            },
            {
                "text": "Guideline: This task aims to evaluate the machine translation quality of two different models. You will be given one article and two corresponding translations from the two models in a random order. Your task is to evaluate the quality of the two translation and determine which one you prefer.Label: Definition Ambiguity, Assuming Prior Knowledge Explanation:The guideline does not contain \"Definition Ambiguity\" error since it has clearly stated the objective of the task. There's no ambiguity about what the annotators are supposed to do, which leaves no room for misunderstanding about how the task is to be carried out. However, it falls under \"Lack of Rating Scale\" defect because the guideline doesn't provide a specific rating scale or evaluation criteria that the annotators can use to objectively assess the translations. For instance, the guideline could instruct them to rate each translation on a scale of 1-5 for various aspects such as accuracy, fluency, and grammatical correctness, with clear descriptions of what each point on the scale signifies. Besides, the guideline does not has the defect of \"Assuming Prior Knowledge\". Evaluating the quality of a translation does not inherently require annotators has specialized knowledge about machine translation models, algorithms, or technical jargon. The expectation is that annotators can read and understand both the source and target languages, and are thus capable of judging the quality of the translation.Table 7 : Full instructions given to annotators of the vulnerability detection task in human evaluation guidelines.[Requirement] = Identify whether the evaluation guideline contains any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\".[Description] = The description of the vulnerabilities is as follows: Ambiguous Definition: instructions for task definition are unclear, vague, or imprecise that can be interpreted in multiple ways. Unconscious Bias: instructions unconsciously favors or disadvantages certain results. Edge Cases: instructions do not specify how to handle edge cases or exceptional situations that don't neatly fit into the usual categories or criteria. Unclear Rating: instructions that lack standardized criteria for evaluating aspects or definition of each point on a rating scale, resulting in potential inconsistency in ratings. Prior Knowledge: instructions assume that evaluators have certain background knowledge or familiarity with a specific subject matter, tool, or principle. Inflexible Instructions: instructions are unnecessarily complex or rigid, making it hard for evaluators to follow and incapable of adjusting to variations in data or task requirements. Ethical Issues: instructions do not consider potential ethical implications related to the evaluation process, like privacy, cultural sensitivity, accessibility, or the potential misuse of the evaluation results. Others: covers any defects that do not fall into the above categories.[Constraints] = Only reply with the name of vulnerabilities or \"None\". Note that a guideline may contain more than one vulnerability.[Guideline] = Human Evaluation Guideline: {guideline}[CoT] = Let's think step by step.[Examples] = shot_1 = GUIDELINE: Factual consistency of summarization is defined as the accuracy and faithfulness of a summary in representing the source. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". ANSWER: Ambiguous Definition shot_2 = GUIDELINE: You will be presented with one human-written reference summary first as well as three system summaries generated by trained neural networks respectively. Please evaluate the quality of opinion summaries with respect to the following 4 features: (1) Informativeness; (2) Faithfulness; (3) Consistence; and (4) Grammaticality. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". ANSWER: Ambiguous Definition, Unconscious Bias, Unclear Rating shot_3 = GUIDELINE: You should make pairwise comparison for the summary evaluation to select the best and worst among them. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". ANSWER: Edge Cases shot_4 = GUIDELINE: Evaluate the website's user interface design on a scale of 1 to 10, considering color aesthetics, balance between text and imagery, navigability, font choices, button placements, menu design, adherence to modern design principles, web page loading speed, and responsive design. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". ANSWER: Inflexible Instructions shot_5 = GUIDELINE: Evaluate the use of object-oriented programming (OOP) principles in the code. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". ANSWER: Prior Knowledge shot_6 = GUIDELINE: Evaluate the comments on this public social media post for sentiment analysis. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". ANSWER: Ethical Issues shot_7 = GUIDELINE: Rate the quality of the website. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". ANSWER: Unclear Rating[Examples_CoT] = shot_1 = GUIDELINE: Factual consistency of summarization is defined as the accuracy and faithfulness of a summary in representing the source. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". Let's think step by step: REASONING: The \"source\" here usually has scenarios: the input document or the common sense. The task definition of the guideline is unclear and imprecise that can be interpreted in multiple ways. So the answer is \"Ambiguous Definition\" ANSWER: Ambiguous Definition shot_2 = GUIDELINE: You will be presented with one human-written reference summary first as well as three system summaries generated by trained neural networks respectively. Please evaluate the quality of opinion summaries with respect to the following 4 features: (1) Informativeness; (2) Faithfulness; (3) Consistence; and (4) Grammaticality. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". Let's think step by step: REASONING: This guideline does not clarify whether the task definition is to evaluate four summaries based on the source review or to evaluate three system summaries based on the reference, causing \"Ambiguous Definition\". It does not present the reference and generated summaries in a random order, causing participants to Unconscious Bias towards perceiving the reference as higher quality, result in \"Unconscious Bias\". It does not provide a detailed explanation of 4 rating aspects, leading to multiple interpretations for different evaluators, causing \"Unclear rating\". So the answer is \"Ambiguous Definition\", \"Unconscious Bias\", \"Unclear Rating\". ANSWER: Ambiguous Definition, Unconscious Bias, Unclear Rating (Continued from previous page) shot_3 = GUIDELINE: Y ou should make pairwise comparison for the summary evaluation to select the best and worst among them. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". Let's think step by step: REASONING: The guideline does not provide guidance on how to handle the edge cases when two summaries have the same quality. So the answer is \"Edge Cases\" ANSWER: Edge Cases shot_4 = GUIDELINE: Evaluate the website's user interface design on a scale of 1 to 10, considering color aesthetics, balance between text and imagery, navigability, font choices, button placements, menu design, adherence to modern design principles, web page loading speed, and responsive design. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". Let's think step by step: REASONING: The guideline is unnecessarily complex and rigid, making it hard for evaluators to follow and incapable of adjusting to variations in data or task requirements. So the answer is \"Inflexible Instructions\" ANSWER: Inflexible Instructions shot_5 = GUIDELINE: Evaluate the use of object-oriented programming (OOP) principles in the code. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". Let's think step by step: REASONING: The guideline doesn't provide detail explanation of OOP principles which assumes that evaluators have certain background knowledge or familiarity with a specific subject matter, tool, or principle. So the answer is \"Prior Knowledge\" ANSWER: Prior Knowledge shot_6 = GUIDELINE: Evaluate the comments on this public social media post for sentiment analysis. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". Let's think step by step: REASONING: The guideline does not specify the collected comments are anonymous or with user consent, disregarding the personal privacy of the commenters. So the answer is \"Ethical Issues\" ANSWER: Ethical Issues shot_7 = GUIDELINE: Rate the quality of the website. REQUIREMENT: Identify the guideline contain any of the following vulnerabilities: \"Ambiguous Definition\", \"Unconscious Bias\", \"Prior Knowledge\", \"Edge Cases\", \"Unclear Rating\", \"Inflexible Instructions\", \"Ethical Issues\", \"Others\". Let's think step by step: REASONING: The guideline does not provide standardized criteria or definition for evaluation aspects or each point on a rating scale, which can lead to inconsistency in ratings. So the answer is \"Unclear Rating\" ANSWER: Unclear Rating (2) Few-shot Prompt ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 1482,
                        "end": 1483,
                        "text": "7",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "NEGTIVE EXAMPLES",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Qameleon: Multilingual qa with only 5 examples",
                "authors": [
                    {
                        "first": "Priyanka",
                        "middle": [],
                        "last": "Agrawal",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Alberti",
                        "suffix": ""
                    },
                    {
                        "first": "Fantine",
                        "middle": [],
                        "last": "Huot",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Maynez",
                        "suffix": ""
                    },
                    {
                        "first": "Ji",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Ruder",
                        "suffix": ""
                    },
                    {
                        "first": "Kuzman",
                        "middle": [],
                        "last": "Ganchev",
                        "suffix": ""
                    },
                    {
                        "first": "Dipanjan",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2211.08264"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Priyanka Agrawal, Chris Alberti, Fantine Huot, Joshua Maynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev, Dipanjan Das, and Mirella Lapata. 2022. Qameleon: Multilingual qa with only 5 examples. arXiv preprint arXiv:2211.08264.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "The use of rating and Likert scales in natural language generation human evaluation tasks: A review and some recommendations",
                "authors": [
                    {
                        "first": "Jacopo",
                        "middle": [],
                        "last": "Amidei",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Piwek",
                        "suffix": ""
                    },
                    {
                        "first": "Alistair",
                        "middle": [],
                        "last": "Willis",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 12th International Conference on Natural Language Generation",
                "volume": "",
                "issue": "",
                "pages": "397--402",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W19-8648"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacopo Amidei, Paul Piwek, and Alistair Willis. 2019. The use of rating and Likert scales in natural lan- guage generation human evaluation tasks: A review and some recommendations. In Proceedings of the 12th International Conference on Natural Language Generation, pages 397-402, Tokyo, Japan. Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
                "authors": [
                    {
                        "first": "Satanjeev",
                        "middle": [],
                        "last": "Banerjee",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
                "volume": "",
                "issue": "",
                "pages": "65--72",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with im- proved correlation with human judgments. In Pro- ceedings of the ACL Workshop on Intrinsic and Ex- trinsic Evaluation Measures for Machine Transla- tion and/or Summarization, pages 65-72, Ann Arbor, Michigan. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Comparing automatic and human evaluation of NLG systems",
                "authors": [
                    {
                        "first": "Anja",
                        "middle": [],
                        "last": "Belz",
                        "suffix": ""
                    },
                    {
                        "first": "Ehud",
                        "middle": [],
                        "last": "Reiter",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "11th Conference of the European Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "313--320",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anja Belz and Ehud Reiter. 2006. Comparing auto- matic and human evaluation of NLG systems. In 11th Conference of the European Chapter of the As- sociation for Computational Linguistics, pages 313- 320, Trento, Italy. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Missing information, unresponsive authors, experimental flaws: The impossibility of assessing the reproducibility of previous human evaluations in nlp",
                "authors": [
                    {
                        "first": "Anya",
                        "middle": [],
                        "last": "Belz",
                        "suffix": ""
                    },
                    {
                        "first": "Craig",
                        "middle": [],
                        "last": "Thomson",
                        "suffix": ""
                    },
                    {
                        "first": "Ehud",
                        "middle": [],
                        "last": "Reiter",
                        "suffix": ""
                    },
                    {
                        "first": "Gavin",
                        "middle": [],
                        "last": "Abercrombie",
                        "suffix": ""
                    },
                    {
                        "first": "Jose",
                        "middle": [
                            "M"
                        ],
                        "last": "Alonso-Moral",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Arvan",
                        "suffix": ""
                    },
                    {
                        "first": "Jackie",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Cieliebak",
                        "suffix": ""
                    },
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Kees",
                        "middle": [],
                        "last": "Van Deemter",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2305.01633"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Anya Belz, Craig Thomson, Ehud Reiter, Gavin Aber- crombie, Jose M Alonso-Moral, Mohammad Arvan, Jackie Cheung, Mark Cieliebak, Elizabeth Clark, Kees van Deemter, et al. 2023. Missing informa- tion, unresponsive authors, experimental flaws: The impossibility of assessing the reproducibility of pre- vious human evaluations in nlp. arXiv preprint arXiv:2305.01633.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Yoad Lewenberg, Roee Aharoni, and Enav Weinreb. 2023. q2d: Turning questions into dialogs to teach models how to search",
                "authors": [
                    {
                        "first": "Yonatan",
                        "middle": [],
                        "last": "Bitton",
                        "suffix": ""
                    },
                    {
                        "first": "Shlomi",
                        "middle": [],
                        "last": "Cohen-Ganor",
                        "suffix": ""
                    },
                    {
                        "first": "Ido",
                        "middle": [],
                        "last": "Hakimi",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2304.14318"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yonatan Bitton, Shlomi Cohen-Ganor, Ido Hakimi, Yoad Lewenberg, Roee Aharoni, and Enav Wein- reb. 2023. q2d: Turning questions into dialogs to teach models how to search. arXiv preprint arXiv:2304.14318.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Evaluation of text generation: A survey",
                "authors": [
                    {
                        "first": "Asli",
                        "middle": [],
                        "last": "Celikyilmaz",
                        "suffix": ""
                    },
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2006.14799"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: A survey. arXiv preprint arXiv:2006.14799.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Ensemble application of convolutional and recurrent neural networks for multi-label text categorization",
                "authors": [
                    {
                        "first": "Guibin",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Deheng",
                        "middle": [],
                        "last": "Ye",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenchang",
                        "middle": [],
                        "last": "Xing",
                        "suffix": ""
                    },
                    {
                        "first": "Jieshan",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Erik",
                        "middle": [],
                        "last": "Cambria",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "2017 International joint conference on neural networks (IJCNN)",
                "volume": "",
                "issue": "",
                "pages": "2377--2383",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guibin Chen, Deheng Ye, Zhenchang Xing, Jieshan Chen, and Erik Cambria. 2017. Ensemble application of convolutional and recurrent neural networks for multi-label text categorization. In 2017 International joint conference on neural networks (IJCNN), pages 2377-2383. IEEE.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement",
                "authors": [
                    {
                        "first": "Anton",
                        "middle": [],
                        "last": "Cheshkov",
                        "suffix": ""
                    },
                    {
                        "first": "Pavel",
                        "middle": [],
                        "last": "Zadorozhny",
                        "suffix": ""
                    },
                    {
                        "first": "Rodion",
                        "middle": [],
                        "last": "Levichev",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "20",
                "issue": "",
                "pages": "37--46",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2304.07232"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Anton Cheshkov, Pavel Zadorozhny, and Rodion Levichev. 2023. Evaluation of chatgpt model for vulnerability detection. arXiv preprint arXiv:2304.07232. Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological mea- surement, 20(1):37-46.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1423"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "A survey on multi label classification",
                "authors": [
                    {
                        "first": "Dhatri",
                        "middle": [],
                        "last": "Ganda",
                        "suffix": ""
                    },
                    {
                        "first": "Rachana",
                        "middle": [],
                        "last": "Buch",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Recent Trends in Programming Languages",
                "volume": "5",
                "issue": "1",
                "pages": "19--23",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dhatri Ganda and Rachana Buch. 2018. A survey on multi label classification. Recent Trends in Program- ming Languages, 5(1):19-23.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Llm-based nlg evaluation: Current status and challenges",
                "authors": [
                    {
                        "first": "Mingqi",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Xinyu",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Ruan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Pu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2402.01383"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun Wan. 2024. Llm-based nlg evaluation: Current status and challenges. arXiv preprint arXiv:2402.01383.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Shiping Yang, and Xiaojun Wan. 2023. Human-like summarization evaluation with chatgpt",
                "authors": [
                    {
                        "first": "Mingqi",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Ruan",
                        "suffix": ""
                    },
                    {
                        "first": "Renliang",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Xunjian",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2304.02554"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Ship- ing Yang, and Xiaojun Wan. 2023. Human-like sum- marization evaluation with chatgpt. arXiv preprint arXiv:2304.02554.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Survey of the state of the art in natural language generation: Core tasks, applications and evaluation",
                "authors": [
                    {
                        "first": "Albert",
                        "middle": [],
                        "last": "Gatt",
                        "suffix": ""
                    },
                    {
                        "first": "Emiel",
                        "middle": [],
                        "last": "Krahmer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Journal of Artificial Intelligence Research",
                "volume": "61",
                "issue": "",
                "pages": "65--170",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Journal of Artifi- cial Intelligence Research, 61:65-170.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "A snapshot of NLG evaluation practices 2005 -2014",
                "authors": [
                    {
                        "first": "Dimitra",
                        "middle": [],
                        "last": "Gkatzia",
                        "suffix": ""
                    },
                    {
                        "first": "Saad",
                        "middle": [],
                        "last": "Mahamood",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 15th European Workshop on Natural Language Generation (ENLG)",
                "volume": "",
                "issue": "",
                "pages": "57--60",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W15-4708"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dimitra Gkatzia and Saad Mahamood. 2015a. A snap- shot of NLG evaluation practices 2005 -2014. In Proceedings of the 15th European Workshop on Nat- ural Language Generation (ENLG), pages 57-60, Brighton, UK. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "A snapshot of nlg evaluation practices 2005-2014",
                "authors": [
                    {
                        "first": "Dimitra",
                        "middle": [],
                        "last": "Gkatzia",
                        "suffix": ""
                    },
                    {
                        "first": "Saad",
                        "middle": [],
                        "last": "Mahamood",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 15th European Workshop on Natural Language Generation (ENLG)",
                "volume": "",
                "issue": "",
                "pages": "57--60",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dimitra Gkatzia and Saad Mahamood. 2015b. A snap- shot of nlg evaluation practices 2005-2014. In Pro- ceedings of the 15th European Workshop on Natural Language Generation (ENLG), pages 57-60.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Twenty years of confusion in human evaluation: Nlg needs evaluation sheets and standardised definitions",
                "authors": [
                    {
                        "first": "Anja",
                        "middle": [],
                        "last": "David M Howcroft",
                        "suffix": ""
                    },
                    {
                        "first": "Miruna-Adriana",
                        "middle": [],
                        "last": "Belz",
                        "suffix": ""
                    },
                    {
                        "first": "Dimitra",
                        "middle": [],
                        "last": "Clinciu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gkatzia",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Sadid",
                        "suffix": ""
                    },
                    {
                        "first": "Saad",
                        "middle": [],
                        "last": "Hasan",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Mahamood",
                        "suffix": ""
                    },
                    {
                        "first": "Emiel",
                        "middle": [],
                        "last": "Mille",
                        "suffix": ""
                    },
                    {
                        "first": "Sashank",
                        "middle": [],
                        "last": "Van Miltenburg",
                        "suffix": ""
                    },
                    {
                        "first": "Verena",
                        "middle": [],
                        "last": "Santhanam",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Rieser",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 13th International Conference on Natural Language Generation",
                "volume": "",
                "issue": "",
                "pages": "169--182",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David M Howcroft, Anja Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A Hasan, Saad Mahamood, Simon Mille, Emiel Van Miltenburg, Sashank San- thanam, and Verena Rieser. 2020. Twenty years of confusion in human evaluation: Nlg needs evaluation sheets and standardised definitions. In Proceedings of the 13th International Conference on Natural Lan- guage Generation, pages 169-182.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Neural CRF model for sentence alignment in text simplification",
                "authors": [
                    {
                        "first": "Chao",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Mounica",
                        "middle": [],
                        "last": "Maddela",
                        "suffix": ""
                    },
                    {
                        "first": "Wuwei",
                        "middle": [],
                        "last": "Lan",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "7943--7960",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.709"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. 2020. Neural CRF model for sentence alignment in text simplification. In Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7943-7960, On- line. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "An analysis of ambiguity in word sense annotations",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Jurgens",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)",
                "volume": "",
                "issue": "",
                "pages": "3006--3012",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Jurgens. 2014. An analysis of ambiguity in word sense annotations. In Proceedings of the Ninth In- ternational Conference on Language Resources and Evaluation (LREC'14), pages 3006-3012, Reykjavik, Iceland. European Language Resources Association (ELRA).",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "BiSECT: Learning to split and rephrase sentences with bitexts",
                "authors": [
                    {
                        "first": "Joongwon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Mounica",
                        "middle": [],
                        "last": "Maddela",
                        "suffix": ""
                    },
                    {
                        "first": "Reno",
                        "middle": [],
                        "last": "Kriz",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "6193--6209",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.emnlp-main.500"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Joongwon Kim, Mounica Maddela, Reno Kriz, Wei Xu, and Chris Callison-Burch. 2021. BiSECT: Learning to split and rephrase sentences with bitexts. In Pro- ceedings of the 2021 Conference on Empirical Meth- ods in Natural Language Processing, pages 6193- 6209, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Large language models are state-of-the-art evaluators of translation quality",
                "authors": [
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Kocmi",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Federmann",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Large language models are zero-shot reasoners",
                "authors": [
                    {
                        "first": "Takeshi",
                        "middle": [],
                        "last": "Kojima",
                        "suffix": ""
                    },
                    {
                        "first": "Shane",
                        "middle": [],
                        "last": "Shixiang",
                        "suffix": ""
                    },
                    {
                        "first": "Machel",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Yutaka",
                        "middle": [],
                        "last": "Reid",
                        "suffix": ""
                    },
                    {
                        "first": "Yusuke",
                        "middle": [],
                        "last": "Matsuo",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Iwasawa",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Advances in neural information processing systems",
                "volume": "35",
                "issue": "",
                "pages": "22199--22213",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2022. Large lan- guage models are zero-shot reasoners. Advances in neural information processing systems, 35:22199- 22213.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Albert: A lite bert for self-supervised learning of language representations",
                "authors": [
                    {
                        "first": "Zhenzhong",
                        "middle": [],
                        "last": "Lan",
                        "suffix": ""
                    },
                    {
                        "first": "Mingda",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Goodman",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Gimpel",
                        "suffix": ""
                    },
                    {
                        "first": "Piyush",
                        "middle": [],
                        "last": "Sharma",
                        "suffix": ""
                    },
                    {
                        "first": "Radu",
                        "middle": [],
                        "last": "Soricut",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.11942"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learn- ing of language representations. arXiv preprint arXiv:1909.11942.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "How many data points is a prompt worth?",
                "authors": [
                    {
                        "first": "Le",
                        "middle": [],
                        "last": "Teven",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Scao",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "2627--2636",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.naacl-main.208"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, pages 2627-2636, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Vuldeepecker: A deep learning-based system for vulnerability detection",
                "authors": [
                    {
                        "first": "Zhen",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Deqing",
                        "middle": [],
                        "last": "Zou",
                        "suffix": ""
                    },
                    {
                        "first": "Shouhuai",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Xinyu",
                        "middle": [],
                        "last": "Ou",
                        "suffix": ""
                    },
                    {
                        "first": "Hai",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Sujuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhijun",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Yuyi",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Text Summarization Branches Out",
                "volume": "",
                "issue": "",
                "pages": "74--81",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1801.01681"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, and Yuyi Zhong. 2018. Vuldeepecker: A deep learning-based sys- tem for vulnerability detection. arXiv preprint arXiv:1801.01681. Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "WANLI: Worker and AI collaboration for natural language inference dataset creation",
                "authors": [
                    {
                        "first": "Alisa",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Swabha",
                        "middle": [],
                        "last": "Swayamdipta",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022",
                "volume": "",
                "issue": "",
                "pages": "6826--6847",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alisa Liu, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. 2022. WANLI: Worker and AI collabora- tion for natural language inference dataset creation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6826-6847, Abu Dhabi, United Arab Emirates. Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
                "authors": [
                    {
                        "first": "Chia-Wei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Iulian",
                        "middle": [],
                        "last": "Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Noseworthy",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Charlin",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "2122--2132",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D16-1230"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose- worthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2122-2132, Austin, Texas. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Cutting down on prompts and parameters: Simple few-shot learning with language models",
                "authors": [
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Logan",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [
                            "V"
                        ],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Ivana",
                        "middle": [],
                        "last": "Balazevic",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Wallace",
                        "suffix": ""
                    },
                    {
                        "first": "Fabio",
                        "middle": [],
                        "last": "Petroni",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
                "volume": "",
                "issue": "",
                "pages": "2824--2835",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2022.findings-acl.222"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Robert Logan IV, Ivana Balazevic, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel. 2022. Cutting down on prompts and parameters: Simple few-shot learning with language models. In Find- ings of the Association for Computational Linguis- tics: ACL 2022, pages 2824-2835, Dublin, Ireland. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Chatgpt as a factual inconsistency evaluator for abstractive text summarization",
                "authors": [
                    {
                        "first": "Zheheng",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "Qianqian",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Sophia",
                        "middle": [],
                        "last": "Ananiadou",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for abstractive text summarization.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Evaluation in the context of natural language generation",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Mellish",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dale",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Computer Speech & Language",
                "volume": "12",
                "issue": "4",
                "pages": "349--373",
                "other_ids": {
                    "DOI": [
                        "10.1006/csla.1998.0106"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "C Mellish and R Dale. 1998. Evaluation in the context of natural language generation. Computer Speech & Language, 12(4):349-373.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "A quantitative study of data in the NLP community",
                "authors": [
                    {
                        "first": "Margot",
                        "middle": [],
                        "last": "Mieskes",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the First ACL Workshop on Ethics in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "23--29",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W17-1603"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Margot Mieskes. 2017. A quantitative study of data in the NLP community. In Proceedings of the First ACL Workshop on Ethics in Natural Language Pro- cessing, pages 23-29, Valencia, Spain. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Reframing instructional prompts to GPTk's language",
                "authors": [
                    {
                        "first": "Swaroop",
                        "middle": [],
                        "last": "Mishra",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Khashabi",
                        "suffix": ""
                    },
                    {
                        "first": "Chitta",
                        "middle": [],
                        "last": "Baral",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
                "volume": "",
                "issue": "",
                "pages": "589--612",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2022.findings-acl.50"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022. Reframing instructional prompts to GPTk's language. In Find- ings of the Association for Computational Linguistics: ACL 2022, pages 589-612, Dublin, Ireland. Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Why we need new evaluation metrics for NLG",
                "authors": [
                    {
                        "first": "Jekaterina",
                        "middle": [],
                        "last": "Novikova",
                        "suffix": ""
                    },
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "Du\u0161ek",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [
                            "Cercas"
                        ],
                        "last": "Curry",
                        "suffix": ""
                    },
                    {
                        "first": "Verena",
                        "middle": [],
                        "last": "Rieser",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "2241--2252",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D17-1238"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cer- cas Curry, and Verena Rieser. 2017a. Why we need new evaluation metrics for NLG. In Proceedings of the 2017 Conference on Empirical Methods in Natu- ral Language Processing, pages 2241-2252, Copen- hagen, Denmark. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Why we need new evaluation metrics for nlg",
                "authors": [
                    {
                        "first": "Jekaterina",
                        "middle": [],
                        "last": "Novikova",
                        "suffix": ""
                    },
                    {
                        "first": "Ond\u0159ej",
                        "middle": [],
                        "last": "Du\u0161ek",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [
                            "Cercas"
                        ],
                        "last": "Curry",
                        "suffix": ""
                    },
                    {
                        "first": "Verena",
                        "middle": [],
                        "last": "Rieser",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1707.06875"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cercas Curry, and Verena Rieser. 2017b. Why we need new evaluation metrics for nlg. arXiv preprint arXiv:1707.06875.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Machine-inthe-loop rewriting for creative image captioning",
                "authors": [
                    {
                        "first": "Vishakh",
                        "middle": [],
                        "last": "Padmakumar",
                        "suffix": ""
                    },
                    {
                        "first": "He",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "573--586",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2022.naacl-main.42"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Vishakh Padmakumar and He He. 2022. Machine-in- the-loop rewriting for creative image captioning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 573-586, Seattle, United States. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {
                    "DOI": [
                        "10.3115/1073083.1073135"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Multi-label vulnerability detection of smart contracts based on bi-lstm and attention mechanism",
                "authors": [
                    {
                        "first": "Haohan",
                        "middle": [],
                        "last": "Shenyi Qian",
                        "suffix": ""
                    },
                    {
                        "first": "Yaqiong",
                        "middle": [],
                        "last": "Ning",
                        "suffix": ""
                    },
                    {
                        "first": "Mengqi",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Electronics",
                "volume": "11",
                "issue": "19",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shenyi Qian, Haohan Ning, Yaqiong He, and Mengqi Chen. 2022. Multi-label vulnerability detection of smart contracts based on bi-lstm and attention mech- anism. Electronics, 11(19):3260.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "2009a. An investigation into the validity of some metrics for automatically evaluating natural language generation systems",
                "authors": [
                    {
                        "first": "Ehud",
                        "middle": [],
                        "last": "Reiter",
                        "suffix": ""
                    },
                    {
                        "first": "Anja",
                        "middle": [],
                        "last": "Belz",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Computational Linguistics",
                "volume": "35",
                "issue": "4",
                "pages": "529--558",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ehud Reiter and Anja Belz. 2009a. An investigation into the validity of some metrics for automatically evalu- ating natural language generation systems. Computa- tional Linguistics, 35(4):529-558.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "An investigation into the validity of some metrics for automatically evaluating natural language generation systems",
                "authors": [
                    {
                        "first": "Ehud",
                        "middle": [],
                        "last": "Reiter",
                        "suffix": ""
                    },
                    {
                        "first": "Anja",
                        "middle": [],
                        "last": "Belz",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Computational Linguistics",
                "volume": "35",
                "issue": "4",
                "pages": "529--558",
                "other_ids": {
                    "DOI": [
                        "10.1162/coli.2009.35.4.35405"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ehud Reiter and Anja Belz. 2009b. An investiga- tion into the validity of some metrics for automati- cally evaluating natural language generation systems. Computational Linguistics, 35(4):529-558.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Prompt programming for large language models: Beyond the few-shot paradigm",
                "authors": [
                    {
                        "first": "Laria",
                        "middle": [],
                        "last": "Reynolds",
                        "suffix": ""
                    },
                    {
                        "first": "Kyle",
                        "middle": [],
                        "last": "Mcdonell",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems",
                "volume": "",
                "issue": "",
                "pages": "1--7",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Laria Reynolds and Kyle McDonell. 2021. Prompt pro- gramming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Comput- ing Systems, pages 1-7.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Identifying morality frames in political tweets using relational learning",
                "authors": [
                    {
                        "first": "Shamik",
                        "middle": [],
                        "last": "Roy",
                        "suffix": ""
                    },
                    {
                        "first": "Maria",
                        "middle": [
                            "Leonor"
                        ],
                        "last": "Pacheco",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Goldwasser",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "9939--9958",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.emnlp-main.783"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shamik Roy, Maria Leonor Pacheco, and Dan Gold- wasser. 2021. Identifying morality frames in political tweets using relational learning. In Proceedings of the 2021 Conference on Empirical Methods in Natu- ral Language Processing, pages 9939-9958, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Better than random: Reliable nlg human evaluation with constrained active sampling",
                "authors": [
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Ruan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Pu",
                        "suffix": ""
                    },
                    {
                        "first": "Mingqi",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    },
                    {
                        "first": "Yuesheng",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "38",
                "issue": "",
                "pages": "18915--18923",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jie Ruan, Xiao Pu, Mingqi Gao, Xiaojun Wan, and Yuesheng Zhu. 2024. Better than random: Reliable nlg human evaluation with constrained active sam- pling. In Proceedings of the AAAI Conference on Ar- tificial Intelligence, volume 38, pages 18915-18923.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "On the definition of prescriptive annotation guidelines for language-agnostic subjectivity detection",
                "authors": [
                    {
                        "first": "Federico",
                        "middle": [],
                        "last": "Ruggeri",
                        "suffix": ""
                    },
                    {
                        "first": "Francesco",
                        "middle": [],
                        "last": "Antici",
                        "suffix": ""
                    },
                    {
                        "first": "Andrea",
                        "middle": [],
                        "last": "Galassi",
                        "suffix": ""
                    },
                    {
                        "first": "Katerina",
                        "middle": [],
                        "last": "Korre",
                        "suffix": ""
                    },
                    {
                        "first": "Arianna",
                        "middle": [],
                        "last": "Muti",
                        "suffix": ""
                    },
                    {
                        "first": "Alberto",
                        "middle": [],
                        "last": "Barr\u00f3n-Cede\u00f1o",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of Text2Story-Sixth Workshop on Narrative Extraction From Texts, held in conjunction with the 45th European Conference on Information Retrieval (ECIR 2023)",
                "volume": "3370",
                "issue": "",
                "pages": "103--111",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Federico Ruggeri, Francesco Antici, Andrea Galassi, Katerina Korre, Arianna Muti, and Alberto Barr\u00f3n- Cede\u00f1o. 2023. On the definition of prescriptive anno- tation guidelines for language-agnostic subjectivity detection. In Proceedings of Text2Story-Sixth Work- shop on Narrative Extraction From Texts, held in conjunction with the 45th European Conference on Information Retrieval (ECIR 2023), volume 3370, pages 103-111. CEUR-WS. org.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Corpus annotation through crowdsourcing: Towards best practice guidelines",
                "authors": [
                    {
                        "first": "Marta",
                        "middle": [],
                        "last": "Sabou",
                        "suffix": ""
                    },
                    {
                        "first": "Kalina",
                        "middle": [],
                        "last": "Bontcheva",
                        "suffix": ""
                    },
                    {
                        "first": "Leon",
                        "middle": [],
                        "last": "Derczynski",
                        "suffix": ""
                    },
                    {
                        "first": "Arno",
                        "middle": [],
                        "last": "Scharl",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)",
                "volume": "",
                "issue": "",
                "pages": "859--866",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marta Sabou, Kalina Bontcheva, Leon Derczynski, and Arno Scharl. 2014. Corpus annotation through crowdsourcing: Towards best practice guidelines. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), pages 859-866, Reykjavik, Iceland. European Lan- guage Resources Association (ELRA).",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Improved boosting algorithms using confidence-rated predictions",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Robert",
                        "suffix": ""
                    },
                    {
                        "first": "Yoram",
                        "middle": [],
                        "last": "Schapire",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proceedings of the eleventh annual conference on Computational learning theory",
                "volume": "",
                "issue": "",
                "pages": "80--91",
                "other_ids": {
                    "DOI": [
                        "10.1023/A:1007614523901"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Robert E Schapire and Yoram Singer. 1998. Improved boosting algorithms using confidence-rated predic- tions. In Proceedings of the eleventh annual confer- ence on Computational learning theory, pages 80-91.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Few-shot text generation with natural language instructions",
                "authors": [
                    {
                        "first": "Timo",
                        "middle": [],
                        "last": "Schick",
                        "suffix": ""
                    },
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "390--402",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.emnlp-main.32"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Timo Schick and Hinrich Sch\u00fctze. 2021. Few-shot text generation with natural language instructions. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 390- 402, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "this is a problem, don't you agree?\" framing and bias in human evaluation for natural language generation",
                "authors": [
                    {
                        "first": "Stephanie",
                        "middle": [],
                        "last": "Schoch",
                        "suffix": ""
                    },
                    {
                        "first": "Diyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Yangfeng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 1st Workshop on Evaluating NLG Evaluation",
                "volume": "",
                "issue": "",
                "pages": "10--16",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stephanie Schoch, Diyi Yang, and Yangfeng Ji. 2020. \"this is a problem, don't you agree?\" framing and bias in human evaluation for natural language generation. In Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 10-16, Online (Dublin, Ire- land). Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Toward human readable prompt tuning: Kubrick's the shining is a good movie",
                "authors": [
                    {
                        "first": "Weijia",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaochuang",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Hila",
                        "middle": [],
                        "last": "Gonen",
                        "suffix": ""
                    },
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Holtzman",
                        "suffix": ""
                    },
                    {
                        "first": "Yulia",
                        "middle": [],
                        "last": "Tsvetkov",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2212.10539"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer. 2022. Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too? arXiv preprint arXiv:2212.10539.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "The human evaluation datasheet 1.0: A template for recording details of human evaluation experiments in nlp",
                "authors": [
                    {
                        "first": "Anastasia",
                        "middle": [],
                        "last": "Shimorina",
                        "suffix": ""
                    },
                    {
                        "first": "Anya",
                        "middle": [],
                        "last": "Belz",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2103.09710"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Anastasia Shimorina and Anya Belz. 2021. The human evaluation datasheet 1.0: A template for recording details of human evaluation experiments in nlp. arXiv preprint arXiv:2103.09710.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Investigating prior knowledge for challenging Chinese machine reading comprehension",
                "authors": [
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Dian",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Dong",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Transactions of the Association for Computational Linguistics",
                "volume": "8",
                "issue": "",
                "pages": "141--155",
                "other_ids": {
                    "DOI": [
                        "10.1162/tacl_a_00305"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. 2020. In- vestigating prior knowledge for challenging Chinese machine reading comprehension. Transactions of the Association for Computational Linguistics, 8:141- 155.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Improving and simplifying pattern exploiting training",
                "authors": [
                    {
                        "first": "Derek",
                        "middle": [],
                        "last": "Tam",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Rakesh",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Menon",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "4980--4991",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.emnlp-main.407"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Derek Tam, Rakesh R. Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel. 2021. Improving and simplifying pattern exploiting training. In Proceed- ings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4980-4991, Online and Punta Cana, Dominican Republic. Asso- ciation for Computational Linguistics.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Best practices for the human evaluation of automatically generated text",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Van Der Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Albert",
                        "middle": [],
                        "last": "Gatt",
                        "suffix": ""
                    },
                    {
                        "first": "Sander",
                        "middle": [],
                        "last": "Emiel Van Miltenburg",
                        "suffix": ""
                    },
                    {
                        "first": "Emiel",
                        "middle": [],
                        "last": "Wubben",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Krahmer",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 12th International Conference on Natural Language Generation",
                "volume": "",
                "issue": "",
                "pages": "355--368",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W19-8643"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chris van der Lee, Albert Gatt, Emiel van Miltenburg, Sander Wubben, and Emiel Krahmer. 2019. Best practices for the human evaluation of automatically generated text. In Proceedings of the 12th Interna- tional Conference on Natural Language Generation, pages 355-368, Tokyo, Japan. Association for Com- putational Linguistics.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Evaluating the text quality, human likeness and tailoring component of PASS: A Dutch data-to-text system for soccer",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Van Der Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Bart",
                        "middle": [],
                        "last": "Verduijn",
                        "suffix": ""
                    },
                    {
                        "first": "Emiel",
                        "middle": [],
                        "last": "Krahmer",
                        "suffix": ""
                    },
                    {
                        "first": "Sander",
                        "middle": [],
                        "last": "Wubben",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 27th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "962--972",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris van der Lee, Bart Verduijn, Emiel Krahmer, and Sander Wubben. 2018. Evaluating the text quality, human likeness and tailoring component of PASS: A Dutch data-to-text system for soccer. In Proceedings of the 27th International Conference on Computa- tional Linguistics, pages 962-972, Santa Fe, New Mexico, USA. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Book review: Introduction to information retrieval by christopher D. manning, prabhakar raghavan, and hinrich Sch\u00fctze",
                "authors": [
                    {
                        "first": "Olga",
                        "middle": [],
                        "last": "Vechtomova",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "35",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1162/coli.2009.35.2.307"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Olga Vechtomova. 2009. Book review: Introduction to information retrieval by christopher D. manning, prabhakar raghavan, and hinrich Sch\u00fctze. Computa- tional Linguistics, 35(2).",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Self-consistency improves chain of thought reasoning in language models",
                "authors": [
                    {
                        "first": "Xuezhi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Dale",
                        "middle": [],
                        "last": "Schuurmans",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Ed",
                        "middle": [],
                        "last": "Chi",
                        "suffix": ""
                    },
                    {
                        "first": "Sharan",
                        "middle": [],
                        "last": "Narang",
                        "suffix": ""
                    },
                    {
                        "first": "Aakanksha",
                        "middle": [],
                        "last": "Chowdhery",
                        "suffix": ""
                    },
                    {
                        "first": "Denny",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2203.11171"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "A unified view of multi-label performance measures",
                "authors": [
                    {
                        "first": "Xi-Zhu",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhi-Hua",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "international conference on machine learning",
                "volume": "",
                "issue": "",
                "pages": "3780--3788",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xi-Zhu Wu and Zhi-Hua Zhou. 2017. A unified view of multi-label performance measures. In international conference on machine learning, pages 3780-3788.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pmlr. Zhilin",
                        "suffix": ""
                    },
                    {
                        "first": "Zihang",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Jaime",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Russ",
                        "middle": [
                            "R"
                        ],
                        "last": "Carbonell",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "PMLR. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for lan- guage understanding. Advances in neural informa- tion processing systems, 32.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Bartscore: Evaluating generated text as text generation",
                "authors": [
                    {
                        "first": "Weizhe",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "34",
                "issue": "",
                "pages": "27263--27277",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text gener- ation. Advances in Neural Information Processing Systems, 34:27263-27277.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Bertscore: Evaluating text generation with bert",
                "authors": [
                    {
                        "first": "Tianyi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Varsha",
                        "middle": [],
                        "last": "Kishore",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [
                            "Q"
                        ],
                        "last": "Weinberger",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.09675"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Eval- uating text generation with bert. arXiv preprint arXiv:1904.09675.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Maxime",
                        "middle": [],
                        "last": "Peyrard",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [
                            "M"
                        ],
                        "last": "Meyer",
                        "suffix": ""
                    },
                    {
                        "first": "Steffen",
                        "middle": [],
                        "last": "Eger",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.02622"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M Meyer, and Steffen Eger. 2019. Moverscore: Text generation evaluating with contextualized em- beddings and earth mover distance. arXiv preprint arXiv:1909.02622.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "Five prompt variations for two instructions RAW INSTRUCTION Raw prompt: Write a human evaluation guideline for the Summarization task. The evaluation type is Pairwise Comparison. Raw prompt with evaluation aspects: Write a human evaluation guideline for the Summarization task. The evaluation type is Pairwise Comparison",
                "authors": [
                    {
                        "first": "Kaitlyn",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Su",
                        "middle": [
                            "Lin"
                        ],
                        "last": "Blodgett",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Trischler",
                        "suffix": ""
                    },
                    {
                        "first": "Hal",
                        "middle": [],
                        "last": "Daum\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Iii",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Kaheer",
                        "middle": [],
                        "last": "Suleman",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Olteanu",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Deconstructing nlg evaluation: Evaluation practices, assumptions, and their implications",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2205.06828"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kaitlyn Zhou, Su Lin Blodgett, Adam Trischler, Hal Daum\u00e9 III, Kaheer Suleman, and Alexandra Olteanu. 2022. Deconstructing nlg evaluation: Evaluation practices, assumptions, and their implications. arXiv preprint arXiv:2205.06828. Five prompt variations for two instructions RAW INSTRUCTION Raw prompt: Write a human evaluation guideline for the Summarization task. The evaluation type is Pairwise Comparison. Raw prompt with evaluation aspects: Write a human evaluation guideline for the Summarization task. The evaluation type is Pairwise Comparison. Evaluate the following aspects: accuracy, coherence, consistency, relevance, fluency, informativeness, coverage, overall. The evaluation scale is 1-5 (1 is poor and 5 is excellent).",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "STRUCTURED INSTRUCTION Structured prompt: Human evaluation task: Summarization Evaluation type: Pairwise Comparison Human evaluation guideline: Structured prompt with evaluation aspects: Human evaluation task: Summarization Evaluation type: Pairwise Comparison Evaluation aspects: accuracy, coherence, consistency, relevance, fluency, informativeness, coverage, overall Evaluation scale: 1-5 (1 is poor and 5 is excellent) Human evaluation guideline: Structured prompt with evaluation aspects and constraints: Human evaluation task: Summarization Evaluation type: Pairwise Comparison Evaluation aspects: accuracy, coherence, consistency, relevance, fluency, informativeness, coverage, overall Evaluation scale: 1-5 (1 is poor and 5 is excellent) Please be mindful of the following issues and avoid them: definition ambiguity, bias, assuming prior knowledge",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "STRUCTURED INSTRUCTION Structured prompt: Human evaluation task: Summarization Evaluation type: Pairwise Comparison Human evaluation guideline: Structured prompt with evaluation aspects: Human evaluation task: Summarization Evaluation type: Pairwise Comparison Evaluation aspects: accuracy, coherence, consistency, relevance, fluency, informativeness, coverage, overall Evaluation scale: 1-5 (1 is poor and 5 is excellent) Human evaluation guideline: Structured prompt with evaluation aspects and constraints: Human evaluation task: Summarization Evaluation type: Pairwise Comparison Evaluation aspects: accuracy, coherence, consistency, relevance, fluency, informativeness, coverage, overall Evaluation scale: 1-5 (1 is poor and 5 is excellent) Please be mindful of the following issues and avoid them: definition ambiguity, bias, assuming prior knowledge, insufficient coverage, lack of rating scale, lack of adaptability, and neglecting ethical implications Human evaluation guideline:",
                "links": null
            }
        },
        "ref_entries": {
            "TABREF0": {
                "text": "Distributions of vulnerability types on authentic and synthetic guidelines with EthI, UncB, AmbD, UncR, EdgC, PriK, InfI, OthE refers to Ethical Issues, Unconscious Bias, Ambiguous Definition, Unclear Rating, Edge Cases, Prior Knowledge, Inflexible Instructions and Others respectively. \"None\" means the guideline has no vulnerability at all. The ratio calculation is achieved by taking the number of guidelines that include a particular category and dividing it by the total count of guidelines.",
                "content": "<table><tr><td/><td/><td/><td/><td colspan=\"4\">Authentic Guidelines</td><td colspan=\"3\">Synthetic Guidelines</td></tr><tr><td/><td/><td/><td/><td/><td/><td>54.0</td><td/><td/><td/></tr><tr><td/><td/><td/><td/><td>48.6</td><td>48.2</td><td/><td/><td/><td/></tr><tr><td/><td/><td/><td/><td/><td/><td>40.0</td><td/><td/><td/><td>42.7</td></tr><tr><td/><td/><td/><td/><td/><td/><td/><td/><td/><td>28.0</td></tr><tr><td/><td/><td/><td/><td/><td/><td/><td>24.0</td><td/><td/><td>22.9</td></tr><tr><td>11.4</td><td>0.0</td><td>7.4</td><td>2.9</td><td/><td/><td/><td/><td>8.8</td><td>4.4</td><td>2.9</td><td>5.1</td><td>4.0</td><td>5.1</td></tr><tr><td colspan=\"2\">EthI</td><td colspan=\"2\">UncB</td><td colspan=\"2\">AmbD</td><td>UncR</td><td colspan=\"2\">EdgC</td><td>PriK</td><td>InfI</td><td>OthE</td><td>None</td></tr><tr><td colspan=\"3\">Figure 1:</td><td/><td/><td/><td/><td/><td/><td/></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF1": {
                "text": "Authentic human evaluation guideline extracted fromPadmakumar and He (2022) with vulnerabilities of Ambiguous Definition and Unclear Rating.",
                "content": "<table/>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF2": {
                "text": "table below for scoring the candidate text. You must provide a rating for all examples to proceed. Example answers of vulnerability types detected (generated) by LLMs with few-shot CoT prompting for a human evaluation guideline in the test set.",
                "content": "<table><tr><td/><td>GOLD</td><td>Ambiguous Definition, Unclear Rating</td></tr><tr><td/><td/><td>REASONING: The guideline does not pro-</td></tr><tr><td/><td/><td>vide a clear definition of the task, making</td></tr><tr><td/><td/><td>it difficult for evaluators to interpret the in-</td></tr><tr><td/><td/><td>structions. It also does not provide a detailed</td></tr><tr><td/><td>TEXT-</td><td>explanation of the rating aspects, leading to</td></tr><tr><td/><td>DAVINCI-</td><td>multiple interpretations for different eval-</td></tr><tr><td/><td>003</td><td>uators, causing \"Unclear Rating\". So the</td></tr><tr><td/><td/><td>answer is \"Ambiguous Definition\" and \"Un-</td></tr><tr><td/><td/><td>clear Rating\".</td></tr><tr><td/><td/><td>ANSWER: Ambiguous Definition, Unclear</td></tr><tr><td>Category: Meaning Mild (1 sent): deduct 5-10 pts e.g., wrong pronoun</td><td/><td>Rating REASONING: The guideline does not pro-vide clear instructions on how to handle edge</td></tr><tr><td>Significant (1 sent): deduct 30 pts e.g., relevant words but diff meaning Mild (2 sent): deduct 10-20 pts</td><td>GPT-3.5-Turbo</td><td>cases where the candidate text contains ran-dom words unrelated to the text. So the an-swer is \"Edge Cases\"</td></tr><tr><td>Significant (2 sent): deduct 60-70 pts</td><td/><td>ANSWER: Edge Cases</td></tr><tr><td>Category: Grammaticality</td><td>LLaMA</td><td>Mild, Significant</td></tr><tr><td>Mild (1 sent): deduct 5-10 pts e.g., missing conjunction Significant (1 sent): deduct 25 pts e.g., incomplete sentence Mild (2 sent): deduct 10-20 pts Significant (2 sent): deduct 50-60 pts</td><td>Flan Flan-Alpaca Falcon</td><td>Let's think step by step: REASONING: The first task is to score Categoryeaningequal to 12 pts e None</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            },
            "TABREF5": {
                "text": "Five prompts utilized for generating synthetic guidelines. The highlighted blue portions are employed across 12 NLG tasks, 2 evaluation methods and 2 key words, which are interchanged interchangeably.",
                "content": "<table/>",
                "html": null,
                "num": null,
                "type_str": "table"
            }
        }
    }
}
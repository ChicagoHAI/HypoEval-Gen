{
    "paper_id": "tacl_a_00373",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2025-02-14T13:47:11.546415Z"
    },
    "title": "SummEval: Re-evaluating Summarization Evaluation",
    "authors": [
        {
            "first": "Alexander",
            "middle": [
                "R"
            ],
            "last": "Fabbri",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Yale University",
                "location": {}
            },
            "email": "alexander.fabbri@yale.edu"
        },
        {
            "first": "Wojciech",
            "middle": [],
            "last": "Kry\u015bci \u0143ski",
            "suffix": "",
            "affiliation": {
                "laboratory": "Salesforce Research",
                "institution": "",
                "location": {}
            },
            "email": "kryscinski@salesforce.com"
        },
        {
            "first": "Bryan",
            "middle": [],
            "last": "Mccann",
            "suffix": "",
            "affiliation": {
                "laboratory": "Salesforce Research",
                "institution": "",
                "location": {}
            },
            "email": "bryan.mccann.is@gmail.com"
        },
        {
            "first": "Caiming",
            "middle": [],
            "last": "Xiong",
            "suffix": "",
            "affiliation": {
                "laboratory": "Salesforce Research",
                "institution": "",
                "location": {}
            },
            "email": "cxiong@salesforce.com"
        },
        {
            "first": "Richard",
            "middle": [],
            "last": "Socher",
            "suffix": "",
            "affiliation": {
                "laboratory": "Salesforce Research",
                "institution": "",
                "location": {}
            },
            "email": "richard@socher.org"
        },
        {
            "first": "Dragomir",
            "middle": [],
            "last": "Radev",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Yale University",
                "location": {}
            },
            "email": "dragomir.radev@yale.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowdsourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of modelgenerated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments. the queen's guard was left red-faced after he slipped on a he Coh: 5.0 Coh: 3.4 manhole cover lost his footing and slid sideways, knocking his Con: 5.0 Con: 3.8 bearskin on the side . the embarrassed soldier quickly scrambled Flu: 5.0 Flu: 3.4 to his feet as his colleagues marched past as if nothing had Rel: 5.0 Rel: 3.8 happened . tourist david meadwell recorded the unscheduled manouevre outside buckingham palace on thursday afternoon . holidaymaker david meadwell recorded the unscheduled Coh: 2.7 Coh: 3.2 manouevre outside buckingham palace . he lost his footing and Con: 2.0 Con: 3.4 slid sideways, knocking bearskin on the side of the box . queen 's Flu: 4.7 Flu: 3.4 guard was left red-faced after he slipped on manhole cover .\nRel: 3.7 Rel: 4.0 the entire incident was caught on a manhole cover . the embarrassed soldier quickly scrambled to his feet as his colleagues marched past . buckingham palace guard slipped on manhole cover in front Coh: 3.3 Coh: 3.0 of hundreds of horrified tourists. the queen 's guard was left Con: 5.0 Con: 3.2 red-faced after he slipped on a manhole cover . he lost his footing Flu: 1.7 Flu: 2.8 and dropped his rifle on the side of the box and dropping his rifle .\nRel:4.3 Rel:3.2 the incident was caught on camera camera camera . the guard is thought to have slipped because of metal shutters nailed to the soles of his boots .\n(a) Generated summary examples illustrate common problems found in model outputs, such as ambiguous pronouns, incorrect references, and repetitive content.\nExpert Crowd-worker scores (avg.) scores (avg.) river plate admit they ' dream ' of manchester united striker Coh: 3.0 Coh: 3.0 radamel falcao . the colombia international spent eight years Con: 2.0 Con: 3.6 with the argentine club . falcao has managed just four goals in Flu: 5.0 Flu: 3.0 19 premier league appearances . read : falcao still ' has faith ' Rel: 2.3 Rel: 4.4 that he could continue at man utd next season . click here for the latest manchester united news. the incident occurred on april 7 north of poland in the baltic Coh: 2.0 Coh: 4.0 sea . u.s. says plane was in international airspace . russia says Con: 1.7 Con: 3.4 it had transponder turned off and was flying toward russia Flu: 3",
    "pdf_parse": {
        "paper_id": "tacl_a_00373",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowdsourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of modelgenerated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments. the queen's guard was left red-faced after he slipped on a he Coh: 5.0 Coh: 3.4 manhole cover lost his footing and slid sideways, knocking his Con: 5.0 Con: 3.8 bearskin on the side . the embarrassed soldier quickly scrambled Flu: 5.0 Flu: 3.4 to his feet as his colleagues marched past as if nothing had Rel: 5.0 Rel: 3.8 happened . tourist david meadwell recorded the unscheduled manouevre outside buckingham palace on thursday afternoon . holidaymaker david meadwell recorded the unscheduled Coh: 2.7 Coh: 3.2 manouevre outside buckingham palace . he lost his footing and Con: 2.0 Con: 3.4 slid sideways, knocking bearskin on the side of the box . queen 's Flu: 4.7 Flu: 3.4 guard was left red-faced after he slipped on manhole cover .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Rel: 3.7 Rel: 4.0 the entire incident was caught on a manhole cover . the embarrassed soldier quickly scrambled to his feet as his colleagues marched past . buckingham palace guard slipped on manhole cover in front Coh: 3.3 Coh: 3.0 of hundreds of horrified tourists. the queen 's guard was left Con: 5.0 Con: 3.2 red-faced after he slipped on a manhole cover . he lost his footing Flu: 1.7 Flu: 2.8 and dropped his rifle on the side of the box and dropping his rifle .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Rel:4.3 Rel:3.2 the incident was caught on camera camera camera . the guard is thought to have slipped because of metal shutters nailed to the soles of his boots .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "(a) Generated summary examples illustrate common problems found in model outputs, such as ambiguous pronouns, incorrect references, and repetitive content.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Expert Crowd-worker scores (avg.) scores (avg.) river plate admit they ' dream ' of manchester united striker Coh: 3.0 Coh: 3.0 radamel falcao . the colombia international spent eight years Con: 2.0 Con: 3.6 with the argentine club . falcao has managed just four goals in Flu: 5.0 Flu: 3.0 19 premier league appearances . read : falcao still ' has faith ' Rel: 2.3 Rel: 4.4 that he could continue at man utd next season . click here for the latest manchester united news. the incident occurred on april 7 north of poland in the baltic Coh: 2.0 Coh: 4.0 sea . u.s. says plane was in international airspace . russia says Con: 1.7 Con: 3.4 it had transponder turned off and was flying toward russia Flu: 3",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Text summarization aims to compress long document(s) into a short, fluent, and human-readable form that preserves the most salient information from the source document. *",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The field has benefited from advances in neural network architectures (Sutskever et al., 2014; Bahdanau et al., 2014; Vinyals et al., 2015; Vaswani et al., 2017) as well as the availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018) . Recent advances in pretrained language models, such as BERT (Devlin et al., 2019) , have motivated a corresponding shift to pretraining methods in summarization (Liu and Lapata, 2019; Zhang et al., 2019b; Dong et al., 2019; Ziegler et al., 2019; Raffel et al., 2019; Lewis et al., 2019) .",
                "cite_spans": [
                    {
                        "start": 70,
                        "end": 94,
                        "text": "(Sutskever et al., 2014;",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 95,
                        "end": 117,
                        "text": "Bahdanau et al., 2014;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 118,
                        "end": 139,
                        "text": "Vinyals et al., 2015;",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 140,
                        "end": 161,
                        "text": "Vaswani et al., 2017)",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 214,
                        "end": 230,
                        "text": "(Sandhaus, 2008;",
                        "ref_id": null
                    },
                    {
                        "start": 231,
                        "end": 252,
                        "text": "Hermann et al., 2015;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 253,
                        "end": 273,
                        "text": "Grusky et al., 2018;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 274,
                        "end": 295,
                        "text": "Narayan et al., 2018)",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 358,
                        "end": 379,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 459,
                        "end": 481,
                        "text": "(Liu and Lapata, 2019;",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 482,
                        "end": 502,
                        "text": "Zhang et al., 2019b;",
                        "ref_id": null
                    },
                    {
                        "start": 503,
                        "end": 521,
                        "text": "Dong et al., 2019;",
                        "ref_id": null
                    },
                    {
                        "start": 522,
                        "end": 543,
                        "text": "Ziegler et al., 2019;",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 544,
                        "end": 564,
                        "text": "Raffel et al., 2019;",
                        "ref_id": "BIBREF53"
                    },
                    {
                        "start": 565,
                        "end": 584,
                        "text": "Lewis et al., 2019)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Equal contributions from authors",
                "sec_num": null
            },
            {
                "text": "A standard dataset for training summarization models is the CNN/DailyMail corpus (Hermann et al., 2015) , originally a question answering task, which was repurposed for summarization by Nallapati et al. (2016) . The dataset consists of news articles and associated human-created bulletpoint summaries. The ROUGE (Lin, 2004b) metric, which measures lexical overlap between generated and target summaries, is then typically used together with crowd-sourced human annotations for model evaluation. While the current setup has become standardized, we believe several factors prevent a more complete comparison of models, thus negatively impacting the progress of the field.",
                "cite_spans": [
                    {
                        "start": 81,
                        "end": 103,
                        "text": "(Hermann et al., 2015)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 186,
                        "end": 209,
                        "text": "Nallapati et al. (2016)",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 312,
                        "end": 324,
                        "text": "(Lin, 2004b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Equal contributions from authors",
                "sec_num": null
            },
            {
                "text": "As noted by Hardy et al. (2019) , recent papers vastly differ in their evaluation protocol. Existing work often limits model comparisons to only a few baselines and offers human evaluations which are largely inconsistent with prior work. Additionally, despite problems associated with ROUGE when used outside of its original setting (Liu and Liu, 2008; Cohan and Goharian, 2016) as well as the introduction of many variations on ROUGE (Zhou et al., 2006; Ng and Abrecht, 2015; Ganesan, 2015; ShafieiBavani et al., 2018) and other text generation metrics (Peyrard, 2019; Zhao et al., 2019; Zhang et al., 2020; Scialom 391 Transactions of the Association for Computational Linguistics, vol. 9, pp. 391-409, 2021. https://doi.org/10.1162/tacl a 00373 ",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 31,
                        "text": "Hardy et al. (2019)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 333,
                        "end": 352,
                        "text": "(Liu and Liu, 2008;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 353,
                        "end": 378,
                        "text": "Cohan and Goharian, 2016)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 435,
                        "end": 454,
                        "text": "(Zhou et al., 2006;",
                        "ref_id": "BIBREF72"
                    },
                    {
                        "start": 455,
                        "end": 476,
                        "text": "Ng and Abrecht, 2015;",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 477,
                        "end": 491,
                        "text": "Ganesan, 2015;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 492,
                        "end": 519,
                        "text": "ShafieiBavani et al., 2018)",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 554,
                        "end": 569,
                        "text": "(Peyrard, 2019;",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 570,
                        "end": 588,
                        "text": "Zhao et al., 2019;",
                        "ref_id": "BIBREF71"
                    },
                    {
                        "start": 589,
                        "end": 608,
                        "text": "Zhang et al., 2020;",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 609,
                        "end": 620,
                        "text": "Scialom 391",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Equal contributions from authors",
                "sec_num": null
            },
            {
                "text": "Action Editor: Andr\u00e9 F.T. Martins. Submission batch: 8/2020; Revision batch: 11/2020; Published 4/2021. c 2021 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. et al., 2019; Clark et al., 2019) , ROUGE has remained the default automatic evaluation metric. We believe that the shortcomings of the current evaluation protocol are partially caused by the lack of easy-to-use resources for evaluation, both in the form of simplified evaluation toolkits and large collections of model outputs.",
                "cite_spans": [
                    {
                        "start": 193,
                        "end": 206,
                        "text": "et al., 2019;",
                        "ref_id": null
                    },
                    {
                        "start": 207,
                        "end": 226,
                        "text": "Clark et al., 2019)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Equal contributions from authors",
                "sec_num": null
            },
            {
                "text": "In parallel, there is an issue with how evaluation metrics are evaluated themselves. Many of the currently used metrics were developed and assessed using the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) shared-tasks datasets (Dang and Owczarzak 2008 Owczarzak , 2009)) . However, it has recently been shown that the mentioned datasets contain human judgments for model outputs scoring on a lower scale compared to current summarization systems putting into question the true performance of those metrics in the new setting (Peyrard, 2019) .",
                "cite_spans": [
                    {
                        "start": 265,
                        "end": 279,
                        "text": "Owczarzak 2008",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 280,
                        "end": 298,
                        "text": "Owczarzak , 2009))",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 553,
                        "end": 568,
                        "text": "(Peyrard, 2019)",
                        "ref_id": "BIBREF49"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Equal contributions from authors",
                "sec_num": null
            },
            {
                "text": "We address these gaps in complementary ways: 1) We re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using outputs from recent neural summarization models along with expert and crowd-sourced human annotations; 2) We consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) We release aligned summarization model outputs from 23 papers (44 model outputs) published between 2017 and 2019 trained on the CNN/DailyMail dataset to allow for large-scale comparisons of recent summarization models; 4) We release a toolkit of 14 evaluation metrics with an extensible and unified API to promote the reporting of additional metrics in papers; 5) We collect and release expert, as well as crowd-sourced, human judgments for 16 model outputs on 100 articles over 4 dimensions to further research into human-correlated evaluation metrics. Code and data associated with this work is available at https://github.com/Yale-LILY /SummEval.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Equal contributions from authors",
                "sec_num": null
            },
            {
                "text": "Previous work examining the research setup of text summarization can be broadly categorized into three groups, based on the subject of analysis: evaluation metrics, datasets, and models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Dealing with evaluation methods, Lin (2004a) examined the effectiveness of the ROUGE metric in various DUC tasks. The authors concluded that evaluating against multiple references results in higher correlation scores with human judgments -however, a single-reference setting is sufficient for the metric to be effective. Owczarzak et al. (2012) studied the effects of inconsistencies in human annotations on the rankings of evaluated summarization systems. Results showed that system-level rankings were robust against annotation inconsistencies, but summary-level rankings were not stable in such settings and largely benefit from improving annotator consistency. Rankel et al. (2013) analyzed the performance of different variants of the ROUGE metric using TAC datasets. The authors found that higher-order and less commonly reported ROUGE settings showed a higher correlation with human judgments. In a similar line of work, Graham (2015) conducted a large-scale study of the effectiveness of different ROUGE metric variants and compared it against the BLEU metric on the DUC datasets. Its results highlighted several superior, non-standard ROUGE settings that achieved strong correlations with human judgments on model-generated summaries. In Chaganty et al. (2018) , the authors investigated using an automatic metric to reduce the cost of human evaluation without introducing bias. Together with the study, the authors released a set of human judgments over several model outputs, limited to a small set of model types. Peyrard (2019) showed that standard metrics are in agreement when dealing with summaries in the scoring range found in TAC summaries, but vastly differ in the higher-scoring range found in current models. The authors reported that additional human annotations on modern model outputs are necessary to conduct a conclusive study of evaluation metrics. Hardy et al. (2019) underscore the differences in approaches to human summary evaluation while proposing a highlight-based reference-less evaluation metric. Other work has examined the problems with applying ROUGE in settings such as meeting summarization (Liu and Liu, 2008) and summarization of scientific articles (Cohan and Goharian, 2016) . We build upon this line of research by examining the performance of several automatic evaluation methods, including ROUGE and its variants, against the performance of expert human annotators.",
                "cite_spans": [
                    {
                        "start": 321,
                        "end": 344,
                        "text": "Owczarzak et al. (2012)",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 665,
                        "end": 685,
                        "text": "Rankel et al. (2013)",
                        "ref_id": null
                    },
                    {
                        "start": 928,
                        "end": 941,
                        "text": "Graham (2015)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 1247,
                        "end": 1269,
                        "text": "Chaganty et al. (2018)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 1526,
                        "end": 1540,
                        "text": "Peyrard (2019)",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 1877,
                        "end": 1896,
                        "text": "Hardy et al. (2019)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 2133,
                        "end": 2152,
                        "text": "(Liu and Liu, 2008)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 2194,
                        "end": 2220,
                        "text": "(Cohan and Goharian, 2016)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In relation to datasets, Dernoncourt et al. (2018) presented a detailed taxonomy of existing summarization datasets. The authors highlighted the differences in formats of available corpora and called for creating a unified data standard. In a similar line of research, Grusky et al. (2018) offered a thorough analysis of existing corpora, focusing their efforts on news summarization datasets. The authors also introduced several metrics for evaluating the extractiveness of summaries that are included in the toolkit implemented as part of this work. Kry\u015bci\u0144ski et al. (2020) showed that news-related summarization datasets, such as CNN/DailyMail, contain strong layout biases. The authors revealed that datasets in the current format, where each news article is associated with a single reference summary, leave the task of summarization underconstrained. The paper also highlighted the problem of noisy, low-quality data in automatically collected news datasets.",
                "cite_spans": [
                    {
                        "start": 25,
                        "end": 50,
                        "text": "Dernoncourt et al. (2018)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 269,
                        "end": 289,
                        "text": "Grusky et al. (2018)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 552,
                        "end": 576,
                        "text": "Kry\u015bci\u0144ski et al. (2020)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Looking into models, Zhang et al. (2018a) analyzed the level of abstraction of several recent abstractive summarization models. The authors showed that word-level extractive models achieved a similar level of abstraction to fully abstractive models. In Kedzie et al. (2018) , the authors examined the influence of various model components on the quality of content selection. The study revealed that in the current setting the training signal is dominated by biases present in summarization datasets preventing models from learning accurate content selection. Kry\u015bci\u0144ski et al. (2020) investigate the problem of factual correctness of text summarization models. The authors concluded that the issue of hallucinating facts touches up to 30% of generated summaries and list common types of errors made by generative models. Closely related to that work, Maynez et al. (2020) conducted a large-scale study of abstractive summarizers from the perspective of faithfulness. The authors reached similar conclusions, stating that improving factual faithfulness is a critical issue in summarization. The results also showed that currently available evaluation methods, such as ROUGE and BertScore, are not sufficient to study the problem at hand. Durmus et al. (2020) and Wang et al. (2020) similarly examine faithfulness evaluation, both proposing question answering frameworks as a means of evaluating factual consistency.",
                "cite_spans": [
                    {
                        "start": 21,
                        "end": 41,
                        "text": "Zhang et al. (2018a)",
                        "ref_id": null
                    },
                    {
                        "start": 253,
                        "end": 273,
                        "text": "Kedzie et al. (2018)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 560,
                        "end": 584,
                        "text": "Kry\u015bci\u0144ski et al. (2020)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 852,
                        "end": 872,
                        "text": "Maynez et al. (2020)",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 1238,
                        "end": 1258,
                        "text": "Durmus et al. (2020)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 1263,
                        "end": 1281,
                        "text": "Wang et al. (2020)",
                        "ref_id": "BIBREF62"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Insights and contributions coming from our work are complementary to the conclusions of previous efforts described in this section. To the best of our knowledge, this is the first work in neural text summarization to offer a large-scale, consistent, side-by-side re-evaluation of summarization model outputs and evaluation methods. We also share resources that we hope will prove useful for future work in analyzing and improving summarization models and metrics.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Shortly before publishing this paper, a library for developing summarization metrics was released by Deutsch and Roth (2020) . Our toolkit is complementary to their work as their toolkit includes only 3 of our 12 evaluation metrics.",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 124,
                        "text": "Deutsch and Roth (2020)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "We briefly introduce metrics included in our evaluation toolkit as well as the summarization models for which outputs were collected at the time of releasing this manuscript.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics and Summarization Models",
                "sec_num": "3"
            },
            {
                "text": "Our selection of evaluation methods includes several recently introduced metrics that have been applied to both text generation and summarization, standard machine translation metrics, and other miscellaneous performance statistics. ROUGE (Lin, 2004b) , (Recall-Oriented Understudy for Gisting Evaluation), measures the number of overlapping textual units (n-grams, word sequences) between the generated summary and a set of gold reference summaries.",
                "cite_spans": [
                    {
                        "start": 239,
                        "end": 251,
                        "text": "(Lin, 2004b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "ROUGE-WE (Ng and Abrecht, 2015) extends ROUGE by using soft lexical matching based on the cosine similarity of Word2Vec (Mikolov et al., 2013) embeddings.",
                "cite_spans": [
                    {
                        "start": 9,
                        "end": 31,
                        "text": "(Ng and Abrecht, 2015)",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 120,
                        "end": 142,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF41"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "S 3 (Peyrard et al., 2017 ) is a model-based metric that uses previously proposed evaluation metrics, such as ROUGE, JS-divergence, and ROUGE-WE, as input features for predicting the evaluation score. The model is trained on human judgment datasets from TAC conferences.",
                "cite_spans": [
                    {
                        "start": 4,
                        "end": 25,
                        "text": "(Peyrard et al., 2017",
                        "ref_id": "BIBREF50"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "BertScore (Zhang et al., 2020) computes similarity scores by aligning generated and reference summaries on a token-level. Token alignments are computed greedily to maximize the cosine similarity between contextualized token embeddings from BERT.",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 30,
                        "text": "(Zhang et al., 2020)",
                        "ref_id": "BIBREF68"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "MoverScore (Zhao et al., 2019) measures the semantic distance between a summary and reference text by making use of the Word Mover's Distance (Kusner et al., 2015) operating over n-gram embeddings pooled from BERT representations. Sentence Mover's Similarity (SMS) (Clark et al., 2019) extends Word Mover's Distance to view documents as a bag of sentence embeddings as well as a variation which represents documents as both a bag of sentences and a bag of words.",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 30,
                        "text": "(Zhao et al., 2019)",
                        "ref_id": "BIBREF71"
                    },
                    {
                        "start": 142,
                        "end": 163,
                        "text": "(Kusner et al., 2015)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 265,
                        "end": 285,
                        "text": "(Clark et al., 2019)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "SummaQA (Scialom et al., 2019) applies a BERT-based question-answering model to answer cloze-style questions using generated summaries. Questions are generated by masking named entities in source documents associated with evaluated summaries. The metric reports both the F1 overlap score and QA-model confidence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "BLANC (Vasilyev et al., 2020 ) is a referenceless metric that measures the performance gains of a pre-trained language model given access to a document summary while carrying out language understanding tasks on the source document's text.",
                "cite_spans": [
                    {
                        "start": 6,
                        "end": 28,
                        "text": "(Vasilyev et al., 2020",
                        "ref_id": "BIBREF58"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "SUPERT (Gao et al., 2020 ) is a reference-less metric, originally designed for multi-document summarization, which measures the semantic similarity of model outputs with pseudo-reference summaries created by extracting salient sentences from the source documents, using soft token alignment techniques.",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 24,
                        "text": "(Gao et al., 2020",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "BLEU (Papineni et al., 2002) is a corpuslevel precision-focused metric that calculates n-gram overlap between a candidate and reference utterance and includes a brevity penalty. It is the primary evaluation metric for machine translation.",
                "cite_spans": [
                    {
                        "start": 5,
                        "end": 28,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF46"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "CHRF (Popovi\u0107, 2015) calculates characterbased n-gram overlap between model outputs and reference documents.",
                "cite_spans": [
                    {
                        "start": 5,
                        "end": 20,
                        "text": "(Popovi\u0107, 2015)",
                        "ref_id": "BIBREF51"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "METEOR (Lavie and Agarwal, 2007 ) computes an alignment between candidate and reference sentences by mapping unigrams in the generated summary to 0 or 1 unigrams in the reference, based on stemming, synonyms, and paraphrastic matches. Precision and recall are computed and reported as a harmonic mean.",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 31,
                        "text": "(Lavie and Agarwal, 2007",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "CIDEr (Vedantam et al., 2015) computes {1-4}-gram co-occurrences between the candidate and reference texts, down-weighting common n-grams and calculating cosine similarity between the n-grams of the candidate and reference texts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "Data Statistics: Grusky et al. ( 2018) define three measures of the extractiveness of a dataset. Extractive fragment coverage is the percentage of words in the summary that are from the source article, measuring the extent to which a summary is a derivative of a text. Density is defined as the average length of the extractive fragment to which each summary word belongs. Compression ratio is defined as the word ratio between the articles and its summaries: In addition to these measures, we also include the percentage of n-grams in the summary not found in the input document as a novelty score and the percentage of n-grams in the summary which repeat as a score of redundancy. For a comprehensive explanation of each metric, please refer to the corresponding paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "3.1"
            },
            {
                "text": "We broadly categorize the models included in this study into extractive and abstractive approaches. For each model, we provide a model code (M*) as well as a descriptive model name, which will allow for easy matching with the released data. (Zhou et al., 2018) jointly scores and selects sentences by first building a hierarchical representation of a document and considering the partially outputted summary at each time step.",
                "cite_spans": [
                    {
                        "start": 241,
                        "end": 260,
                        "text": "(Zhou et al., 2018)",
                        "ref_id": "BIBREF73"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Summarization Models",
                "sec_num": "3.2"
            },
            {
                "text": "M2 -BanditSum (Dong et al., 2018) treats extractive summarization as a contextual bandit problem where the document is the context and the sequence of sentences to include in the summary is the action.",
                "cite_spans": [
                    {
                        "start": 14,
                        "end": 33,
                        "text": "(Dong et al., 2018)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "M1 -NEUSUM",
                "sec_num": null
            },
            {
                "text": "M3 - LATENT Zhang et al. (2018b) propose a latent variable extractive model which views rele-vance labels of sentences in a document as binarylatent variables.",
                "cite_spans": [
                    {
                        "start": 5,
                        "end": 32,
                        "text": "LATENT Zhang et al. (2018b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "M1 -NEUSUM",
                "sec_num": null
            },
            {
                "text": "M4 -REFRESH Narayan et al. ( 2018) propose using REINFORCE (Williams, 1992) to extract summaries, approximating the search space during training by limiting to combinations of individually high-scoring sentences.",
                "cite_spans": [
                    {
                        "start": 59,
                        "end": 75,
                        "text": "(Williams, 1992)",
                        "ref_id": "BIBREF63"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "M1 -NEUSUM",
                "sec_num": null
            },
            {
                "text": "M5 -RNES Wu and Hu (2018) propose a coherence model to capture cross-sentence coherence, combining output from the coherence model and ROUGE scores as a reward in a REINFORCE framework.",
                "cite_spans": [
                    {
                        "start": 9,
                        "end": 25,
                        "text": "Wu and Hu (2018)",
                        "ref_id": "BIBREF64"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "M1 -NEUSUM",
                "sec_num": null
            },
            {
                "text": "M6 -JECS (Xu and Durrett, 2019) first extracts sentences from a document and then scores possible constituency-based compressed units to produce the final compressed summary.",
                "cite_spans": [
                    {
                        "start": 9,
                        "end": 31,
                        "text": "(Xu and Durrett, 2019)",
                        "ref_id": "BIBREF65"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "M1 -NEUSUM",
                "sec_num": null
            },
            {
                "text": "M7 -STRASS (Bouscarrat et al., 2019 ) extracts a summary by selecting the sentences with the closest embeddings to the document embedding, learning a transformation to maximize the similarity between the summary and the ground truth reference. -Pointer Generator See et al. (2017) propose a variation of encoder-decoder models, the Pointer Generator Network, where the decoder can choose to generate a word from the vocabulary or copy a word from the input. A coverage mechanism is also proposed to prevent repeatedly attending to the same part of the source document.",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 35,
                        "text": "(Bouscarrat et al., 2019",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 244,
                        "end": 280,
                        "text": "-Pointer Generator See et al. (2017)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "M1 -NEUSUM",
                "sec_num": null
            },
            {
                "text": "M9 -Fast-abs-rl Chen and Bansal (2018) propose a model which first extracts salient sentences with a Pointer Network and rewrites these sentences with a Pointer Generator Network. In addition to maximum likelihood training, a ROUGE-L reward is used to update the extractor via REINFORCE (Williams, 1992) . 2019) learn a reward function from 2,500 human judgments that is used in a reinforcement learning setting.",
                "cite_spans": [
                    {
                        "start": 287,
                        "end": 303,
                        "text": "(Williams, 1992)",
                        "ref_id": "BIBREF63"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "M8",
                "sec_num": null
            },
            {
                "text": "M19 -BertSum-abs Liu and Lapata (2019) introduce a novel document-level encoder on top of BERT (Devlin et al., 2019) , over which they introduce both an extractive and an abstractive model. M20 -GPT-2 Ziegler et al. ( 2019) build off of GPT-2 (Radford et al., 2019) and fine-tune the model by using human labels of which of four sampled summaries is the best to direct fine-tuning in a reinforcement learning framework.",
                "cite_spans": [
                    {
                        "start": 95,
                        "end": 116,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 243,
                        "end": 265,
                        "text": "(Radford et al., 2019)",
                        "ref_id": "BIBREF52"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "M8",
                "sec_num": null
            },
            {
                "text": "M21 -UniLM Dong et al. ( 2019) introduce a model pretrained on three language modeling tasks: unidirectional, bidirectional, and sequenceto-sequence prediction. It is thus applicable to natural language understanding tasks and generation tasks such as abstractive summarization.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "M8",
                "sec_num": null
            },
            {
                "text": "M22 -BART Lewis et al. ( 2019) introduce a denoising autoencoder for pretraining sequence to sequence tasks which is applicable to both natural language understanding and generation tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "M8",
                "sec_num": null
            },
            {
                "text": "M23 -Pegasus Zhang et al. (2019a) introduce a model pretrained with a novel objective function designed for summarization by which important sentences are removed from an input document and then generated from the remaining sentences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "M8",
                "sec_num": null
            },
            {
                "text": "We now describe the resources collected and released together with this manuscript.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Resources",
                "sec_num": "4"
            },
            {
                "text": "The model output collection contains summaries associated with 23 recent papers on neural text summarization described in Section 3.2. We obtained a total of 44 model outputs, as many papers include variations of the main model. All models were trained on the CNN/DailyMail news corpus and the collected summaries were generated using the test split of the dataset without constraints limiting the output length. Outputs were solicited from the authors of papers to ensure comparability between results presented in this paper with those in the original works. They are shared publicly with the consent of the authors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Outputs",
                "sec_num": "4.1"
            },
            {
                "text": "Model outputs were transformed into a unified format and are shared with IDs of the original CNN/DailyMail examples so that generated summaries can be matched with corresponding source articles. Pairing model outputs with original articles was done using a heuristic approach that relied on aligning reference summaries. The pairing process revealed that 38 examples in the CNN/DailyMail test split contained duplicate reference summaries preventing those examples to be correctly aligned. However, this problem involves only 0.3% of the available data and should not have a significant impact on downstream results. IDs of duplicate examples are provided together with the data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Outputs",
                "sec_num": "4.1"
            },
            {
                "text": "The evaluation toolkit contains 14 automatic evaluation metrics described in Section 3.1 consolidated into a Python package. The package provides a high-level, easy-to-use interface unifying all of the underlying metrics. For each metric, we implement both evaluate example and evaluate batch functions that return the metric's score on example-and corpus-levels accordingly. Function inputs and outputs are also unified across all metrics to streamline multimetric evaluation and result processing. The toolkit comes with a standard configuration resembling the most popular settings for each of the metrics to enable easy, out-of-the-box use. However, each metric can be further configured using external gin configuration files. We also provide a command-line tool to evaluate a summarization model with several metrics in parallel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Toolkit",
                "sec_num": "4.2"
            },
            {
                "text": "The collection of human annotations contains summary evaluations of 16 recent neural summarization models solicited from crowd-sourced and expert judges. Annotations were collected for 100 articles randomly picked from the CNN/DailyMail test set. To ensure high quality of annotations, each summary was scored by 5 crowd-sourced and 3 expert workers, amounting to 12800 summary-level annotations. Model outputs were evaluated along the following four dimensions, as in Kry\u015bci\u0144ski et al. (2019) :",
                "cite_spans": [
                    {
                        "start": 469,
                        "end": 493,
                        "text": "Kry\u015bci\u0144ski et al. (2019)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "4.3"
            },
            {
                "text": "Coherence -the collective quality of all sentences. We align this dimension with the DUC quality question (Dang, 2005) of structure and coherence whereby ''the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic.''",
                "cite_spans": [
                    {
                        "start": 106,
                        "end": 118,
                        "text": "(Dang, 2005)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "4.3"
            },
            {
                "text": "Consistency -the factual alignment between the summary and the summarized source. A factually consistent summary contains only statements that are entailed by the source document. Annotators were also asked to penalize summaries that contained hallucinated facts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "4.3"
            },
            {
                "text": "Fluency -the quality of individual sentences. Drawing again from the DUC quality guidelines, sentences in the summary ''should have no formatting problems, capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read.''",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "4.3"
            },
            {
                "text": "Relevance -selection of important content from the source. The summary should include only important information from the source document. Annotators were instructed to penalize summaries that contained redundancies and excess information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "4.3"
            },
            {
                "text": "The data collection interface provided judges with the source article and associated summaries grouped in sets of 5. Each group of summaries contained the reference summary associated with the source article to establish a common point of reference between groups. Summary grouping and order within groups were randomized for each annotator. Judges were asked to rate the summaries on a Likert scale from 1 to 5 (higher better) along the four mentioned dimensions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "4.3"
            },
            {
                "text": "Crowd-sourced annotators were hired through the Amazon Mechanical Turk platform. The hiring criteria were set to a minimum of 10000 approved HITs and an approval rate of 97% or higher. Geographic constraints for workers were set to United States, United Kingdom, and Australia to ensure that summaries were evaluated by native English speakers. Compensation was carefully calculated to ensure an average wage of 12 USD per hour. Gillick and Liu (2010) showed that summary judgments obtained through non-experts may differ greatly from expert annotations and could exhibit worse inter-annotator agreement. As a result, in addition to the hired crowd-sourced workers, we enlisted three expert annotators who have written papers on summarization either for academic conferences (2) or as part of a senior thesis (1). The expert annotators were asked to evaluate the same set of summaries under the same instructions as the hired crowd-sourced workers. For expert judgments, we proceeded with two rounds of annotation to correct any obvious mistakes as well as to confirm judgments and ensure a higher quality of annotations. In the second round, annotators were asked to check all examples for which their score of a dimension differed from another annotator by more than 2 points and where the other annotators were within 1 point of each other. In cases where a score differed by more than 2 points for which such a pattern did not exist, all annotators examined the annotation. When re-evaluating examples, judges were allowed to see scores assigned by other expert annotators in the first round of annotations. While such a setting could undermine the wisdom of the crowd and shift the re-assigned scores towards the average judgment from the first round, we encouraged experts to remain critical and discuss contested examples when necessary. For completeness, the data collection user interface and additional details regarding the data collection process are presented in the Appendix.",
                "cite_spans": [
                    {
                        "start": 429,
                        "end": 451,
                        "text": "Gillick and Liu (2010)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "4.3"
            },
            {
                "text": "5 Metric Re-evaluation",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "4.3"
            },
            {
                "text": "Considering the concerns raised in previous work (Gillick and Liu, 2010) about the quality differences between crowd-sourced and expert annotations we study this issue using the human annotations collected as part of this work.",
                "cite_spans": [
                    {
                        "start": 49,
                        "end": 72,
                        "text": "(Gillick and Liu, 2010)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "5.1"
            },
            {
                "text": "To evaluate the inter-annotator agreement of collected crowd-sourced and expert annotations we computed the Krippendorff's alpha coefficient (Krippendorff, 2011) . We found the inter-annotator interval kappa to be below an acceptable range-0.4920 and 0.4132 for the crowd-sourced workers and the first round of expert annotations, respectively. However, the second round of expert annotations improved the inter-annotator agreement, achieving a kappa coefficient of 0.7127. For further insights, we computed standard deviations of annotator scores within the respective groups and present histograms of those statistics in Figure 1 . Plots of crowd-sourced annotations show strong similarities across all evaluated dimensions. Such an effect could be caused by an insufficient distinction made by the annotators between the 4 scored axes, where the overall quality of a summary biased scores of the individual dimensions. The histograms also show that while the second round of expert annotations lowered the standard deviation of scores and substantially increased inter-annotator agreement, relevance and coherence remained the most disagreed on dimensions between experts. This could be attributed to the subjective nature of relevance and coherence as an evaluation dimensions (Kry\u015bci\u0144ski et al., 2020) .",
                "cite_spans": [
                    {
                        "start": 141,
                        "end": 161,
                        "text": "(Krippendorff, 2011)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 1281,
                        "end": 1306,
                        "text": "(Kry\u015bci\u0144ski et al., 2020)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 630,
                        "end": 631,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "5.1"
            },
            {
                "text": "To assess the similarity of annotations between the crowd-sourced and expert annotators, we averaged the assigned scores per example within the respective annotator groups and computed Pearson's correlation coefficient. The statistic returned a value close to 0, indicating no correlation between expert and crowd-sourced judges.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "5.1"
            },
            {
                "text": "We also manually inspected the human annotations and present examples of annotated summaries, both generated and reference, as well as the differences in human judgments in Table 1a . The first row shows a well written, comprehensive summary. The high quality of the summary is reflected by top scores assigned by expert annotators, while being rated as average by crowd-sourced workers. The second row shows a summary with ambiguous pronoun usage and factual inconsistencies. The errors result in a decrease in coherence, consistency, and relevance scores in the expert annotations, but do not see a corresponding decrease in crowd-worker annotations. The third row presents a factually correct summary that contains token and phrase repetitions. The errors were caught by the expert annotators resulting in a low fluency score, while crowd-sourced annotators incorrectly classified them as issues with factual consistency. These examples again illustrate the disparities in the understanding of evaluated dimensions between judges and underscore our observation above about the uniformity of crowd-sourced annotations; the crowd-sourced annotations tend to be similar across quality dimensions even when distinctions exist, which are captured in the expert annotations.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 179,
                        "end": 181,
                        "text": "1a",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "5.1"
            },
            {
                "text": "Results presented in this section highlight the difficulties of crowd-sourcing high-quality annotations and the necessity for protocols for improving human evaluation in text summarization.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Annotations",
                "sec_num": "5.1"
            },
            {
                "text": "Crowd-worker scores (avg.) scores (avg.) (b) Reference summaries highlight issues found in theCNN/DailyMail dataset, such as click-baits and references to other articles as well as unreferenced dates and lowcoherence caused by concatenating bullet-point summaries.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Expert",
                "sec_num": null
            },
            {
                "text": "Table 1 : Example summaries with the corresponding averaged expert and crowd-sourced annotations for coherence, consistency, fluency, and relevance. Expert annotations better differentiate coherence, consistency, and fluency among the examples when compared to the crowd-sourced annotations.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Expert",
                "sec_num": null
            },
            {
                "text": "generation models. However, the field lacks a comprehensive study that would offer a consistent side-by-side comparison of their performance. We address this issue with the following experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Expert",
                "sec_num": null
            },
            {
                "text": "In Table 2 we show Kendall's tau rank correlations between automatic metrics and human judgments calculated on a system-level following Louis and Nenkova (2013) . The statistics were computed using the available expert annotations to avoid possible quality problems associated with crowd-sourced ratings, as highlighted in the previous subsection. Automatic metrics were computed 2020), and the length of model outputs was not constrained. We report correlations without differentiating between abstractive and extractive models, as most metrics did not exhibit large differences in correlation when reported separately.",
                "cite_spans": [
                    {
                        "start": 136,
                        "end": 160,
                        "text": "Louis and Nenkova (2013)",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 9,
                        "end": 10,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Expert",
                "sec_num": null
            },
            {
                "text": "Correlation results show several trends. We find that most metrics have the lowest correlation within the coherence dimension, where the correlation strength can be classified as weak or moderate. This finding follows intuition as the majority of metrics rely on hard or soft subsequence alignments, which do not measure well the interdependence between consecutive sentences. Low and moderate correlation scores were also found for the relevance dimension. As discussed in the previous subsection, such trends could result from the inherent subjectiveness of the dimension and the difficulty of collecting consistent human annotations. Model correlations increase considerably across the consistency and fluency dimensions. Although unexpected, the strong correlation with consistency could be attributed to the low abstractiveness of most neural models, which could increase the effectiveness of metrics using Table 2: Kendall's tau correlation coefficients of expert annotations computed on a system-level along four quality dimensions with automatic metrics using 11 reference summaries per example. \u02c6denotes metrics which use the source document. The five most-correlated metrics in each column are bolded.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Expert",
                "sec_num": null
            },
            {
                "text": "higher-order n-gram overlap, such as ROUGE-3 or Extractive Density. Referring back to the previous subsection, both of the mentioned dimensions achieved high inter-annotator agreement between expert judges which could also positively affect the correlation scores. Additionally, the results show a substantially higher correlation between all evaluated dimensions and ROUGE scores computed for higher-order n-grams in comparison to ROUGE-L, which corroborates with findings of Rankel et al. (2013) .",
                "cite_spans": [
                    {
                        "start": 477,
                        "end": 497,
                        "text": "Rankel et al. (2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Expert",
                "sec_num": null
            },
            {
                "text": "To examine the dependencies between different metrics, we computed Kendall's tau rank correlation coefficients, pairwise, between all metrics. Results are presented as a correlation matrix in Figure 2 . Following intuition, we observe a strong correlation between all metrics that compute, implicitly or explicitly, the lexical overlap between generated and reference summaries. Metrics measuring the n-gram novelty and repetitiveness show a weak negative correlation with all ROUGE-related metrics. Length as a feature is weakly correlated with most metrics apart from S 3 , BLANC, and SuPERT, which might suggest the mentioned metrics favor longer summaries. Worth noting is also the weak correlation of reference-less SummaQA, BLANC, and SuPERT metrics with most other evaluated metrics.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 199,
                        "end": 200,
                        "text": "2",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Expert",
                "sec_num": null
            },
            {
                "text": "Results presented in this section highlight the evaluation dimensions that are not reliably covered by currently available metrics and pave the way for future work in model evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Expert",
                "sec_num": null
            },
            {
                "text": "We now turn to an analysis of model scores across human evaluations and automatic metrics. The evaluated models were released between 2017 and 2019, represent different approaches to summarization: abstractive, extractive, and hybrid, and their architectures reflect the trends in summarization research. Although in many cases we obtained multiple variants of the same model, in the study we focus on the versions with the highest ROUGE-L scores.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Re-evaluation",
                "sec_num": "6"
            },
            {
                "text": "Table 3 contains the results of human evaluation across the four dimensions described in Section 4.3. Scores for ground truth summaries are included as a point of reference. We find that pretrained models such as Pegasus, BART, and T5 consistently performed best on most dimensions. Notably, the mentioned models scored highest on consistency and fluency while obtaining lower scores for relevance and coherence. Scores for extractive models highlight the known shortcomings of such approaches, which are lack of coherence of summaries and issues with selecting relevant content. Abstractive model ratings show an increasing trend with respect to the date of publication. This is a promising result as it suggests that the quality of models is improving with time. Worth noting is also the fact that reference summaries did not score well on consistency, coherence, and relevance. Upon examination of the annotations, we found that the reference summaries often contained extraneous information, such as hyperlinks and click-bait descriptions of other articles. As this information was not present Table 3 : Human ratings of summaries along four evaluation dimensions, averaged over three expert annotators, broken down by extractive and abstractive models. The M* codes follow the notation described in Section 3.2. The three highest-rated models in each column are in bold.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 1104,
                        "end": 1105,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Model Re-evaluation",
                "sec_num": "6"
            },
            {
                "text": "in the source documents nor relevant for the summaries, the annotators interpreted it as hallucinations and assigned lower consistency and relevance scores. Additionally, many reference summaries in the CNN/DailyMail dataset were constructed by naively concatenating bullet-point summaries into contiguous sequences. Such processing steps negatively affected the coherence of examples. Similar trends in human studies of reference summaries were reported by Stiennon et al. (2020) . Examples of noisy reference summaries are shown in Table 1b . Table 4 shows scores for model outputs across all automatic evaluation metrics. Parameters of metrics used in this study can be found in the evaluation toolkit repository listed in Section 1. The results align with insights coming from the human evaluation of models. We found that for most metrics, the highest scores were assigned to large models pretrained on vast quantities of data. However, several metrics, such as S 3 , SummaQA, SMS, CHRF, and METEOR tended to favor extractive models, assigning the highest scores to their outputs.",
                "cite_spans": [
                    {
                        "start": 458,
                        "end": 480,
                        "text": "Stiennon et al. (2020)",
                        "ref_id": "BIBREF56"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 540,
                        "end": 542,
                        "text": "1b",
                        "ref_id": null
                    },
                    {
                        "start": 551,
                        "end": 552,
                        "text": "4",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Model Re-evaluation",
                "sec_num": "6"
            },
            {
                "text": "1 The zero-shot model was used for evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Re-evaluation",
                "sec_num": "6"
            },
            {
                "text": "Presented results provide a comprehensive perspective on the current state of the field and highlight directions for future modeling work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model Re-evaluation",
                "sec_num": "6"
            },
            {
                "text": "We introduced SummEval, a set of resources for summarization model and evaluation research that include: a collection of summaries generated by recent summarization models on the CNN/DailyMail dataset, an extensible and unified toolkit for summarization model evaluation, and a diverse collection of human annotations of model outputs collected from the crowd-source and expert annotators. Using the accumulated resources we re-evaluated a broad selection of current models and evaluation metrics in a consistent and comprehensive manner. We hope that this work will prove to be a valuable resource for future research on text summarization evaluation and models. We also encourage the research community to join our efforts by contributing model outputs and extending the evaluation toolkit with new metrics. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "7"
            },
            {
                "text": "Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00373/1923949/tacl_a_00373.pdf by guest on 27 December 2024",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00373/1923949/tacl_a_00373.pdf by guest on 27 December 2024",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank all authors for sharing model outputs and Tony Wong for assistance with annotations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": "8"
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1409.0473"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Better rewards yield better summaries: Learning to summarise without references",
                "authors": [
                    {
                        "first": "Florian",
                        "middle": [],
                        "last": "B\u00f6hm",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [
                            "M"
                        ],
                        "last": "Meyer",
                        "suffix": ""
                    },
                    {
                        "first": "Ori",
                        "middle": [],
                        "last": "Shapira",
                        "suffix": ""
                    },
                    {
                        "first": "Ido",
                        "middle": [],
                        "last": "Dagan",
                        "suffix": ""
                    },
                    {
                        "first": "Iryna",
                        "middle": [],
                        "last": "Gurevych",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "3110--3120",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1307"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Florian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. 2019. Better rewards yield better summaries: Learning to summarise without references. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110-3120, Hong Kong, China. Association for Computational Linguistics. DOI: https://doi.org/10 .18653/v1/D19-1307",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "STRASS: A light and effective method for extractive summarization based on sentence embeddings",
                "authors": [
                    {
                        "first": "L\u00e9o",
                        "middle": [],
                        "last": "Bouscarrat",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bonnefoy",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Peel",
                        "suffix": ""
                    },
                    {
                        "first": "C\u00e9cile",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
                "volume": "",
                "issue": "",
                "pages": "243--252",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-2034"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "L\u00e9o Bouscarrat, Antoine Bonnefoy, Thomas Peel, and C\u00e9cile Pereira. 2019. STRASS: A light and effective method for extractive summarization based on sentence embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 243-252, Florence, Italy. Association for Computational Linguistics. DOI: https:// doi.org/10.18653/v1/P19-2034",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The price of debiasing automatic metrics in natural language evaluation",
                "authors": [
                    {
                        "first": "Arun",
                        "middle": [],
                        "last": "Chaganty",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Mussmann",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "643--653",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P18-1060"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing automatic metrics in natural language evaluation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 643-653, Melbourne, Australia. Association for Compu- tational Linguistics. DOI: https://doi .org/10.18653/v1/P18-1060",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Fast abstractive summarization with reinforceselected sentence rewriting",
                "authors": [
                    {
                        "first": "Yen-Chun",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "675--686",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P18-1063"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yen-Chun Chen and Mohit Bansal. 2018. Fast abstractive summarization with reinforce- selected sentence rewriting. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675-686, Melbourne, Australia. Association for Computational Linguistics. DOI: https://doi.org/10.18653/v1 /P18-1063",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Sentence mover's similarity: Automatic evaluation for multi-sentence texts",
                "authors": [
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Asli",
                        "middle": [],
                        "last": "Celikyilmaz",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "2748--2760",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1264"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Elizabeth Clark, Asli Celikyilmaz, and Noah A. Smith. 2019. Sentence mover's similarity: Automatic evaluation for multi-sentence texts. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2748-2760, Florence, Italy. Association for Computational Linguistics. DOI: https://doi.org/10.18653/v1 /P19-1264",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Revisiting summarization evaluation for scientific articles",
                "authors": [
                    {
                        "first": "Arman",
                        "middle": [],
                        "last": "Cohan",
                        "suffix": ""
                    },
                    {
                        "first": "Nazli",
                        "middle": [],
                        "last": "Goharian",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)",
                "volume": "",
                "issue": "",
                "pages": "806--813",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arman Cohan and Nazli Goharian. 2016. Revis- iting summarization evaluation for scientific articles. In Proceedings of the Tenth Inter- national Conference on Language Resources and Evaluation (LREC'16), pages 806-813, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Overview of DUC 2005",
                "authors": [
                    {
                        "first": "Trang",
                        "middle": [],
                        "last": "Hoa",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dang",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the document understanding conference",
                "volume": "",
                "issue": "",
                "pages": "1--12",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hoa Trang Dang. 2005. Overview of DUC 2005. In Proceedings of the document understanding conference, volume 2005, pages 1-12.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Overview of the TAC 2008 update summarization task",
                "authors": [
                    {
                        "first": "Trang",
                        "middle": [],
                        "last": "Hoa",
                        "suffix": ""
                    },
                    {
                        "first": "Karolina",
                        "middle": [],
                        "last": "Dang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Owczarzak",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hoa Trang Dang and Karolina Owczarzak. 2008. Overview of the TAC 2008 update summarization task. In TAC.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Overview of the TAC 2009 summarization track",
                "authors": [
                    {
                        "first": "Trang",
                        "middle": [],
                        "last": "Hoa",
                        "suffix": ""
                    },
                    {
                        "first": "Karolina",
                        "middle": [],
                        "last": "Dang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Owczarzak",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Text Analysis Conference",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hoa Trang Dang and Karolina Owczarzak. 2009. Overview of the TAC 2009 summarization track. In Proceedings of the Text Analysis Conference.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "A repository of corpora for summarization",
                "authors": [
                    {
                        "first": "Franck",
                        "middle": [],
                        "last": "Dernoncourt",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Ghassemi",
                        "suffix": ""
                    },
                    {
                        "first": "Walter",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franck Dernoncourt, Mohammad Ghassemi, and Walter Chang. 2018. A repository of corpora for summarization. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Deutsch",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.nlposs-1.17"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Daniel Deutsch and Dan Roth. 2020. SacreROUGE: An Open-Source Library for Using and Developing Summarization Eval- uation Metrics. DOI: https://doi.org /10.18653/v1/2020.nlposs-1.17",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Computational Nan Yang, Wenhui Wang, Furu",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Unified language model pre-training for natural language understanding and generation",
                "authors": [
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Hsiao-Wuen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Hon",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "13042--13054",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems, pages 13042-13054.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "BanditSum: Extractive summarization as a contextual bandit",
                "authors": [
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Yikang",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Crawford",
                        "suffix": ""
                    },
                    {
                        "first": "Herke",
                        "middle": [],
                        "last": "Van Hoof",
                        "suffix": ""
                    },
                    {
                        "first": "Jackie",
                        "middle": [],
                        "last": "Chi",
                        "suffix": ""
                    },
                    {
                        "first": "Kit",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "3739--3748",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1409"
                    ],
                    "PMID": [
                        "30577265"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. 2018. BanditSum: Extractive summarization as a contextual bandit. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3739-3748, Brussels, Belgium. Associa- tion for Computational Linguistics. DOI: https://doi.org/10.18653/v1/D18 -1409, PMID: 30577265",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
                "authors": [
                    {
                        "first": "Esin",
                        "middle": [],
                        "last": "Durmus",
                        "suffix": ""
                    },
                    {
                        "first": "He",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Mona",
                        "middle": [],
                        "last": "Diab",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "5055--5070",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.454"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 5055-5070, Online. Association for Computational Linguistics. DOI: https:// doi.org/10.18653/v1/2020.acl- main.454",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Rouge 2.0: Updated and improved measures for evaluation of summarization tasks",
                "authors": [
                    {
                        "first": "Kavita",
                        "middle": [],
                        "last": "Ganesan",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kavita Ganesan. 2015. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "SUPERT: towards new frontiers in unsupervised evaluation metrics for multidocument summarization",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Steffen",
                        "middle": [],
                        "last": "Eger",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "volume": "",
                "issue": "",
                "pages": "1347--1354",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.124"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yang Gao, Wei Zhao, and Steffen Eger. 2020. SUPERT: towards new frontiers in unsupervised evaluation metrics for multi- document summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 1347-1354. Association for Computational Linguistics. DOI: https://doi.org/10.18653/v1 /2020.acl-main.124",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Bottom-up abstractive summarization",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Gehrmann",
                        "suffix": ""
                    },
                    {
                        "first": "Yuntian",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "4098--4109",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1443"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natu- ral Language Processing, pages 4098-4109, Brussels, Belgium. Association for Computa- tional Linguistics. DOI: https://doi.org /10.18653/v1/D18-1443",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Nonexpert evaluation of summarization systems is risky",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Gillick",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk",
                "volume": "",
                "issue": "",
                "pages": "148--151",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Gillick and Yang Liu. 2010. Non- expert evaluation of summarization systems is risky. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk, pages 148-151, Los Angeles. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE",
                "authors": [
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "128--137",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D15-1013"
                    ],
                    "PMID": [
                        "24802104"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yvette Graham. 2015. Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In Proceedings of the 2015 Con- ference on Empirical Methods in Natural Language Processing, pages 128-137, Lisbon, Portugal. Association for Computational Lin- guistics. DOI: https://doi.org/10 .18653/v1/D15-1013, PMID: 24802104",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
                "authors": [
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Grusky",
                        "suffix": ""
                    },
                    {
                        "first": "Mor",
                        "middle": [],
                        "last": "Naaman",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "708--719",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-1065"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 708-719, New Orleans, Louisiana. Association for Computational Linguistics. DOI: https://doi.org/10.18653/v1 /N18-1065",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Soft layer-specific multi-task summarization with entailment and question generation",
                "authors": [
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Ramakanth",
                        "middle": [],
                        "last": "Pasunuru",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "687--697",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P18-1064"
                    ],
                    "PMCID": [
                        "PMC6428206"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Han Guo, Ramakanth Pasunuru, and Mohit Bansal. 2018. Soft layer-specific multi-task summarization with entailment and ques- tion generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 687-697, Melbourne, Aus- tralia. Association for Computational Linguis- tics. DOI: https://doi.org/10.18653 /v1/P18-1064, PMCID: PMC6428206",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "HighRES: Highlight-based reference-less evaluation of summarization",
                "authors": [
                    {
                        "first": "Hardy",
                        "middle": [],
                        "last": "Hardy",
                        "suffix": ""
                    },
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Vlachos",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3381--3392",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1330"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hardy Hardy, Shashi Narayan, and Andreas Vlachos. 2019. HighRES: Highlight-based reference-less evaluation of summarization. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics, pages 3381-3392, Florence, Italy. Association for Computational Linguis- tics. DOI: https://doi.org/10.18653 /v1/P19-1330",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Teaching machines to read and comprehend",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Moritz Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Kocisky",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Lasse",
                        "middle": [],
                        "last": "Espeholt",
                        "suffix": ""
                    },
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Kay",
                        "suffix": ""
                    },
                    {
                        "first": "Mustafa",
                        "middle": [],
                        "last": "Suleyman",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "1693--1701",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 1693-1701.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "A unified model for extractive and abstractive summarization using inconsistency loss",
                "authors": [
                    {
                        "first": "Wan-Ting",
                        "middle": [],
                        "last": "Hsu",
                        "suffix": ""
                    },
                    {
                        "first": "Chieh-Kai",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Ying",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kerui",
                        "middle": [],
                        "last": "Min",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "132--141",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min, Jing Tang, and Min Sun. 2018. A unified model for extractive and abstractive summarization using inconsistency loss. In Proceedings of the 56th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 132-141, Melbourne, Australia. Associ- ation for Computational Linguistics.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Closed-book training to improve summarization encoder memory",
                "authors": [
                    {
                        "first": "Yichen",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "4067--4077",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1440"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yichen Jiang and Mohit Bansal. 2018. Closed-book training to improve summa- rization encoder memory. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, pages 4067-4077, Brussels, Belgium. Association for Computational Linguistics. DOI: https://doi.org/10.18653/v1 /D18-1440",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Content selection in deep learning models of summarization",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Kedzie",
                        "suffix": ""
                    },
                    {
                        "first": "Kathleen",
                        "middle": [],
                        "last": "Mckeown",
                        "suffix": ""
                    },
                    {
                        "first": "Hal",
                        "middle": [],
                        "last": "Daum\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Iii",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1818--1828",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1208"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chris Kedzie, Kathleen McKeown, and Hal Daum\u00e9 III. 2018. Content selection in deep learning models of summarization. In Pro- ceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1818-1828, Brussels, Belgium. Asso- ciation for Computational Linguistics. DOI: https://doi.org/10.18653/v1/D18 -1208",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Computing krippendorff's alpha-reliability",
                "authors": [
                    {
                        "first": "Klaus",
                        "middle": [],
                        "last": "Krippendorff",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Klaus Krippendorff. 2011. Computing krip- pendorff's alpha-reliability. Retrieved from https://repository.upenn.edu/asc papers/43.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Neural text summarization: A critical evaluation",
                "authors": [
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Kry\u015bci\u0144ski",
                        "suffix": ""
                    },
                    {
                        "first": "Nitish",
                        "middle": [],
                        "last": "Shirish Keskar",
                        "suffix": ""
                    },
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Mccann",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "540--551",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.emnlp-main.750"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wojciech Kry\u015bci\u0144ski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural text summarization: A critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 540-551, Hong Kong, China. Association for Computa- tional Linguistics. DOI: https://doi.org /10.18653/v1/2020.emnlp-main.750",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Evaluating the factual consistency of abstractive text summarization",
                "authors": [
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Kry\u015bci\u0144ski",
                        "suffix": ""
                    },
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Mccann",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "9332--9346",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1207"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wojciech Kry\u015bci\u0144ski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computational Linguistics. DOI: https:// doi.org/10.18653/v1/D18-1207",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Improving abstraction in text summarization",
                "authors": [
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Kry\u015bci\u0144ski",
                        "suffix": ""
                    },
                    {
                        "first": "Romain",
                        "middle": [],
                        "last": "Paulus",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1808--1817",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wojciech Kry\u015bci\u0144ski, Romain Paulus, Caiming Xiong, and Richard Socher. 2018. Improv- ing abstraction in text summarization. In Pro- ceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1808-1817, Brussels, Belgium. Asso- ciation for Computational Linguistics.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "From word embeddings to document distances",
                "authors": [
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Kusner",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Kolkin",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [],
                        "last": "Weinberger",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "957--966",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document distances. In Inter- national Conference on Machine Learning, pages 957-966.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments",
                "authors": [
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    },
                    {
                        "first": "Abhaya",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the Second Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "228--231",
                "other_ids": {
                    "DOI": [
                        "10.3115/1626355.1626389"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228-231, Prague, Czech Republic. Asso- ciation for Computational Linguistics. DOI: https://doi.org/10.3115/1626355 .1626389",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Marjan",
                        "middle": [],
                        "last": "Ghazvininejad",
                        "suffix": ""
                    },
                    {
                        "first": "Abdelrahman",
                        "middle": [],
                        "last": "Mohamed",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.703"
                    ],
                    "arXiv": [
                        "arXiv:1910.13461"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461. DOI: https://doi.org /10.18653/v1/2020.acl-main.703",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Looking for a few good metrics: Automatic summarization evaluationhow many samples are enough",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin. 2004a. Looking for a few good metrics: Automatic summarization evaluation- how many samples are enough? In NTCIR.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "ROUGE: A package for automatic evaluation of summaries",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Text Summarization Branches Out",
                "volume": "",
                "issue": "",
                "pages": "74--81",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin. 2004b. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Correlation between ROUGE and human evaluation of extractive meeting summaries",
                "authors": [
                    {
                        "first": "Feifan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of ACL-08: HLT, Short Papers",
                "volume": "",
                "issue": "",
                "pages": "201--204",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Feifan Liu and Yang Liu. 2008. Correla- tion between ROUGE and human evaluation of extractive meeting summaries. In Pro- ceedings of ACL-08: HLT, Short Papers, pages 201-204, Columbus, Ohio. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Text summarization with pretrained encoders",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "3730--3740",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1387"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lang- uage Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 3730-3740, Hong Kong, China. Association for Computa- tional Linguistics. DOI: https://doi.org /10.18653/v1/D19-1387",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Automatically assessing machine summary content without a gold standard",
                "authors": [
                    {
                        "first": "Annie",
                        "middle": [],
                        "last": "Louis",
                        "suffix": ""
                    },
                    {
                        "first": "Ani",
                        "middle": [],
                        "last": "Nenkova",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Computational Linguistics",
                "volume": "39",
                "issue": "2",
                "pages": "267--300",
                "other_ids": {
                    "DOI": [
                        "10.1162/COLI_a_00123"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Annie Louis and Ani Nenkova. 2013. Auto- matically assessing machine summary content without a gold standard. Computational Lin- guistics, 39(2):267-300. DOI: https://doi .org/10.1162/COLI a 00123",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "On faithfulness and factuality in abstractive summarization",
                "authors": [
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Maynez",
                        "suffix": ""
                    },
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Bernd",
                        "middle": [],
                        "last": "Bohnet",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [
                            "T"
                        ],
                        "last": "Mcdonald",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "volume": "",
                "issue": "",
                "pages": "1906--1919",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.173"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan T. McDonald. 2020. On faithfulness and factuality in abstractive summariza- tion. In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 1906-1919. Associ- ation for Computational Linguistics. DOI: https://doi.org/10.18653/v1/2020 .acl-main.173",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Distributed representations of words and phrases and their compositionality",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "26",
                "issue": "",
                "pages": "3111--3119",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111-3119. Curran Associates, Inc.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Abstractive text summarization using sequence-to-sequence rnns and beyond",
                "authors": [
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Nallapati",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Caglar",
                        "middle": [],
                        "last": "Gulcehre",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/K16-1028"
                    ],
                    "arXiv": [
                        "arXiv:1602.06023"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang et al. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv: 1602.06023. DOI: https://doi.org/10 .18653/v1/K16-1028",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Ranking sentences for extractive summarization with reinforcement learning",
                "authors": [
                    {
                        "first": "Shashi",
                        "middle": [],
                        "last": "Narayan",
                        "suffix": ""
                    },
                    {
                        "first": "Shay",
                        "middle": [
                            "B"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "1747--1759",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-1158"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Ranking sentences for extractive summarization with reinforcement learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1747-1759, New Orleans, Louisiana. Association for Computational Linguistics. DOI: https://doi.org/10.18653/v1 /N18-1158",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Better summarization evaluation with word embeddings for ROUGE",
                "authors": [
                    {
                        "first": "Jun-Ping",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Viktoria",
                        "middle": [],
                        "last": "Abrecht",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1925--1930",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jun-Ping Ng and Viktoria Abrecht. 2015. Better summarization evaluation with word embeddings for ROUGE. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1925-1930, Lisbon, Portugal. Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Assessing the effect of inconsistent assessors on summarization evaluation",
                "authors": [
                    {
                        "first": "Karolina",
                        "middle": [],
                        "last": "Owczarzak",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "A"
                        ],
                        "last": "Rankel",
                        "suffix": ""
                    },
                    {
                        "first": "Hoa",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Trang",
                        "middle": [],
                        "last": "Dang",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "M"
                        ],
                        "last": "Conroy",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "The 50th Annual Meeting of the Association for Computational Linguistics",
                "volume": "2",
                "issue": "",
                "pages": "359--362",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karolina Owczarzak, Peter A. Rankel, Hoa Trang Dang, and John M. Conroy. 2012. Assessing the effect of inconsistent assessors on sum- marization evaluation. In The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, July 8-14, 2012, Jeju Island, Korea -Volume 2: Short Papers, pages 359-362. The Association for Computer Linguistics.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "BLEU: A method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {
                    "DOI": [
                        "10.3115/1073083.1073135"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine trans- lation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Com- putational Linguistics. DOI: https:// doi.org/10.3115/1073083.1073135",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Multi-reward reinforced summarization with saliency and entailment",
                "authors": [
                    {
                        "first": "Ramakanth",
                        "middle": [],
                        "last": "Pasunuru",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "2",
                "issue": "",
                "pages": "646--653",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-2102"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ramakanth Pasunuru and Mohit Bansal. 2018. Multi-reward reinforced summarization with saliency and entailment. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 646-653, New Orleans, Louisiana. Association for Computational Linguistics. DOI: https:// doi.org/10.18653/v1/N18-2102",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "A deep reinforced model for abstractive summarization",
                "authors": [
                    {
                        "first": "Romain",
                        "middle": [],
                        "last": "Paulus",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1705.04304"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Studying summarization evaluation metrics in the appropriate scoring range",
                "authors": [
                    {
                        "first": "Maxime",
                        "middle": [],
                        "last": "Peyrard",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "5093--5100",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1502"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maxime Peyrard. 2019. Studying summarization evaluation metrics in the appropriate scoring range. In Proceedings of the 57th Annual Meeting of the Association for Computa- tional Linguistics, pages 5093-5100, Florence, Italy. Association for Computational Linguis- tics. DOI: https://doi.org/10.18653 /v1/P19-1502",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Learning to score system summaries for better content selection evaluation",
                "authors": [
                    {
                        "first": "Maxime",
                        "middle": [],
                        "last": "Peyrard",
                        "suffix": ""
                    },
                    {
                        "first": "Teresa",
                        "middle": [],
                        "last": "Botschen",
                        "suffix": ""
                    },
                    {
                        "first": "Iryna",
                        "middle": [],
                        "last": "Gurevych",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Workshop on New Frontiers in Summarization",
                "volume": "",
                "issue": "",
                "pages": "74--84",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W17-4510"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maxime Peyrard, Teresa Botschen, and Iryna Gurevych. 2017. Learning to score system summaries for better content selection evaluation. In Proceedings of the Workshop on New Frontiers in Summarization, pages 74-84, Copenhagen, Denmark. Association for Computational Linguistics. DOI: https:// doi.org/10.18653/W17-4510",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "chrF: character n-gram F-score for automatic MT evaluation",
                "authors": [
                    {
                        "first": "Maja",
                        "middle": [],
                        "last": "Popovi\u0107",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation",
                "volume": "",
                "issue": "",
                "pages": "392--395",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W15-3049"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maja Popovi\u0107. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Pro- ceedings of the Tenth Workshop on Statisti- cal Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computa- tional Linguistics. DOI: https://doi.org /10.18653/v1/W15-3049",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Language models are unsupervised multitask learners",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "OpenAI Blog",
                "volume": "1",
                "issue": "8",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Exploring the limits of transfer learning with a unified text-to",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sharan",
                        "middle": [],
                        "last": "Narang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Matena",
                        "suffix": ""
                    },
                    {
                        "first": "Yanqi",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "A graph-theoretic summary evaluation for ROUGE",
                "authors": [
                    {
                        "first": "Elaheh",
                        "middle": [],
                        "last": "Shafieibavani",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Ebrahimi",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "Fang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "762--767",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1085"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Elaheh ShafieiBavani, Mohammad Ebrahimi, Raymond Wong, and Fang Chen. 2018. A graph-theoretic summary evaluation for ROUGE. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Lan- guage Processing, pages 762-767, Brussels, Belgium. Association for Computational Linguistics. DOI: https://doi.org/10 .18653/v1/D18-1085",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "An entity-driven framework for abstractive summarization",
                "authors": [
                    {
                        "first": "Eva",
                        "middle": [],
                        "last": "Sharma",
                        "suffix": ""
                    },
                    {
                        "first": "Luyang",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "3280--3291",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1323"
                    ],
                    "PMID": [
                        "31698456"
                    ],
                    "PMCID": [
                        "PMC6855099"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Eva Sharma, Luyang Huang, Zhe Hu, and Lu Wang. 2019. An entity-driven framework for abstractive summarization. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3280-3291, Hong Kong, China. Association for Computational Linguistics. DOI: https://doi.org/10.18653/v1 /D19-1323, PMID: 31698456, PMCID: PMC6855099",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Learning to summarize from human feedback",
                "authors": [
                    {
                        "first": "Nisan",
                        "middle": [],
                        "last": "Stiennon",
                        "suffix": ""
                    },
                    {
                        "first": "Long",
                        "middle": [],
                        "last": "Ouyang",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "M"
                        ],
                        "last": "Ziegler",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Chelsea",
                        "middle": [],
                        "last": "Voss",
                        "suffix": ""
                    },
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Christiano",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. CoRR, abs/2009.01325.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Neural Information processing Systems",
                "volume": "",
                "issue": "",
                "pages": "3104--3112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Infor- mation processing Systems, pages 3104-3112.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Fill in the BLANC: human-free quality estimation of document summaries",
                "authors": [
                    {
                        "first": "Oleg",
                        "middle": [
                            "V"
                        ],
                        "last": "Vasilyev",
                        "suffix": ""
                    },
                    {
                        "first": "Vedant",
                        "middle": [],
                        "last": "Dharnidharka",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Bohannon",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "4--5",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.eval4nlp-1.2"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Oleg V. Vasilyev, Vedant Dharnidharka, and John Bohannon. 2020. Fill in the BLANC: human-free quality estimation of document summaries. CoRR, abs/2002.09836. DOI: https://doi.org/10.18653/v1/2020 .eval4nlp-1.2",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "CIDEr: Consensus-based image description evaluation",
                "authors": [
                    {
                        "first": "C",
                        "middle": [
                            "Lawrence"
                        ],
                        "last": "Ramakrishna Vedantam",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Zitnick",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "4566--4575",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2015.7299087"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566-4575. DOI: https://doi.org/10.1109/CVPR.2015 .7299087",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "Pointer networks",
                "authors": [
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Meire",
                        "middle": [],
                        "last": "Fortunato",
                        "suffix": ""
                    },
                    {
                        "first": "Navdeep",
                        "middle": [],
                        "last": "Jaitly",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "2692--2700",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in Neural Information Processing Systems, pages 2692-2700.",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Asking and answering questions to evaluate the factual consistency of summaries",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "5008--5020",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.450"
                    ],
                    "PMCID": [
                        "PMC7367613"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 5008-5020, Online. Asso- ciation for Computational Linguistics. DOI: https://doi.org/10.18653/v1/2020 .acl-main.450, PMCID: PMC7367613",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
                "authors": [
                    {
                        "first": "Ronald",
                        "middle": [
                            "J"
                        ],
                        "last": "Williams",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Machine Learning",
                "volume": "8",
                "issue": "3-4",
                "pages": "229--256",
                "other_ids": {
                    "DOI": [
                        "10.1007/BF00992696"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ronald J. Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229-256. DOI: https://doi.org /10.1007/BF00992696",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "Learning to extract coherent summary via deep reinforcement learning",
                "authors": [
                    {
                        "first": "Yuxiang",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Baotian",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Thirty-Second AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuxiang Wu and Baotian Hu. 2018. Learning to extract coherent summary via deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence.",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "Neural extractive text summarization with syntactic compression",
                "authors": [
                    {
                        "first": "Jiacheng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Durrett",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "EMNLP-IJCNLP 2019",
                "volume": "",
                "issue": "",
                "pages": "3292--3303",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiacheng Xu and Greg Durrett. 2019. Neural extractive text summarization with syntactic compression. In EMNLP-IJCNLP 2019, pages 3292-3303, Hong Kong, China. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "On the abstractiveness of neural document summarization",
                "authors": [
                    {
                        "first": "Fangfang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jin-Ge",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "785--790",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1089"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Fangfang Zhang, Jin-ge Yao, and Rui Yan. 2018a. On the abstractiveness of neural document summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 785-790, Brussels, Belgium. Association for Compu- tational Linguistics. DOI: https://doi .org/10.18653/v1/D18-1089",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "2019a. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
                "authors": [
                    {
                        "first": "Jingqing",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yao",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Saleh",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1912.08777"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2019a. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. arXiv preprint arXiv:1912. 08777.",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "Bertscore: Evaluating text generation with BERT",
                "authors": [
                    {
                        "first": "Tianyi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Varsha",
                        "middle": [],
                        "last": "Kishore",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [
                            "Q"
                        ],
                        "last": "Weinberger",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In International Conference on Learn- ing Representations.",
                "links": null
            },
            "BIBREF69": {
                "ref_id": "b69",
                "title": "Neural latent extractive document summarization",
                "authors": [
                    {
                        "first": "Xingxing",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "779--784",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D18-1088"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xingxing Zhang, Mirella Lapata, Furu Wei, and Ming Zhou. 2018b. Neural latent extractive document summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 779-784, Brussels, Belgium. Associ- ation for Computational Linguistics. DOI: https://doi.org/10.18653/v1/D18 -1088",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "HIBERT: Document level pre-training of hierarchical bidirectional transformers for document summarization",
                "authors": [
                    {
                        "first": "Xingxing",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Lin-guistics",
                "volume": "",
                "issue": "",
                "pages": "5059--5069",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1499"
                    ],
                    "PMID": [
                        "31638247"
                    ],
                    "PMCID": [
                        "PMC6854546"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xingxing Zhang, Furu Wei, and Ming Zhou. 2019b. HIBERT: Document level pre-training of hierarchical bidirectional transformers for document summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 5059-5069, Florence, Italy. Association for Computational Linguistics. DOI: https://doi.org/10.18653/v1 /P19-1499, PMID: 31638247, PMCID: PMC6854546",
                "links": null
            },
            "BIBREF71": {
                "ref_id": "b71",
                "title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Maxime",
                        "middle": [],
                        "last": "Peyrard",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [
                            "M"
                        ],
                        "last": "Meyer",
                        "suffix": ""
                    },
                    {
                        "first": "Steffen",
                        "middle": [],
                        "last": "Eger",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "563--578",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/D19-1053"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In EMNLP- IJCNLP 2019, pages 563-578, Hong Kong, China. Association for Computational Linguis- tics. DOI: https://doi.org/10.18653 /v1/D19-1053",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "ParaEval: Using paraphrases to evaluate summaries automatically",
                "authors": [
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Dragos",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Munteanu",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference",
                "volume": "",
                "issue": "",
                "pages": "447--454",
                "other_ids": {
                    "DOI": [
                        "10.3115/1220835.1220892"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy. 2006. ParaEval: Using paraphrases to evaluate summaries automatically. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 447-454, New York City, USA. Association for Computational Linguistics. DOI: https:// doi.org/10.3115/1220835.1220892",
                "links": null
            },
            "BIBREF73": {
                "ref_id": "b73",
                "title": "Neural document summarization by jointly learning to score and select sentences",
                "authors": [
                    {
                        "first": "Qingyu",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Shaohan",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Tiejun",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Melbourne, Australia. Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "654--663",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P18-1061"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun Zhao. 2018. Neural document summarization by jointly learning to score and select sentences. In ACL 2018, pages 654-663, Melbourne, Aus- tralia. Association for Computational Linguis- tics. DOI: https://doi.org/10.18653 /v1/P18-1061",
                "links": null
            },
            "BIBREF74": {
                "ref_id": "b74",
                "title": "Fine-tuning language models from human preferences",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Daniel",
                        "suffix": ""
                    },
                    {
                        "first": "Nisan",
                        "middle": [],
                        "last": "Ziegler",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Stiennon",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [
                            "B"
                        ],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Christiano",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Irving",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1909.08593"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.",
                "links": null
            },
            "BIBREF75": {
                "ref_id": "b75",
                "title": "Appendix Data Collection The data collection interface used by both crowd-source and expert annotators is presented in Figure 3. In the annotation process, judges were first asked to carefully read the content of the source article and next proceed to evaluating the associated summaries along four axes: relevance, consistency, fluency",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "9 Appendix Data Collection The data collection interface used by both crowd-source and expert annotators is presented in Figure 3. In the annotation process, judges were first asked to carefully read the con- tent of the source article and next proceed to eval- uating the associated summaries along four axes: relevance, consistency, fluency, and coherence.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "uris": null,
                "fig_num": null,
                "text": "M10 -Bottom-Up Gehrmann et al. (2018) introduce a bottom-up approach whereby a content selection model restricts the copy attention distribution of a pretrained Pointer Generator Network during inference. M11 -Improve-abs Kry\u015bci\u0144ski et al. (2018) extend the model of Paulus et al. (2017) by augmenting the decoder with an external LSTM language model and add a novelty RL-based objective during training. M12 -Unified-ext-abs Hsu et al. (2018) propose to use the probability output of an extractive model as sentence-level attention to modify wordlevel attention scores of an abstractive model, introducing an inconsistency loss to encourage consistency between these two levels of attention. M13 -ROUGESal Pasunuru and Bansal (2018) propose a keyphrase-based salience reward as well as an entailment-based reward in addition to using a ROUGE-based reward in a REINFORCE setting, optimizing rewards simultaneously in alternate mini-batches. M14 -Multi-task (Ent + QG) Guo et al. (2018) propose question generation and entailment generation as auxiliary tasks in a multi-task framework along with a corresponding multi-task architecture. M15 -Closed book decoder Jiang and Bansal (2018) build upon a Pointer Generator Network by adding copy-less and attention-less decoder during training time to force the encoder to be more selective in encoding salient content. M16 -SENECA Sharma et al. (2019) propose to use entity-aware content selection module and an abstractive generation module to generate the final summary. M17 -T5 Raffel et al. (2019) perform a systematic study of transfer learning techniques and apply their insights to a set of tasks all framed as text-input to text-output generation tasks, including summarization. M18 -NeuralTD B\u00f6hm et al. (",
                "type_str": "figure",
                "num": null
            },
            "FIGREF1": {
                "uris": null,
                "fig_num": "1",
                "text": "Figure 1: Histogram of standard deviations of inter-annotator scores between: crowd-sourced annotations, first round expert annotations, and second round expert annotations, respectively.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF2": {
                "uris": null,
                "fig_num": "2",
                "text": "Figure 2: Pairwise Kendall's tau correlations for all automatic evaluation metrics.",
                "type_str": "figure",
                "num": null
            },
            "TABREF3": {
                "text": "Model scores from automatic evaluation metrics available in the evaluation toolkit. The five highest scores for each metric (and lowest for Length and Repeated-1/2/3) are bolded. Peter A. Rankel, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. 2013. A decade of automatic content evaluation of news summaries: Reassessing the state of the art. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 131-136, Sofia, Bulgaria. Association for Computational Linguistics.",
                "content": "<table><tr><td>Evan Sandhaus. 2008. The New York Times</td></tr><tr><td>annotated corpus. Linguistic Data Consortium,</td></tr><tr><td>Philadelphia, 6(12):e26752.</td></tr><tr><td>Thomas Scialom, Sylvain Lamprier, Benjamin</td></tr><tr><td>Piwowarski, and Jacopo Staiano. 2019.</td></tr><tr><td>Answers unite! unsupervised metrics for rein-</td></tr><tr><td>forced summarization models. In Proceed-</td></tr><tr><td>ings of the 2019 Conference on Empirical</td></tr><tr><td>Methods in Natural Language Processing</td></tr><tr><td>and the 9th International Joint Conference</td></tr><tr><td>on Natural Language Processing (EMNLP-</td></tr><tr><td>IJCNLP), pages 3246-3256, Hong Kong,</td></tr><tr><td>China. Association for Computational Linguis-</td></tr><tr><td>tics. DOI: https://doi.org/10.18653</td></tr><tr><td>/v1/D19-1320</td></tr><tr><td>Abigail See, Peter J. Liu, and Christopher D.</td></tr><tr><td>Manning. 2017. Get to the point: Summariza-</td></tr><tr><td>tion with pointer-generator networks. In Pro-</td></tr><tr><td>ceedings of the 55th Annual Meeting of</td></tr><tr><td>the Association for Computational Linguistics</td></tr><tr><td>(Volume 1: Long Papers), pages 1073-1083,</td></tr><tr><td>Vancouver, Canada. Association for Computa-</td></tr><tr><td>tional Linguistics.</td></tr></table>",
                "html": null,
                "num": null,
                "type_str": "table"
            }
        }
    }
}
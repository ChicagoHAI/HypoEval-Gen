{
    "paper_id": "2408",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2025-02-24T17:02:51.728754Z"
    },
    "title": "What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation",
    "authors": [
        {
            "first": "Dingyi",
            "middle": [],
            "last": "Yang",
            "suffix": "",
            "affiliation": {},
            "email": "yangdingyi@ruc.edu.cn"
        },
        {
            "first": "Qin",
            "middle": [],
            "last": "Jin",
            "suffix": "",
            "affiliation": {},
            "email": "qjin@ruc.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "With the development of artificial intelligence, particularly the success of Large Language Models (LLMs), the quantity and quality of automatically generated stories have significantly increased. This has led to the need for automatic story evaluation to assess the generative capabilities of computing systems and analyze the quality of both automatic-generated and human-written stories.\nEvaluating a story can be more challenging than other generation evaluation tasks. While tasks like machine translation primarily focus on assessing the aspects of fluency and accuracy, story evaluation demands complex additional measures such as overall coherence, character development, interestingness, etc. This requires a thorough review of relevant research. In this survey, we first summarize existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation challenges, identify various human criteria to measure stories, and present existing benchmark datasets. Then, we propose a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation. We also provide descriptions of these metrics, along with the discussion of their merits and limitations. Later, we discuss the human-AI collaboration for story evaluation and generation. Finally, we suggest potential future research directions, extending from story evaluation to general evaluations. CCS Concepts: \u2022 Computing methodologies \u2192 Natural language processing; Natural language generation; \u2022 General and reference \u2192 Evaluation; \u2022 Human-centered computing \u2192 Collaborative and social computing design and evaluation methods; \u2022 Information systems \u2192 Multimedia content creation.",
    "pdf_parse": {
        "paper_id": "2408",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "With the development of artificial intelligence, particularly the success of Large Language Models (LLMs), the quantity and quality of automatically generated stories have significantly increased. This has led to the need for automatic story evaluation to assess the generative capabilities of computing systems and analyze the quality of both automatic-generated and human-written stories.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "Evaluating a story can be more challenging than other generation evaluation tasks. While tasks like machine translation primarily focus on assessing the aspects of fluency and accuracy, story evaluation demands complex additional measures such as overall coherence, character development, interestingness, etc. This requires a thorough review of relevant research. In this survey, we first summarize existing storytelling tasks, including text-to-text, visual-to-text, and text-to-visual. We highlight their evaluation challenges, identify various human criteria to measure stories, and present existing benchmark datasets. Then, we propose a taxonomy to organize evaluation metrics that have been developed or can be adopted for story evaluation. We also provide descriptions of these metrics, along with the discussion of their merits and limitations. Later, we discuss the human-AI collaboration for story evaluation and generation. Finally, we suggest potential future research directions, extending from story evaluation to general evaluations. CCS Concepts: \u2022 Computing methodologies \u2192 Natural language processing; Natural language generation; \u2022 General and reference \u2192 Evaluation; \u2022 Human-centered computing \u2192 Collaborative and social computing design and evaluation methods; \u2022 Information systems \u2192 Multimedia content creation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Storytelling plays a significant role in human communication. It is widely used in our daily life for various purposes such as education, entertainment, and marketing [73, 75, 121] . Numerous studies have proposed methods for automatic storytelling, which generate textual narratives from textual [38, 62, 127] or visual inputs [69, 86, 96] . Another research direction involves animating textual stories with visual content to create a multi-modal story [12, 18, 144] .",
                "cite_spans": [
                    {
                        "start": 167,
                        "end": 171,
                        "text": "[73,",
                        "ref_id": "BIBREF72"
                    },
                    {
                        "start": 172,
                        "end": 175,
                        "text": "75,",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 176,
                        "end": 180,
                        "text": "121]",
                        "ref_id": "BIBREF120"
                    },
                    {
                        "start": 297,
                        "end": 301,
                        "text": "[38,",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 302,
                        "end": 305,
                        "text": "62,",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 306,
                        "end": 310,
                        "text": "127]",
                        "ref_id": "BIBREF126"
                    },
                    {
                        "start": 328,
                        "end": 332,
                        "text": "[69,",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 333,
                        "end": 336,
                        "text": "86,",
                        "ref_id": "BIBREF85"
                    },
                    {
                        "start": 337,
                        "end": 340,
                        "text": "96]",
                        "ref_id": "BIBREF95"
                    },
                    {
                        "start": 455,
                        "end": 459,
                        "text": "[12,",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 460,
                        "end": 463,
                        "text": "18,",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 464,
                        "end": 468,
                        "text": "144]",
                        "ref_id": "BIBREF143"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "To improve the quality of automatically generated stories and bridge the gap with human-generated ones, systematic evaluation is crucial. Since storytelling is a creative and open-ended generation task, it is more reasonable to explore metrics based on human standards, rather than only comparing results with the ground truth text [105] . This necessitates understanding different aspects that humans value, such as fluency, coherence, interestingness, etc. However, the issue of vague and inconsistent evaluation criteria definitions is a long-standing problem [64, 66, 209] that also exists in story evaluation. In this paper, we analyze various criteria in existing works of story evaluation, summarizing the commonly considered aspects for evaluating stories and their definitions. These criteria can improve the reliability and interpretability of evaluation metrics by accessing their correlation with different human standards.",
                "cite_spans": [
                    {
                        "start": 332,
                        "end": 337,
                        "text": "[105]",
                        "ref_id": "BIBREF104"
                    },
                    {
                        "start": 563,
                        "end": 567,
                        "text": "[64,",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 568,
                        "end": 571,
                        "text": "66,",
                        "ref_id": "BIBREF65"
                    },
                    {
                        "start": 572,
                        "end": 576,
                        "text": "209]",
                        "ref_id": "BIBREF208"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "Traditional lexical-based metrics such as BLEU [132] and ROUGE [103] are widely used, but they often fail to assess semantic aspects [40] and show a low correlation with human judgments [169] . More recent metrics that utilize neural embeddings or generation probabilities, such as BERTScore and [205] BARTScore [201] , perform better in terms of semantic comprehension, but they are still not particularly effective for evaluating stories. Some researchers have proposed metrics [53, 160] trained on human evaluation benchmarks, achieving better correlation with human criteria.",
                "cite_spans": [
                    {
                        "start": 47,
                        "end": 52,
                        "text": "[132]",
                        "ref_id": "BIBREF131"
                    },
                    {
                        "start": 63,
                        "end": 68,
                        "text": "[103]",
                        "ref_id": "BIBREF102"
                    },
                    {
                        "start": 133,
                        "end": 137,
                        "text": "[40]",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 186,
                        "end": 191,
                        "text": "[169]",
                        "ref_id": "BIBREF168"
                    },
                    {
                        "start": 296,
                        "end": 301,
                        "text": "[205]",
                        "ref_id": "BIBREF204"
                    },
                    {
                        "start": 312,
                        "end": 317,
                        "text": "[201]",
                        "ref_id": "BIBREF200"
                    },
                    {
                        "start": 480,
                        "end": 484,
                        "text": "[53,",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 485,
                        "end": 489,
                        "text": "160]",
                        "ref_id": "BIBREF159"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "However, these metrics are still limited by the benchmark itself and the size of the model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "Recently, the development of Large Language Models (LLMs) has led to unprecedented success in understanding and generating text, inspiring research into applying LLMs to evaluation [22, 42, 102, 136] . Compared to traditional methods, LLM-based metrics provide evaluations that are more consistent with human judgment [102] . Additionally, they can provide the reasoning process for the generated score, greatly improving the reliability and interpretability of automatic evaluation scores. This progress also fosters collaborative evaluation [97] , which can leverage both the strength of human and automatic evaluation. Despite the effectiveness of recent methods, there are still under-explored areas in story evaluation, such as personalized evaluation (particularly in terms of subjective aspects like empathy and interestingness), long story evaluation, etc. These areas still require further research and exploration.",
                "cite_spans": [
                    {
                        "start": 181,
                        "end": 185,
                        "text": "[22,",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 186,
                        "end": 189,
                        "text": "42,",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 190,
                        "end": 194,
                        "text": "102,",
                        "ref_id": "BIBREF101"
                    },
                    {
                        "start": 195,
                        "end": 199,
                        "text": "136]",
                        "ref_id": "BIBREF135"
                    },
                    {
                        "start": 318,
                        "end": 323,
                        "text": "[102]",
                        "ref_id": "BIBREF101"
                    },
                    {
                        "start": 543,
                        "end": 547,
                        "text": "[97]",
                        "ref_id": "BIBREF96"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "Our survey provides a comprehensive review of story evaluation. We aim to assist researchers in understanding the challenges and progression, as well as identifying potential future directions. The organization of this survey is summarized as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We first summarize the existing story generation tasks and datasets, including text-to-text, visual-to-text, and text-to-visual in Section 2. The evaluation considerations and challenges of various tasks are discussed.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Section 3 outlines detailed standards to evaluate stories. To address the issue of vague and inconsistent evaluation criteria, we analyze the commonly considered aspects and their definitions in Section 3.1, which differ from general NLG tasks [64, 66] . We then present the existing story evaluation benchmarks and the aspects they cover in Section 3.2.",
                "cite_spans": [
                    {
                        "start": 246,
                        "end": 250,
                        "text": "[64,",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 251,
                        "end": 254,
                        "text": "66]",
                        "ref_id": "BIBREF65"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "\u2022 From Section 4 to 7, we describe existing metrics using the taxonomy we propose, and explore their correlation with human annotators for story evaluation. Section 4 introduces a taxonomy to organize existing metrics that have been proposed or can be adopted for story evaluation. We then provide detailed descriptions of traditional metrics in Section 5 and LLM-based metrics in Section 6. In Section 7, we specifically discuss the capabilities of different metrics in evaluating stories.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "\u2022 The development of automatic generation and evaluation also encourages human-AI collaborative writing and evaluation. In Section 8, we discuss the collaborative evaluation, and the methods for measuring collaborative writing systems.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Finally, in Section 9, we recommend potential future research directions in story evaluation, which can also be extended to general domain.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "In this section, we summarize the existing story generation tasks and discuss the challenges in evaluating them. The tasks are divided into three categories: text-to-text (Section 2.1), which generates or completes a textual story based on textual inputs; visual-to-text (Section 2.1), which generates a relevant story based on visual input; text-to-visual (Section 2.1), which creates visual contents to narrate a textual story. Examples of these various tasks can be found in Table 1 , while related datasets are provided in Table 2 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 484,
                        "end": 485,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 533,
                        "end": 534,
                        "text": "2",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "STORY GENERATION TASKS",
                "sec_num": "2"
            },
            {
                "text": "Most research on story generation focuses on creating stories based on textual input. Earlier tasks solve the problem of Story Completion, attempting to complete the missing content in the story context. This requires comprehending Crong are sitting around the table . -Poby, Loopy, Pororo and Crong are clapping.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Text",
                "sec_num": "2.1"
            },
            {
                "text": "-Eddy is standing in front of the eight glasses containing different colors of fluids.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Text",
                "sec_num": "2.1"
            },
            {
                "text": "-They deliver an orphaned infant named Harry Potter to his only remaining relatives, the Dursleys.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "-Ten years later, Harry has been battling a disjointed life with the Dursleys.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "-During a family outing, he inadvertently causes an accident.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "-Soon after, he begins receiving unsolicited letters delivered by owls.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "-Finally, one day, Hagrid reappears and reveals to Harry that he is, in fact, a wizard.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "Table 1 . Example of various story generation tasks, including text-to-text ( \u00a72.1), visual-to-text ( \u00a72.2), and text-to-visual ( \u00a72.3).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "the given context, then generating missing entities [59, 62] or spans [36, 70] within that narrative, or creating a story ending based on the preceding content [127] , as shown in the first line of Table 1 .",
                "cite_spans": [
                    {
                        "start": 52,
                        "end": 56,
                        "text": "[59,",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 57,
                        "end": 60,
                        "text": "62]",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 70,
                        "end": 74,
                        "text": "[36,",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 75,
                        "end": 78,
                        "text": "70]",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 160,
                        "end": 165,
                        "text": "[127]",
                        "ref_id": "BIBREF126"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 204,
                        "end": 205,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "More complicated Story Generation tasks aime to create a whole story guided by the following textual controls:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "\u2022 Title/Topic [92, 127] refers to the basic textual input, consisting of a single or a few words that highly summarize the story. For instance, \"going on a date\", \"bank robbery\", etc.",
                "cite_spans": [
                    {
                        "start": 14,
                        "end": 18,
                        "text": "[92,",
                        "ref_id": "BIBREF91"
                    },
                    {
                        "start": 19,
                        "end": 23,
                        "text": "127]",
                        "ref_id": "BIBREF126"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "\u2022 Prompt/Premise [38, 113] refers to a more informative input, usually a sentence that captures the main content of the story, including the setting, characters, trajectory, etc. An example is shown in line 2 of Table 1 .",
                "cite_spans": [
                    {
                        "start": 17,
                        "end": 21,
                        "text": "[38,",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 22,
                        "end": 26,
                        "text": "113]",
                        "ref_id": "BIBREF112"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 218,
                        "end": 219,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "\u2022 Outline/Plot/Storyline refers to a list of keywords [197] , events [120] , or sentences [145] that outline the logical progression of a story, serving as its backbone. For long-form generation, hierarchical outline [195] with a tree-like format can improve both overall and detailed control, as the example shown in line 2 of Table 1 .",
                "cite_spans": [
                    {
                        "start": 54,
                        "end": 59,
                        "text": "[197]",
                        "ref_id": "BIBREF196"
                    },
                    {
                        "start": 69,
                        "end": 74,
                        "text": "[120]",
                        "ref_id": "BIBREF119"
                    },
                    {
                        "start": 90,
                        "end": 95,
                        "text": "[145]",
                        "ref_id": "BIBREF144"
                    },
                    {
                        "start": 217,
                        "end": 222,
                        "text": "[195]",
                        "ref_id": "BIBREF194"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 334,
                        "end": 335,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "\u2022 Other Control Signals include descriptions of the story's main elements, such as characters and scene-settings, which can influence the development of the narrative.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "Generating an entire story from a basic input such as a topic or a prompt can lead to issues of plot repetition and incoherence, particularly in longer stories [195, 196] . To address this, some research works [196, 197] have adopted a hierarchical generation process, as shown in Figure 1 . The first step, Outline Generation, involves creating a high-level structured outline. This outline can be generated sequentially [145] , hierarchically [195] , or sampled from a plot graph containing multiple possible event progressions [93] . The second step develops the outline into a detailed story. This generation process mirrors the way human authors might draft an initial storyline before writing the full story, enhancing the coherence of model-generated stories.",
                "cite_spans": [
                    {
                        "start": 160,
                        "end": 165,
                        "text": "[195,",
                        "ref_id": "BIBREF194"
                    },
                    {
                        "start": 166,
                        "end": 170,
                        "text": "196]",
                        "ref_id": "BIBREF195"
                    },
                    {
                        "start": 210,
                        "end": 215,
                        "text": "[196,",
                        "ref_id": "BIBREF195"
                    },
                    {
                        "start": 216,
                        "end": 220,
                        "text": "197]",
                        "ref_id": "BIBREF196"
                    },
                    {
                        "start": 422,
                        "end": 427,
                        "text": "[145]",
                        "ref_id": "BIBREF144"
                    },
                    {
                        "start": 445,
                        "end": 450,
                        "text": "[195]",
                        "ref_id": "BIBREF194"
                    },
                    {
                        "start": 530,
                        "end": 534,
                        "text": "[93]",
                        "ref_id": "BIBREF92"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 288,
                        "end": 289,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "To evaluate the story completion tasks, evaluation metrics are required to judge whether the generated content is fluent, aligned with commonsense, and coherent within the given context. Story generation tasks, on the other hand, are less restrictive. Therefore, the evaluation should consider additional aspects, such as input relevance and interestingness. More aspects and detailed definitions are provided in Section 3.1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Retrieval Narrating Images/Videos:",
                "sec_num": null
            },
            {
                "text": "Nowadays, tremendous images and videos can be found on sources like television, the internet, news, and in our daily lives. Generating textual descriptions for these visual resources can help people quickly grasp the main visual content and share it with others [162] . Traditional visual captioning tasks generate descriptions that are mundane and may not capture the social relations and emotions within the visual source [69] . Recent years, some works [69, 86] attend to craft more interesting and attractive stories for visual inputs, showing both academic and applicable values. For instance, when sharing photos on social media, users may tend to add automatically generated narratives that are coherent and interesting. [86] aims to generate a coherent and detailed story for an image. As the example shown in Table 1 , it produces a logical description that reflects the detailed content of the image. On the other hand, Visual storytelling [69] creates a story for an image sequence. Compared to image paragraph captioning, it requires understanding each image and its relationship to each other, constructing a story with a more complex structure and possibly some imaginative content. Video is another common type of visual content. Video storytelling [96] primarily focuses on summarizing sequential events within the video to form a coherent narrative.",
                "cite_spans": [
                    {
                        "start": 262,
                        "end": 267,
                        "text": "[162]",
                        "ref_id": "BIBREF161"
                    },
                    {
                        "start": 424,
                        "end": 428,
                        "text": "[69]",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 456,
                        "end": 460,
                        "text": "[69,",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 461,
                        "end": 464,
                        "text": "86]",
                        "ref_id": "BIBREF85"
                    },
                    {
                        "start": 728,
                        "end": 732,
                        "text": "[86]",
                        "ref_id": "BIBREF85"
                    },
                    {
                        "start": 950,
                        "end": 954,
                        "text": "[69]",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 1264,
                        "end": 1268,
                        "text": "[96]",
                        "ref_id": "BIBREF95"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 824,
                        "end": 825,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Visual-to-Text",
                "sec_num": "2.2"
            },
            {
                "text": "When evaluating a story generated based on visual content, it is crucial to assess both its textual quality and its relevance to the visual input. Specifically, the objects/entities of the story should be characterized based on the depicted visual world, allowing for reasonable imaginations. For instance, an image showing people dancing could be narrated as \"a group of people are attending a party\", however, \"they are attending a funeral\" would be unreasonable. Additionally, the story plot should be spatially and temporally grounded [56] in the visual inputs.",
                "cite_spans": [
                    {
                        "start": 539,
                        "end": 543,
                        "text": "[56]",
                        "ref_id": "BIBREF55"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Image Paragraph Captioning",
                "sec_num": null
            },
            {
                "text": "With the development of generative models [49, 84, 153] capable of generating high-quality images or videos conditioned on textual inputs, there has been an increase in explorations of Story Visualization [55, 98] . This process involves creating visual content to narrate a textual story, making it much more engaging. However, models trained for story visualization may face the challenge of being limited by the characters, backgrounds, and events in the dataset, finding it hard to generate unseen visual content. To address this, Maharana et al. [117] and Bugliarello et al. [12] propose to explore the task of Continuous Story Visualization, which generates visual scenes continuing with a prompt image or video to illustrate the textual story. These works study how the visual scene may change over time to reflect the continuing narrative. Such tasks are less limited by the training dataset, making them more closely aligned with real-world applications. Zang et al. [202] combine the tasks of visual storytelling and continuous visual storytelling.",
                "cite_spans": [
                    {
                        "start": 42,
                        "end": 46,
                        "text": "[49,",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 47,
                        "end": 50,
                        "text": "84,",
                        "ref_id": "BIBREF83"
                    },
                    {
                        "start": 51,
                        "end": 55,
                        "text": "153]",
                        "ref_id": "BIBREF152"
                    },
                    {
                        "start": 205,
                        "end": 209,
                        "text": "[55,",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 210,
                        "end": 213,
                        "text": "98]",
                        "ref_id": "BIBREF97"
                    },
                    {
                        "start": 551,
                        "end": 556,
                        "text": "[117]",
                        "ref_id": "BIBREF116"
                    },
                    {
                        "start": 580,
                        "end": 584,
                        "text": "[12]",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 976,
                        "end": 981,
                        "text": "[202]",
                        "ref_id": "BIBREF201"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Visual",
                "sec_num": "2.3"
            },
            {
                "text": "They first create a textual story based on the events depicted in the input images. Since this story might be incomplete, they forecast further story developments, which contain both textual descriptions and accompanying visual scenes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Visual",
                "sec_num": "2.3"
            },
            {
                "text": "To evaluate the visual outputs in story visualization, it is important to assess their quality and their relevance to the input text. Furthermore, they should maintain visual consistency across dynamic visual scenes. For continuous story visualization, the generated scenes, which are constrained by the visual prompt, can be directly compared to the ground truth [12] . Additional evaluation considerations are the same as those for story visualization.",
                "cite_spans": [
                    {
                        "start": 364,
                        "end": 368,
                        "text": "[12]",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Visual",
                "sec_num": "2.3"
            },
            {
                "text": "Another direction of the text-to-visual task is Story Illustration, which focuses on retrieving existing images [18, 50] or videos [111] to illustrate the textual story. Such tasks overcome the difficulty of generating high-quality visual content, however, the retrieved visuals would be limited by the available visual gallery. Thus, such tasks are usually explored in the domain of movie or TV shows, aiming to create a multi-modal storyboard for a long video.",
                "cite_spans": [
                    {
                        "start": 112,
                        "end": 116,
                        "text": "[18,",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 117,
                        "end": 120,
                        "text": "50]",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 131,
                        "end": 136,
                        "text": "[111]",
                        "ref_id": "BIBREF110"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Visual",
                "sec_num": "2.3"
            },
            {
                "text": "The evaluation of story illustration should also access the multi-modal relevance and visual consistency. Specifically, such tasks will evaluate retrieval performance by calculating Recall@k [147] , measuring the percentage of sentences in the story whose ground truth is in the top-K of retrieved results. Other common retrieval metrics [18] include median rank (MedR), mean rank (MeanR), and mean average precision (MAP). Lu et al. [111] apply the Average k-th Order Precision (AOP-k) [193] to measure how many sub-sequences are reconstructed in the retrieval visual sequence.",
                "cite_spans": [
                    {
                        "start": 191,
                        "end": 196,
                        "text": "[147]",
                        "ref_id": "BIBREF146"
                    },
                    {
                        "start": 338,
                        "end": 342,
                        "text": "[18]",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 434,
                        "end": 439,
                        "text": "[111]",
                        "ref_id": "BIBREF110"
                    },
                    {
                        "start": 487,
                        "end": 492,
                        "text": "[193]",
                        "ref_id": "BIBREF192"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Visual",
                "sec_num": "2.3"
            },
            {
                "text": "To address story evaluation, we must first identify and clearly define various aspects that human evaluation should consider. The issue of vague and inconsistent evaluation criteria is a long-standing problem and has been explored in the general evaluation domain [64, 66, 209] . Compared to other tasks like machine translation, story generation introduces a more diverse set of evaluation aspects. The definitions of the same aspects may also differ. For instance, characters can play an important role in narratives -an aspect overlooked in other tasks. This introduces the specific aspect of character development. Moreover, if characters are crucial to plot progression, they will play a significant role in the evaluation of coherence as well.",
                "cite_spans": [
                    {
                        "start": 264,
                        "end": 268,
                        "text": "[64,",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 269,
                        "end": 272,
                        "text": "66,",
                        "ref_id": "BIBREF65"
                    },
                    {
                        "start": 273,
                        "end": 277,
                        "text": "209]",
                        "ref_id": "BIBREF208"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "STORY EVALUATION CRITERIA AND BENCHMARK DATASETS",
                "sec_num": "3"
            },
            {
                "text": "In Section 3.1, we summarize the story evaluation criteria, including the considered aspects and their definitions. We then present existing story evaluation benchmarks in Section 3.2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "STORY EVALUATION CRITERIA AND BENCHMARK DATASETS",
                "sec_num": "3"
            },
            {
                "text": "We analyze various aspects proposed in existing story evaluation research [21, 52-54, 118, 179, 190] , as well as evaluation criteria considered in existing story generation works. We summarize commonly used aspects, and integrate their definitions as shown in Table 3 . These aspects are organized in a hierarchical structure, where some are sub-aspects of others. Among these commonly explored aspects, the following are more subjective and could be influenced by personal preference [179] : character development, interestingness, empathy, and surprise. For user-oriented scenarios like recommendation systems and search applications [30, 33] , personal preference should be considered in the evaluation.",
                "cite_spans": [
                    {
                        "start": 74,
                        "end": 100,
                        "text": "[21, 52-54, 118, 179, 190]",
                        "ref_id": null
                    },
                    {
                        "start": 486,
                        "end": 491,
                        "text": "[179]",
                        "ref_id": "BIBREF178"
                    },
                    {
                        "start": 637,
                        "end": 641,
                        "text": "[30,",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 642,
                        "end": 645,
                        "text": "33]",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 267,
                        "end": 268,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Story Evaluation Criteria",
                "sec_num": "3.1"
            },
            {
                "text": "Except for the commonly considered aspects in Table 3 , other factors that might be considered under specific conditions include:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 52,
                        "end": 53,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Story Evaluation Criteria",
                "sec_num": "3.1"
            },
            {
                "text": "-Style. Whether the writing style, formality, or tone remains consistent throughout the whole story [152] . This is achieved by using proper words, sentences, quotes, terms, and so on. In previous works on stylized story generation [85, 128, 139] , such \"style\" can be simplified as a specific emotion [10] or sentiment [112, 137] , where the performance is evaluated by style classification accuracy.",
                "cite_spans": [
                    {
                        "start": 100,
                        "end": 105,
                        "text": "[152]",
                        "ref_id": "BIBREF151"
                    },
                    {
                        "start": 232,
                        "end": 236,
                        "text": "[85,",
                        "ref_id": "BIBREF84"
                    },
                    {
                        "start": 237,
                        "end": 241,
                        "text": "128,",
                        "ref_id": "BIBREF127"
                    },
                    {
                        "start": 242,
                        "end": 246,
                        "text": "139]",
                        "ref_id": "BIBREF138"
                    },
                    {
                        "start": 302,
                        "end": 306,
                        "text": "[10]",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 320,
                        "end": 325,
                        "text": "[112,",
                        "ref_id": "BIBREF111"
                    },
                    {
                        "start": 326,
                        "end": 330,
                        "text": "137]",
                        "ref_id": "BIBREF136"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Evaluation Criteria",
                "sec_num": "3.1"
            },
            {
                "text": "-Controllable Accuracy. In the task of controllable storytelling, guided by elements such as scene descriptions [2] and character relationships [178] , it is necessary to evaluate the controllable accuracy, which means whether the story content correctly reflects the controllable signals.",
                "cite_spans": [
                    {
                        "start": 112,
                        "end": 115,
                        "text": "[2]",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 144,
                        "end": 149,
                        "text": "[178]",
                        "ref_id": "BIBREF177"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Evaluation Criteria",
                "sec_num": "3.1"
            },
            {
                "text": "-Toxicity. Whether the story includes some rude, unreasonable, or disrespectful components. This aspect is quite important for children stories [9] .",
                "cite_spans": [
                    {
                        "start": 144,
                        "end": 147,
                        "text": "[9]",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Evaluation Criteria",
                "sec_num": "3.1"
            },
            {
                "text": "-Naturalness/Human-like. Whether the model-generated story is likely to be written by a human [138] .",
                "cite_spans": [
                    {
                        "start": 94,
                        "end": 99,
                        "text": "[138]",
                        "ref_id": "BIBREF137"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Evaluation Criteria",
                "sec_num": "3.1"
            },
            {
                "text": "-Non-Hallucination. Whether the generated story contains unreasonable information that cannot be supported by the source input [68] . As story generation is a creative task, this aspect is randomly considered. It might be considered in visual storytelling, referring to unreasonable imagination. For instance, in the visual story shown in line 5 of Table 1 , it is reasonable to state that \"his mom was proud of him\" when observing the owner playing with her dog. However, \"his mom played frisbee with him\" would be considered a hallucination, as there are no visual elements related to a frisbee. Imagination is acceptable, but it has to be reasonable.",
                "cite_spans": [
                    {
                        "start": 127,
                        "end": 131,
                        "text": "[68]",
                        "ref_id": "BIBREF67"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 355,
                        "end": 356,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Story Evaluation Criteria",
                "sec_num": "3.1"
            },
            {
                "text": "-Visual Quality. Specifically for text-to-visual story generation tasks, it is required to measure the visual quality and visual consistency of the generated visual content, as detailed in Section 5.5.2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Evaluation Criteria",
                "sec_num": "3.1"
            },
            {
                "text": "Table 3 . Common aspects used for evaluating story quality and their definitions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Story Evaluation Criteria",
                "sec_num": "3.1"
            },
            {
                "text": "Relevance Whether the story is relevant to and reasonably reflects the source input.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aspect Definition",
                "sec_num": null
            },
            {
                "text": "Whether the stories generated by one model have many variations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Diversity",
                "sec_num": null
            },
            {
                "text": "Whether the individual sentences within a story are of high quality. They should be grammatically correct, free of typos, non-repetitive, and in line with common language usage.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fluency",
                "sec_num": null
            },
            {
                "text": "Whether the individual sentences are grammatically correct without lexical or syntax errors. Note it focuses on syntax only, not semantics.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammaticality",
                "sec_num": null
            },
            {
                "text": "Whether the individual sentences are free of redundant elements, such as repetition, over-specificity, etc.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Non-redundancy",
                "sec_num": null
            },
            {
                "text": "Whether all sentences and plots are well structured, with the context organized and connected logically.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Coherence",
                "sec_num": null
            },
            {
                "text": "Evaluating coherence usually ignores grammar or spelling errors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Coherence",
                "sec_num": null
            },
            {
                "text": "Whether the sentences in a story are formally connected. They can be connected by either referential links (co-reference, bridging anaphora) or by semantic connectors [118] .",
                "cite_spans": [
                    {
                        "start": 167,
                        "end": 172,
                        "text": "[118]",
                        "ref_id": "BIBREF117"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cohesion",
                "sec_num": null
            },
            {
                "text": "Whether the sentences are logically aligned with the preceding story.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Consistency",
                "sec_num": null
            },
            {
                "text": "Whether a story follows the same topic from beginning to end.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implicit Relevance",
                "sec_num": null
            },
            {
                "text": "Whether the story covers all its underlying concepts, theories, and historical context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Completeness",
                "sec_num": null
            },
            {
                "text": "Whether the story has a clear and rational ending.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ending",
                "sec_num": null
            },
            {
                "text": "Whether the story is clear and easy to understand, with no confusing or ambiguous elements.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Clarity",
                "sec_num": null
            },
            {
                "text": "Whether the story adheres to commonsense knowledge, such as physical entities and social interactions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Commonsense",
                "sec_num": null
            },
            {
                "text": "Informativeness/Complexity Whether the story contains rich and detailed information to support its progression and world-building.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Commonsense",
                "sec_num": null
            },
            {
                "text": "Whether the story features well-developed and engaging characters that are believable, relatable, and contribute to the overall narrative or theme.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Character Development",
                "sec_num": null
            },
            {
                "text": "Whether the story is highly enjoyable or entertaining to read, with rich details and descriptions that engage the readers' senses and imagination.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Interestingness/Engagement",
                "sec_num": null
            },
            {
                "text": "Whether the story arouses the readers' emotional experience, passion, and empathy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Empathy",
                "sec_num": null
            },
            {
                "text": "Whether the story creates suspense and surprise, especially in mystery fictions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Surprise",
                "sec_num": null
            },
            {
                "text": "We present detailed statistics of existing story evaluation benchmark datasets in Table 4 . As displayed in Figure 1 , annotators are required to evaluate a single story or compare multiple stories, with or without the source input and reference text. Some datasets measure the overall quality of the story, while others evaluate a specific aspectfoot_0 . Among these datasets, OpenMEVA and HANNA are the most cited. OpenMEVA [54] propose evaluation on overall quality, while HANNA [21] annotates evaluation scores on multiple aspects of a story. Notably, COHESENTIA [118] firstly introduces a benchmark for the vague aspect of coherence, evaluating both global and local coherence (scoring sentence by sentence). PERSE [179] firstly focuses on personalized story evaluation, considering the readers' personal preferences.",
                "cite_spans": [
                    {
                        "start": 426,
                        "end": 430,
                        "text": "[54]",
                        "ref_id": "BIBREF53"
                    },
                    {
                        "start": 482,
                        "end": 486,
                        "text": "[21]",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 567,
                        "end": 572,
                        "text": "[118]",
                        "ref_id": "BIBREF117"
                    },
                    {
                        "start": 720,
                        "end": 725,
                        "text": "[179]",
                        "ref_id": "BIBREF178"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 88,
                        "end": 89,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 115,
                        "end": 116,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Story Evaluation Benchmark Datasets",
                "sec_num": "3.2"
            },
            {
                "text": "Table 4 . The statistics of human-annotated story evaluation benchmark datasets. \"Format\" and \"Reasoning\" refers to the evaluation format (Sec. 4.2) and whether the reasoning process is annotated; \"Criteria\" denotes whether each sample considers the Overall Quality or a Single Aspect, while \"Aspects\" refers to the considered aspects; \"#Stories\" and \"#Samples\" refers to the correlated data size. For the abbreviation of each aspect, REL: relevance, FLU: fluency, COH: coherence, END: ending, CLA: clarity, COMM: commonsense, INF: informativeness, CHA: character development, INT: interestingness, ADAP: adaptability (whether a plot could guide the generation of a interesting story [179] ), EMP: empathy, SUR: surprise, STY: style.",
                "cite_spans": [
                    {
                        "start": 684,
                        "end": 689,
                        "text": "[179]",
                        "ref_id": "BIBREF178"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Story Evaluation Benchmark Datasets",
                "sec_num": "3.2"
            },
            {
                "text": "Criteria Fluency: ... Coherence: ... ...",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Story Evaluation Benchmark Datasets",
                "sec_num": "3.2"
            },
            {
                "text": "A long time ago, there lived a young prince, ...",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Source Target Story",
                "sec_num": null
            },
            {
                "text": "A young prince visited various planets, ...",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Source Target Story",
                "sec_num": null
            },
            {
                "text": "Once when I was six years old I saw a magnificent picture in a book, ...",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Source Target Story",
                "sec_num": null
            },
            {
                "text": "Fig. 1 . General Framework of Story Evaluation, which shows the evaluation inputs and output formats (Section 4.2). All the dashed boxes are optional input or output.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 5,
                        "end": 6,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Output Formats",
                "sec_num": null
            },
            {
                "text": "So far, we have discussed current story generation tasks and what characteristics make a good story. As human evaluation can be time-consuming and labor-intensive, in recent years, several automatic metrics have been proposed",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TAXONOMY OF EVALUATION METRICS",
                "sec_num": "4"
            },
            {
                "text": "or can be adopted for story evaluation. This section proposes a taxonomy to organize existing metrics, as illustrated in Figure 2 . We categorize them broadly into traditional (Section 5) and LLM-based methods (Section 6). LLM-based metrics refer to those based on models with billion-level parameters.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 128,
                        "end": 129,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "TAXONOMY OF EVALUATION METRICS",
                "sec_num": "4"
            },
            {
                "text": "Considering the task type of story generation, both text-to-text and visual-to-text tasks require textual story evaluation, measuring the quality of the generated textual story. Multi-modal evaluation is required for cross-modal generation, including metrics to measure multi-modal relevance (for both visual-to-text and text-to-visual tasks) and the quality of visual output (for text-to-visual tasks).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TAXONOMY OF EVALUATION METRICS",
                "sec_num": "4"
            },
            {
                "text": "Prompt GPT-4V to evaluate the multi-modal story [130, 203] Textual Story Evaluation Trained ( \u00a76.4)",
                "cite_spans": [
                    {
                        "start": 48,
                        "end": 53,
                        "text": "[130,",
                        "ref_id": "BIBREF129"
                    },
                    {
                        "start": 54,
                        "end": 58,
                        "text": "203]",
                        "ref_id": "BIBREF202"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Evaluation",
                "sec_num": null
            },
            {
                "text": "Reference-free: T5Score [141] ; PandaLM [186] ; Prometheus [81] ; Shepherd [183] ; Auto-J [95] ; CritiqueLLM [76] ; JudgeLM [211] ; TIGERScore [74] ; COHESENTIA [118] ; NOV_COHERENCE [119] ; PERSE [179] Reference-based: UniEval [208] ; InstructScore [194] Generativebased ( \u00a76.3)",
                "cite_spans": [
                    {
                        "start": 24,
                        "end": 29,
                        "text": "[141]",
                        "ref_id": "BIBREF140"
                    },
                    {
                        "start": 40,
                        "end": 45,
                        "text": "[186]",
                        "ref_id": "BIBREF185"
                    },
                    {
                        "start": 59,
                        "end": 63,
                        "text": "[81]",
                        "ref_id": "BIBREF80"
                    },
                    {
                        "start": 75,
                        "end": 80,
                        "text": "[183]",
                        "ref_id": "BIBREF182"
                    },
                    {
                        "start": 90,
                        "end": 94,
                        "text": "[95]",
                        "ref_id": "BIBREF94"
                    },
                    {
                        "start": 109,
                        "end": 113,
                        "text": "[76]",
                        "ref_id": "BIBREF75"
                    },
                    {
                        "start": 124,
                        "end": 129,
                        "text": "[211]",
                        "ref_id": "BIBREF210"
                    },
                    {
                        "start": 143,
                        "end": 147,
                        "text": "[74]",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 161,
                        "end": 166,
                        "text": "[118]",
                        "ref_id": "BIBREF117"
                    },
                    {
                        "start": 183,
                        "end": 188,
                        "text": "[119]",
                        "ref_id": "BIBREF118"
                    },
                    {
                        "start": 197,
                        "end": 202,
                        "text": "[179]",
                        "ref_id": "BIBREF178"
                    },
                    {
                        "start": 228,
                        "end": 233,
                        "text": "[208]",
                        "ref_id": "BIBREF207"
                    },
                    {
                        "start": 250,
                        "end": 255,
                        "text": "[194]",
                        "ref_id": "BIBREF193"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Evaluation",
                "sec_num": null
            },
            {
                "text": "Reference-based/free: PORTIA [101] ; FairEval [182] ; G-EVAL [107] ; ICE [72] ; BSM [157] ; WideDeep [206] ; COAScore [48] ; CheckEval [90] ; ChatEval [15] ; MATEval [99] ; Bai [6] ; SCALEEVAL [20] Probabilitybased ( \u00a76.2) Reference-free: DELTASCORE [192] ; Likelihood-Bias-Mitigation [131] Reference-based/free: GPTScore [41] Embeddingbased ( \u00a76.1) Reference-based: CosineSimilarity-ada-002-embedding [37] Traditional ( \u00a75)",
                "cite_spans": [
                    {
                        "start": 29,
                        "end": 34,
                        "text": "[101]",
                        "ref_id": "BIBREF100"
                    },
                    {
                        "start": 46,
                        "end": 51,
                        "text": "[182]",
                        "ref_id": "BIBREF181"
                    },
                    {
                        "start": 61,
                        "end": 66,
                        "text": "[107]",
                        "ref_id": "BIBREF106"
                    },
                    {
                        "start": 73,
                        "end": 77,
                        "text": "[72]",
                        "ref_id": "BIBREF71"
                    },
                    {
                        "start": 84,
                        "end": 89,
                        "text": "[157]",
                        "ref_id": "BIBREF156"
                    },
                    {
                        "start": 101,
                        "end": 106,
                        "text": "[206]",
                        "ref_id": "BIBREF205"
                    },
                    {
                        "start": 118,
                        "end": 122,
                        "text": "[48]",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 135,
                        "end": 139,
                        "text": "[90]",
                        "ref_id": "BIBREF89"
                    },
                    {
                        "start": 151,
                        "end": 155,
                        "text": "[15]",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 166,
                        "end": 170,
                        "text": "[99]",
                        "ref_id": "BIBREF98"
                    },
                    {
                        "start": 177,
                        "end": 180,
                        "text": "[6]",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 193,
                        "end": 197,
                        "text": "[20]",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 250,
                        "end": 255,
                        "text": "[192]",
                        "ref_id": "BIBREF191"
                    },
                    {
                        "start": 285,
                        "end": 290,
                        "text": "[131]",
                        "ref_id": "BIBREF130"
                    },
                    {
                        "start": 322,
                        "end": 326,
                        "text": "[41]",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 402,
                        "end": 406,
                        "text": "[37]",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Evaluation",
                "sec_num": null
            },
            {
                "text": "Multi-modal Evaluation Visual Output Evaluation ( \u00a75.5.1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Evaluation",
                "sec_num": null
            },
            {
                "text": "Reference-free: FID [61] ; FVD [176] ; Character Accuracy [98] ; Character F1-Score [116] ; Background Accuracy [144] ; Background F1-Score [144] ; DOVER [188] Multi-modal Relevance ( \u00a75.5.2)",
                "cite_spans": [
                    {
                        "start": 20,
                        "end": 24,
                        "text": "[61]",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 31,
                        "end": 36,
                        "text": "[176]",
                        "ref_id": "BIBREF175"
                    },
                    {
                        "start": 58,
                        "end": 62,
                        "text": "[98]",
                        "ref_id": "BIBREF97"
                    },
                    {
                        "start": 84,
                        "end": 89,
                        "text": "[116]",
                        "ref_id": "BIBREF115"
                    },
                    {
                        "start": 112,
                        "end": 117,
                        "text": "[144]",
                        "ref_id": "BIBREF143"
                    },
                    {
                        "start": 140,
                        "end": 145,
                        "text": "[144]",
                        "ref_id": "BIBREF143"
                    },
                    {
                        "start": 154,
                        "end": 159,
                        "text": "[188]",
                        "ref_id": "BIBREF187"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Evaluation",
                "sec_num": null
            },
            {
                "text": "Reference-free: R-Precision [116] ; Visual Captioning Accuracy [116] ; ClipScore [60] ; EMScore [163] ; RoViST-VG [180] ; GROOViST [171] Textual Story Evaluation Trained ( \u00a75.4) Hybrid: RUBER [174] ; RUBER-BERT [44] Reference-free: COMET-QE [149] ; COMETKiwi [150] ; MAUVE [138] ; Union [53] ; MANPLTS [43] ; NSP (Next Sentence Prediction) [13, 34] ; Ending ACC [134] Reference-based: BLEURT [160] ; COMET [149] ; COMET22 [148] Probabilitybased ( \u00a75.3)",
                "cite_spans": [
                    {
                        "start": 28,
                        "end": 33,
                        "text": "[116]",
                        "ref_id": "BIBREF115"
                    },
                    {
                        "start": 63,
                        "end": 68,
                        "text": "[116]",
                        "ref_id": "BIBREF115"
                    },
                    {
                        "start": 81,
                        "end": 85,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 96,
                        "end": 101,
                        "text": "[163]",
                        "ref_id": "BIBREF162"
                    },
                    {
                        "start": 114,
                        "end": 119,
                        "text": "[180]",
                        "ref_id": "BIBREF179"
                    },
                    {
                        "start": 131,
                        "end": 136,
                        "text": "[171]",
                        "ref_id": "BIBREF170"
                    },
                    {
                        "start": 192,
                        "end": 197,
                        "text": "[174]",
                        "ref_id": "BIBREF173"
                    },
                    {
                        "start": 211,
                        "end": 215,
                        "text": "[44]",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 241,
                        "end": 246,
                        "text": "[149]",
                        "ref_id": "BIBREF148"
                    },
                    {
                        "start": 259,
                        "end": 264,
                        "text": "[150]",
                        "ref_id": "BIBREF149"
                    },
                    {
                        "start": 273,
                        "end": 278,
                        "text": "[138]",
                        "ref_id": "BIBREF137"
                    },
                    {
                        "start": 287,
                        "end": 291,
                        "text": "[53]",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 302,
                        "end": 306,
                        "text": "[43]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 340,
                        "end": 344,
                        "text": "[13,",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 345,
                        "end": 348,
                        "text": "34]",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 362,
                        "end": 367,
                        "text": "[134]",
                        "ref_id": "BIBREF133"
                    },
                    {
                        "start": 392,
                        "end": 397,
                        "text": "[160]",
                        "ref_id": "BIBREF159"
                    },
                    {
                        "start": 406,
                        "end": 411,
                        "text": "[149]",
                        "ref_id": "BIBREF148"
                    },
                    {
                        "start": 422,
                        "end": 427,
                        "text": "[148]",
                        "ref_id": "BIBREF147"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Evaluation",
                "sec_num": null
            },
            {
                "text": "Reference-free: Perplexity [5] ; BARTScore [201] ; CTRLEval [77] Embeddingbased ( \u00a75.2) Reference-free: BERTScore-Source [13] ; TAACO 2.0 [31] Reference-based: ROUGE-WE [129] ; WMD [87] ; SMS [25] ; S+WMS [25] ; BERTScore [205] ; MoverScore [207] ; BaryScore [27] ; DepthScore [166] ; InfoLM [28] Lexical-based ( \u00a75.1) Reference-free: Distinct-n [94] ; Self-BLEU [212] ; Inter-Story Repetition [197] ; Intra-Story Repetition [197] ; Repetition Percentage [161] ; Unique Verbs [39] ; Diverse Verbs [39] ; Coverage [52] ; TAACO [32] Reference-based: BLEU [132] ; ROUGE [103] ; METEOR [8] ; CIDEr [177] ; SPICE [4] ; Backward BLEU [164] ; MS-Jaccard [126] Fig. 2 . Taxonomy of evaluation metrics proposed or can be adopted for story evaluation. The metrics that are specifically proposed for story evaluation are colored.",
                "cite_spans": [
                    {
                        "start": 27,
                        "end": 30,
                        "text": "[5]",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 43,
                        "end": 48,
                        "text": "[201]",
                        "ref_id": "BIBREF200"
                    },
                    {
                        "start": 60,
                        "end": 64,
                        "text": "[77]",
                        "ref_id": "BIBREF76"
                    },
                    {
                        "start": 121,
                        "end": 125,
                        "text": "[13]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 138,
                        "end": 142,
                        "text": "[31]",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 169,
                        "end": 174,
                        "text": "[129]",
                        "ref_id": "BIBREF128"
                    },
                    {
                        "start": 181,
                        "end": 185,
                        "text": "[87]",
                        "ref_id": "BIBREF86"
                    },
                    {
                        "start": 192,
                        "end": 196,
                        "text": "[25]",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 205,
                        "end": 209,
                        "text": "[25]",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 222,
                        "end": 227,
                        "text": "[205]",
                        "ref_id": "BIBREF204"
                    },
                    {
                        "start": 241,
                        "end": 246,
                        "text": "[207]",
                        "ref_id": "BIBREF206"
                    },
                    {
                        "start": 259,
                        "end": 263,
                        "text": "[27]",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 277,
                        "end": 282,
                        "text": "[166]",
                        "ref_id": "BIBREF165"
                    },
                    {
                        "start": 292,
                        "end": 296,
                        "text": "[28]",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 346,
                        "end": 350,
                        "text": "[94]",
                        "ref_id": "BIBREF93"
                    },
                    {
                        "start": 363,
                        "end": 368,
                        "text": "[212]",
                        "ref_id": "BIBREF211"
                    },
                    {
                        "start": 394,
                        "end": 399,
                        "text": "[197]",
                        "ref_id": "BIBREF196"
                    },
                    {
                        "start": 425,
                        "end": 430,
                        "text": "[197]",
                        "ref_id": "BIBREF196"
                    },
                    {
                        "start": 455,
                        "end": 460,
                        "text": "[161]",
                        "ref_id": "BIBREF160"
                    },
                    {
                        "start": 476,
                        "end": 480,
                        "text": "[39]",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 497,
                        "end": 501,
                        "text": "[39]",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 513,
                        "end": 517,
                        "text": "[52]",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 526,
                        "end": 530,
                        "text": "[32]",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 553,
                        "end": 558,
                        "text": "[132]",
                        "ref_id": "BIBREF131"
                    },
                    {
                        "start": 567,
                        "end": 572,
                        "text": "[103]",
                        "ref_id": "BIBREF102"
                    },
                    {
                        "start": 582,
                        "end": 585,
                        "text": "[8]",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 594,
                        "end": 599,
                        "text": "[177]",
                        "ref_id": "BIBREF176"
                    },
                    {
                        "start": 608,
                        "end": 611,
                        "text": "[4]",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 628,
                        "end": 633,
                        "text": "[164]",
                        "ref_id": "BIBREF163"
                    },
                    {
                        "start": 647,
                        "end": 652,
                        "text": "[126]",
                        "ref_id": "BIBREF125"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 658,
                        "end": 659,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Multi-modal Evaluation",
                "sec_num": null
            },
            {
                "text": "Based on the implementation methods of text evaluation metrics, we categorize them into five types, as detailed in Section 4.1. We also classify their output formats, as explained in Section 4.2. Note that most story-related generation tasks do not have a standard result, with exceptions like story entity completion and continuous story visualization.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Evaluation",
                "sec_num": null
            },
            {
                "text": "Therefore, while reference-based metrics (comparing target text to reference text) can be useful, reference-free metrics (evaluating target text based on its overall or multi-aspect quality) may be more appropriate. There are also some hybrid metrics that combine reference-free and reference-based results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Evaluation",
                "sec_num": null
            },
            {
                "text": "As discussed in our introduction, metrics that measure specific aspects of a story can enhance the reliability and interpretability of automatic evaluation compared to those assessing overall quality. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Evaluation",
                "sec_num": null
            },
            {
                "text": "According to the implementation methods, we categorize the evaluation metrics into the following types :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Methods",
                "sec_num": "4.1"
            },
            {
                "text": "-Lexical-Based. Such methods process the story as bag-of tokens or n-grams (n continuous tokens). They are usually used to measure the similarity between two stories or to assess the diversity of stories generated by one model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Methods",
                "sec_num": "4.1"
            },
            {
                "text": "-Embedding-Based. As shown in the left part of Figure 3 (a), some embedding-based approaches use pre-trained models to encode the source input, target story, and its reference as multiple embeddings, then perform further processing.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 54,
                        "end": 55,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation Methods",
                "sec_num": "4.1"
            },
            {
                "text": "Specifically, the matching-based model measures the equivalence between the target and reference vector, while the regressor or classifier achieves the result with or without the reference. As displayed in the right part of Figure 3 (a), other embedding-based methods encode all inputs jointly to obtain a vector for further calculation.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 231,
                        "end": 232,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation Methods",
                "sec_num": "4.1"
            },
            {
                "text": "-Probability-Based. These types of methods calculate the evaluation score based on the generation probability of the target text through generative models, as illustrated in Figure 3 (b) .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 181,
                        "end": 186,
                        "text": "3 (b)",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation Methods",
                "sec_num": "4.1"
            },
            {
                "text": "-Generative-Based. These methods can also be referred to as prompt-based methods. They provide humans or generative models (Figure 3 (c)) with an evaluation prompt and collect the generated results. As an example shown in Figure 1 , the input prompt provides the instructions, including the task instruction and specific criteria. It also provides information about the evaluation sample, including the source input, target story, and reference story.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 131,
                        "end": 132,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 229,
                        "end": 230,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation Methods",
                "sec_num": "4.1"
            },
            {
                "text": "-Trained. These methods might employ any of the three model types shown in Figure 3 , with further training on evaluation benchmark datasets to improve the evaluating abilities. Some approaches also incorporate training on specially designed tasks using the story generation dataset to improve text comprehension. In the \"Method\" column of Table 5 , we present the model types of the trained metrics, i.e., embedding-based (trained), probability-based (trained), and generative-based (trained).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 82,
                        "end": 83,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 346,
                        "end": 347,
                        "text": "5",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation Methods",
                "sec_num": "4.1"
            },
            {
                "text": "The output format types of different metrics can be categorized as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Output Format",
                "sec_num": "4.2"
            },
            {
                "text": "-Score. Evaluate a discrete score, usually on a scale of 0-1 or 0-100, such as BLEU, ROUGE, etc.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Output Format",
                "sec_num": "4.2"
            },
            {
                "text": "-Likert Scale [151] . Rate on an integer scale, usually from 1-5, with 1 being the lowest scale and 5 the highest. ",
                "cite_spans": [
                    {
                        "start": 14,
                        "end": 19,
                        "text": "[151]",
                        "ref_id": "BIBREF150"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Output Format",
                "sec_num": "4.2"
            },
            {
                "text": "Embedding-based (trained) Boolean MANPLTS [43] \u2713 Embedding-based (trained) Boolean BERTScore-Source [13] \u2713 Embedding-based Score Perplexity [5] \u2713 \u2713",
                "cite_spans": [
                    {
                        "start": 42,
                        "end": 46,
                        "text": "[43]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 100,
                        "end": 104,
                        "text": "[13]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 140,
                        "end": 143,
                        "text": "[5]",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2713 \u2713 \u2713",
                "sec_num": null
            },
            {
                "text": "Probability-based Score BARTScore [201] \u2713 \u2713 \u2713",
                "cite_spans": [
                    {
                        "start": 34,
                        "end": 39,
                        "text": "[201]",
                        "ref_id": "BIBREF200"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2713 \u2713 \u2713",
                "sec_num": null
            },
            {
                "text": "Probability-based Score CTRLEval [77] \u2713 \u2713 -Comparison. Choose the better of two stories. Previous works [109] have proposed that it is easier and more robust to compare two outputs than to score them independently.",
                "cite_spans": [
                    {
                        "start": 33,
                        "end": 37,
                        "text": "[77]",
                        "ref_id": "BIBREF76"
                    },
                    {
                        "start": 104,
                        "end": 109,
                        "text": "[109]",
                        "ref_id": "BIBREF108"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2713 \u2713 \u2713",
                "sec_num": null
            },
            {
                "text": "Probability-based Score UniEval [208] \u2713 \u2713 \u2713 Generative-based (trained) Score GPTScore [41] \u2713 \u2713 \u2713 \u2713 \u2713 Probability-based Score COHESENTIA [118] \u2713 Generative-based (trained) Likert NOV_COHERENCE [119] \u2713 Generative-based (trained) Likert PERSE [118] \u2713 \u2713 \u2713 \u2713 Generative-based (trained) Likert G-EVAL [107] \u2713 * \u2713 \u2713 * *",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2713 \u2713 \u2713",
                "sec_num": null
            },
            {
                "text": "-Ranking. For comparison, only two stories are involved, while for ranking, the order of N (N\u22652) samples needs to be decided. Specifically, we can get the ranking list through N(N-1) pairwise comparisons or calculate a win-loss ratio within selected comparisons to order the candidates [109] .",
                "cite_spans": [
                    {
                        "start": 286,
                        "end": 291,
                        "text": "[109]",
                        "ref_id": "BIBREF108"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2713 \u2713 \u2713",
                "sec_num": null
            },
            {
                "text": "-Error Analysis. To ensure the trustworthiness and reliability of evaluation results, some studies [192] provide a detailed evaluation process for each score, including specific error locations and explanationsfoot_1 . For instance, TIGERScore [74] assigns a penalty score to each identified error and then calculates the discrete score. Alternatively, as suggested in Hu et al. [67] , one could simply count the number of errors to categorize the score from low to high scale. For aspects such as commonsense, we can calculate the error percentage as done in Min et al. [124] .",
                "cite_spans": [
                    {
                        "start": 99,
                        "end": 104,
                        "text": "[192]",
                        "ref_id": "BIBREF191"
                    },
                    {
                        "start": 244,
                        "end": 248,
                        "text": "[74]",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 379,
                        "end": 383,
                        "text": "[67]",
                        "ref_id": "BIBREF66"
                    },
                    {
                        "start": 571,
                        "end": 576,
                        "text": "[124]",
                        "ref_id": "BIBREF123"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2713 \u2713 \u2713",
                "sec_num": null
            },
            {
                "text": "In this section, we discuss traditional evaluation metrics. From Sections 5.1 to 5.4, we analyze metrics for evaluating textual stories, categorized by their method types (Section 4.1). In Section 5.5, we delve into metrics for evaluating multi-modal relevance and the quality of visual output.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "TRADITIONAL EVALUATION",
                "sec_num": "5"
            },
            {
                "text": "As mentioned in Section 4.1, lexical-based metrics process the story as a bag of tokens or n-grams. The most popular metrics such as BLEU [132] , METEOR [8] , ROUGE [103] , and CIDEr [177] focus on the lexical overlap between candidates and the reference story. These methods can assess the Overall Quality of the generated results.",
                "cite_spans": [
                    {
                        "start": 138,
                        "end": 143,
                        "text": "[132]",
                        "ref_id": "BIBREF131"
                    },
                    {
                        "start": 153,
                        "end": 156,
                        "text": "[8]",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 165,
                        "end": 170,
                        "text": "[103]",
                        "ref_id": "BIBREF102"
                    },
                    {
                        "start": 183,
                        "end": 188,
                        "text": "[177]",
                        "ref_id": "BIBREF176"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lexical-based Metrics",
                "sec_num": "5.1"
            },
            {
                "text": "Some metrics, on the other hand, evaluate a specific aspect of a story: (1) Fluency can be measured by calculating intra-story repetition. The Repetition Percentage metric [161] computes the percentage of tokens in the generated story that repeat at least one 4-gram; the Intra-story Repetition metric [197] measures the repetition within each generated story through 3-gram word overlaps. ( 2) TAACO [32] is a freely available text analysis tool for evaluating Coherence. It There are also lexical-based metrics focusing on the Diversity of stories generated by a model: (1) To access General Diversity, for reference-based metrics, the Backward BLEU metric [164] measures diversity by applying generated texts as reference, evaluating each test set text with a BLEU score. The MS-Jaccard metric [126] assesses both quality and diversity, calculating n-gram overlap between generated and referenced stories using the Jaccard index. For reference-free methods, the Distinct-n metric [94] computes the average ratio of distinct n-grams to total n-grams.",
                "cite_spans": [
                    {
                        "start": 172,
                        "end": 177,
                        "text": "[161]",
                        "ref_id": "BIBREF160"
                    },
                    {
                        "start": 302,
                        "end": 307,
                        "text": "[197]",
                        "ref_id": "BIBREF196"
                    },
                    {
                        "start": 401,
                        "end": 405,
                        "text": "[32]",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 659,
                        "end": 664,
                        "text": "[164]",
                        "ref_id": "BIBREF163"
                    },
                    {
                        "start": 797,
                        "end": 802,
                        "text": "[126]",
                        "ref_id": "BIBREF125"
                    },
                    {
                        "start": 983,
                        "end": 987,
                        "text": "[94]",
                        "ref_id": "BIBREF93"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lexical-based Metrics",
                "sec_num": "5.1"
            },
            {
                "text": "The Self-BLEU metric [212] computes the average BLEU score of each generated story using all generated stories as reference. The Inter-Story Repetition metric [45] examines 3-gram repetition across stories. (2) Specifically, Fan et al.",
                "cite_spans": [
                    {
                        "start": 21,
                        "end": 26,
                        "text": "[212]",
                        "ref_id": "BIBREF211"
                    },
                    {
                        "start": 159,
                        "end": 163,
                        "text": "[45]",
                        "ref_id": "BIBREF44"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lexical-based Metrics",
                "sec_num": "5.1"
            },
            {
                "text": "[39] and Goldfarb-Tarrant et al. [45] access Event Diversity, proposing Unique Verbs and Diverse Verbs metrics that calculate the number and percentage of unique verbs that are not one of the top 5 most frequent verbs in the story.",
                "cite_spans": [
                    {
                        "start": 33,
                        "end": 37,
                        "text": "[45]",
                        "ref_id": "BIBREF44"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lexical-based Metrics",
                "sec_num": "5.1"
            },
            {
                "text": "Although lexical-based metrics are widely used, they demonstrate a low correlation with human annotations [170] and show poor semantic comprehension. Embedding-based metrics [25, 27, 28, 87, 129, 166, 205, 207] , which evaluate based on embeddings from strong pre-trained models, can perform much better in terms of semantic comprehension. In this section, we discuss the metrics that directly use the pre-trained models, without further fine-tuning. Such methods evaluate the equivalence between the embeddings of the target text and references using a matching algorithm, as shown in the model structure on the left side of Figure 3 (a) . Generally, the key point is to propose better-designed matching algorithms based on stronger pre-trained models.",
                "cite_spans": [
                    {
                        "start": 106,
                        "end": 111,
                        "text": "[170]",
                        "ref_id": "BIBREF169"
                    },
                    {
                        "start": 174,
                        "end": 178,
                        "text": "[25,",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 179,
                        "end": 182,
                        "text": "27,",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 183,
                        "end": 186,
                        "text": "28,",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 187,
                        "end": 190,
                        "text": "87,",
                        "ref_id": "BIBREF86"
                    },
                    {
                        "start": 191,
                        "end": 195,
                        "text": "129,",
                        "ref_id": "BIBREF128"
                    },
                    {
                        "start": 196,
                        "end": 200,
                        "text": "166,",
                        "ref_id": "BIBREF165"
                    },
                    {
                        "start": 201,
                        "end": 205,
                        "text": "205,",
                        "ref_id": "BIBREF204"
                    },
                    {
                        "start": 206,
                        "end": 210,
                        "text": "207]",
                        "ref_id": "BIBREF206"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 633,
                        "end": 638,
                        "text": "3 (a)",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Embedding-based Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "Earlier approaches apply the pre-trained word2vec model [123] . ROUGE-WE [129] , the variant of ROUGE, replaces the hard lexical matching in the ROUGE function with soft matching based on the cosine similarity of word2vec embeddings.",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 61,
                        "text": "[123]",
                        "ref_id": "BIBREF122"
                    },
                    {
                        "start": 73,
                        "end": 78,
                        "text": "[129]",
                        "ref_id": "BIBREF128"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding-based Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "The matching algorithm of Word Mover's Distance (WMD) [87] uses the concept of Earth Mover's Distance [155] .",
                "cite_spans": [
                    {
                        "start": 54,
                        "end": 58,
                        "text": "[87]",
                        "ref_id": "BIBREF86"
                    },
                    {
                        "start": 102,
                        "end": 107,
                        "text": "[155]",
                        "ref_id": "BIBREF154"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding-based Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "It calculates the minimum cumulative distance that words in the generated text must travel to reach words in the reference text within the word2vec embedding space. However, WMD's reliance on word embeddings limits its ability to capture whole-sentence semantics. To address this, Clark et al. [25] introduce Sentence Mover's Similarity (SMS), which uses sentence embeddings. They also introduce Sentence and Word Mover's Similarity (S+WMS), which uses both word and sentence embeddings. SMS and S+WMS that incorporate sentence-level semantic comprehension would be more appropriate for story evaluation.",
                "cite_spans": [
                    {
                        "start": 294,
                        "end": 298,
                        "text": "[25]",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding-based Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "More recent methods apply stronger pre-trained models such as BERT [34] . BERTScore [205] , a popular embeddingbased method using contextual BERT embeddings, matches tokens in generated and reference texts via cosine similarity.",
                "cite_spans": [
                    {
                        "start": 67,
                        "end": 71,
                        "text": "[34]",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 84,
                        "end": 89,
                        "text": "[205]",
                        "ref_id": "BIBREF204"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding-based Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "It can be adapted to assess input relevance by averaging the BERTScore between the source input and each story sentence, termed BERTScore-Source [13] . Unlike BERTScore, which applies hard alignments (one-to-one) for words in the generated text and one reference, MoverScore [207] provides soft alignments (many-to-one) between the generated result and multiple references. It employs a matching algorithm similar to WMD, leveraging Earth Mover Distance to compute the semantic distance. As the experiments conducted in Chhun et al. [21] , MoverScore shows slightly better performance in story evaluation than BERTScore.",
                "cite_spans": [
                    {
                        "start": 145,
                        "end": 149,
                        "text": "[13]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 275,
                        "end": 280,
                        "text": "[207]",
                        "ref_id": "BIBREF206"
                    },
                    {
                        "start": 533,
                        "end": 537,
                        "text": "[21]",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding-based Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "The previous metrics evaluate based on encoded vectors. Some metrics [27, 28, 166] , however, treat the output of pretrained models as probability distributions rather than vector embeddings. They then calculate semantic similarity through these probability distributions. In our taxonomy, we categorize these methods as special embedding-based methods. BaryScore [27] models the output of multiple layers as a probability distribution, aggregates layer information using the Wasserstein barycenter, and computes similarity using WMD's matching function. DepthScore [166] obtains discrete probability distributions of two texts and calculates their similarity by extending univariate quantiles to multivariate spaces. InfoLM [28] masks each token to derive the discrete probability distribution for each text. It aggregates these distributions using a weighted sum and then calculates the similarity. These metrics [27, 28, 166] show better performance in tasks like Summarization (focusing on faithfulness), but no clear advantage in story evaluation [21] . Possibly because they do not capture higher-level semantic comprehension.",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 73,
                        "text": "[27,",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 74,
                        "end": 77,
                        "text": "28,",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 78,
                        "end": 82,
                        "text": "166]",
                        "ref_id": "BIBREF165"
                    },
                    {
                        "start": 364,
                        "end": 368,
                        "text": "[27]",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 566,
                        "end": 571,
                        "text": "[166]",
                        "ref_id": "BIBREF165"
                    },
                    {
                        "start": 725,
                        "end": 729,
                        "text": "[28]",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 915,
                        "end": 919,
                        "text": "[27,",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 920,
                        "end": 923,
                        "text": "28,",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 924,
                        "end": 928,
                        "text": "166]",
                        "ref_id": "BIBREF165"
                    },
                    {
                        "start": 1052,
                        "end": 1056,
                        "text": "[21]",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding-based Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "TAACO 2.0 [31] is an upgrade from TAACO 1.0 [32] , measuring the aspect of coherence. It introduces several new indices tied to local and global cohesion based on semantic embeddings. It also supports reference-based indices that calculate lexical and semantic overlap between a candidate text and a reference text.",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 14,
                        "text": "[31]",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 44,
                        "end": 48,
                        "text": "[32]",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding-based Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "Probability-based methods [5, 77, 201] count the evaluation score using the generation probability of a story based on pre-trained models, as shown in Figure 3 (b). Such methods are motivated by the idea that better stories will have a high generation probability.",
                "cite_spans": [
                    {
                        "start": 26,
                        "end": 29,
                        "text": "[5,",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 30,
                        "end": 33,
                        "text": "77,",
                        "ref_id": "BIBREF76"
                    },
                    {
                        "start": 34,
                        "end": 38,
                        "text": "201]",
                        "ref_id": "BIBREF200"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 158,
                        "end": 159,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Probability-based Metrics",
                "sec_num": "5.3"
            },
            {
                "text": "Perplexity [5] might be the most widely used probability-based metric, which calculates the negative log-likelihood of a text sequence generated by a language model. A lower perplexity score indicates better quality. Existing works usually apply a powerful pre-trained generative model like GPT-2 [143] , or further fine-tune it on the story generation training set (Equation 1).",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 14,
                        "text": "[5]",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 297,
                        "end": 302,
                        "text": "[143]",
                        "ref_id": "BIBREF142"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probability-based Metrics",
                "sec_num": "5.3"
            },
            {
                "text": "With the proposal of models like BART [91] , which excel in both text generation and comprehension tasks, it's possible to access specific evaluation aspects using probability-based metrics. BARTScore [201] calculates the per-token probability of the BART model. By altering the source and the generated text, it can assess various aspects. For example, it can measure relevance and text quality by calculating the generation probability of the story based on the input premise. CTRLEval [77] designs more elaborate text infilling tasks for different evaluation aspects: sentence cohesion (coherence), consistency, and relevance, achieving better performance than BARTScore.",
                "cite_spans": [
                    {
                        "start": 38,
                        "end": 42,
                        "text": "[91]",
                        "ref_id": "BIBREF90"
                    },
                    {
                        "start": 201,
                        "end": 206,
                        "text": "[201]",
                        "ref_id": "BIBREF200"
                    },
                    {
                        "start": 488,
                        "end": 492,
                        "text": "[77]",
                        "ref_id": "BIBREF76"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probability-based Metrics",
                "sec_num": "5.3"
            },
            {
                "text": "As mentioned in Section 4.1, such metrics require training on the evaluation benchmark to enhance evaluating quality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Trained Metrics",
                "sec_num": "5.4"
            },
            {
                "text": "Some metrics also train on the text generation dataset to improve the model's comprehension ability. In this Section, we first present the commonly used training objects in Section 5.4.1, and then discuss the trained metrics in Section 5.4.2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Trained Metrics",
                "sec_num": "5.4"
            },
            {
                "text": "Objects. Given a textual story \ud835\udc66 constructed with \ud835\udc5b sequential sentences {\ud835\udc60 \ud835\udc56 } \ud835\udc5b \ud835\udc56 and k tokens {\ud835\udc64 \ud835\udc56 } \ud835\udc58 \ud835\udc56 , our goal is to obtain a evaluation result S achieved by function \ud835\udc53 (\ud835\udc66). For evaluations based on source input \ud835\udc65 and reference \ud835\udc5f , S is obtained by \ud835\udc53 (\ud835\udc66, \ud835\udc65, \ud835\udc5f ). In the following discussion, we present the loss function for the basic condition.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "For trained metrics, the common training tasks and correlated objectives are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "Story Generation. For probability-based and generative-based methods, the generative model can be trained on the story generation dataset to improve the high-quality text's generation probability. The corresponding negative log-likelihood loss is as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L \ud835\udc54\ud835\udc52\ud835\udc5b = - 1 \ud835\udc58 \ud835\udc58 \u2211\ufe01 \ud835\udc61 =1 \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc5d (\ud835\udc66 \ud835\udc61 |\ud835\udc66 < \ud835\udc61; \ud835\udf03 ),",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "where \ud835\udf03 are the parameters of the generative model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "Discriminative/Contrastive Learning. Under the evaluation target that high-quality text should receive higher scores than low-quality ones, some methods construct contrastive story pairs (\ud835\udc66 + , \ud835\udc66 -). Given an evaluation dataset, \ud835\udc66 + is the text with a higher score than the negative sample \ud835\udc66 -. Without evaluation datasets, \ud835\udc66 + is the ground truth output, and \ud835\udc66 -is either randomly selected or carefully crafted. The model is then trained using the discriminative loss:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L \ud835\udc51\ud835\udc56\ud835\udc60 = \ud835\udc5a\ud835\udc4e\ud835\udc65 (0, \ud835\udeff -\ud835\udc53 (\ud835\udc66 + ) + \ud835\udc53 (\ud835\udc66 -)),",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "where \ud835\udeff is a threshold parameter. It suggests that the score of a positive sample should exceed the score of a negative sample by at least a margin of \ud835\udeff.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "Evaluation Result Classification. If the evaluation result is formatted as a Likert-type scale or boolean output, a classifier model can be trained to predict appropriate classes, usually trained with the classification loss:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L \ud835\udc50\ud835\udc59\ud835\udc60 = -S \ud835\udc50 \u2022 \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc53 (\ud835\udc66),",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "where S \ud835\udc50 denotes a one-hot encoded vector representing the ground truth class, and \ud835\udc53 (\ud835\udc66) outputs the vector of predicted probabilities for each class.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "Evaluation Result Generation. As shown in Figure 3 , for methods that directly generate a numeric score with the regressor (like BLEURT, using an MLP layer for score generation) can be trained using regression loss as follows:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 49,
                        "end": 50,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L \ud835\udc5f\ud835\udc52\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc60 = ||S -\ud835\udc53 (\ud835\udc66)|| 2 , (",
                        "eq_num": "4"
                    }
                ],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "where S is the ground truth score in the evaluation dataset, and \ud835\udc53 (\ud835\udc66) yields the predicted score.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "On the other hand, for generative-based methods shown in Figure 3 (c), all evaluation output formats can be converted into text. For example, the Likert-type format can be expressed as integer text, and the boolean format can be represented as \"yes\" or \"no\". So, all evaluation problems can be turned into text generation problems and trained using cross-entropy loss as follows:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 64,
                        "end": 65,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L \ud835\udc36\ud835\udc38 = - 1 \ud835\udc59 \ud835\udc59 \u2211\ufe01 \ud835\udc61 =1 \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc5d (ST \ud835\udc61 |ST < \ud835\udc61, \ud835\udc66; \ud835\udf03 ),",
                        "eq_num": "(5)"
                    }
                ],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "where ST refers to the text converted from the ground truth evaluation result, containing \ud835\udc59 words, and \ud835\udf03 represents the parameters of the generative model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "Sentence Reordering. This is a self-supervised task proposed by Lapata [88] , which means to reorder shuffled sentences to their original sequence. This task can be applied to access the coherence of a story [119] . There are typically two task settings: (1) Classification-based [140] , which takes a pair of sentences <\ud835\udc60 \ud835\udc56 , \ud835\udc60 \ud835\udc57 >(\ud835\udc56 < \ud835\udc57) and determines whether \ud835\udc60 \ud835\udc56 comes before \ud835\udc60 \ud835\udc57 . This is trained using the classification loss. ( 2) Generation-based [110] , which involves all the sentences and predicts their appropriate order. This is trained using the generation loss.",
                "cite_spans": [
                    {
                        "start": 71,
                        "end": 75,
                        "text": "[88]",
                        "ref_id": "BIBREF87"
                    },
                    {
                        "start": 208,
                        "end": 213,
                        "text": "[119]",
                        "ref_id": "BIBREF118"
                    },
                    {
                        "start": 280,
                        "end": 285,
                        "text": "[140]",
                        "ref_id": "BIBREF139"
                    },
                    {
                        "start": 455,
                        "end": 460,
                        "text": "[110]",
                        "ref_id": "BIBREF109"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "Instruction Tuning [204] . This training object is usually applied by LLM-based models (Section 6.4), which finetune the LLMs with a large-scale instruction dataset for evaluation. The loss function is similar to Equation 5, with instructions as additional input.",
                "cite_spans": [
                    {
                        "start": 19,
                        "end": 24,
                        "text": "[204]",
                        "ref_id": "BIBREF203"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "5.4.1"
            },
            {
                "text": "Metrics. This subsection discusses the trained metrics proposed or can be adopted for story evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Trained",
                "sec_num": "5.4.2"
            },
            {
                "text": "RUBER [174] is a hybrid metric. Its reference-free part RUBER \ud835\udc62 uses embeddings of the source and target text, and trains the model using contrastive loss (Equation 2). The negative sample is randomly selected from the dataset. RUBER-BERT [44] extends the RUBER score by employing BERT contextual embeddings, resulting in improved correlation with human evaluations. BLEURT [160] aligns more closely with human annotators due to its training setup. It is first pre-trained on large-scale synthetic data, using various automatic metrics (such as BLEU and BERTScore) as supervision signals. It is then fine-tuned on human rating scores. The loss function is shown in Equation 4. The model structure is depicted on the left side of Figure 3 , predicting a score based on the BERT embeddings of the target and reference texts.",
                "cite_spans": [
                    {
                        "start": 6,
                        "end": 11,
                        "text": "[174]",
                        "ref_id": "BIBREF173"
                    },
                    {
                        "start": 239,
                        "end": 243,
                        "text": "[44]",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 374,
                        "end": 379,
                        "text": "[160]",
                        "ref_id": "BIBREF159"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 736,
                        "end": 737,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Trained",
                "sec_num": "5.4.2"
            },
            {
                "text": "The series of COMET scores are built upon the stronger XLM-RoBERTa model [29] . Reference-based COMET [149] combines the embeddings of the target and reference text into one single vector which is then passed to a regression model. Reference-free COMET-QE evaluates quality using only the source and target text as input. Both COMET and COMET-QE are trained on human evaluation data using the regression loss shown in Equation 4. COMETKiwi [150] utilizes the same training method, while adapting the predictor-estimator architecture of OPENKIWI [79] (as shown in the right side of Figure 3 (a)). COMET22 [148] incorporates additional pre-training data and larger encoder models, resulting in enhanced performance. Although the series of COMET scores are proposed for the machine translation task, they also perform well on reference-based story evaluation [74] .",
                "cite_spans": [
                    {
                        "start": 73,
                        "end": 77,
                        "text": "[29]",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 102,
                        "end": 107,
                        "text": "[149]",
                        "ref_id": "BIBREF148"
                    },
                    {
                        "start": 440,
                        "end": 445,
                        "text": "[150]",
                        "ref_id": "BIBREF149"
                    },
                    {
                        "start": 545,
                        "end": 549,
                        "text": "[79]",
                        "ref_id": "BIBREF78"
                    },
                    {
                        "start": 604,
                        "end": 609,
                        "text": "[148]",
                        "ref_id": "BIBREF147"
                    },
                    {
                        "start": 856,
                        "end": 860,
                        "text": "[74]",
                        "ref_id": "BIBREF73"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 588,
                        "end": 589,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Trained",
                "sec_num": "5.4.2"
            },
            {
                "text": "UNION and MANPLTS are specifically proposed for story evaluation. UNION [53] trains a model to distinguish human-written stories from negative samples. These samples, focusing on fluency, coherence, and commonsense, are created using four techniques: lexical and sentence-level repetition, random keyword and sentence substitution, sentence reordering, and negation alteration. On the other hand, MANPLTS [43] trains a classification model primarily focused on coherence. They create negative samples by introducing plot-level incoherence sources. The plot manipulations include logical reordering, contradiction insertion, repetition insertion, and random sentence substitution. These manipulated plots are then fed into generation models to produce implausible stories as negative samples. Maimon and Tsarfaty [119] propose an evaluation metric, NOV_COHERENCE, to thoroughly assess coherence. It is first pre-trained on various tasks that contribute to coherence detection (sentence reordering, irrelevant sentence recognition, etc.) and then trained on a coherence evaluation dataset. Their experimental results demonstrate that these pre-training tasks help the model achieve better performance in coherence evaluation.",
                "cite_spans": [
                    {
                        "start": 72,
                        "end": 76,
                        "text": "[53]",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 405,
                        "end": 409,
                        "text": "[43]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 812,
                        "end": 817,
                        "text": "[119]",
                        "ref_id": "BIBREF118"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Trained",
                "sec_num": "5.4.2"
            },
            {
                "text": "When evaluating a multi-modal story, we should measure the cross-modal relevance (Section 5.5.1) for both visual-totext and text-to-visual tasks. When evaluating visual-to-text generation, the measurement of generated textual stories remains the same as discussed in previous sections. Whereas for text-to-visual generation tasks, we should evaluate the quality of the visual outputs (Section 5.5.2), taking into account their quality and visual coherence.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Tasks Evaluation",
                "sec_num": "5.5"
            },
            {
                "text": "5.5.1 Multi-modal Relevance. Visual Captioning Accuracy [115, 116] is commonly used in early research on story visualization. It generates captions for created images and measures the similarity between these captions and the input story. With the introduction of the CLIP model, which effectively aligns text and visuals in the same embedding space, ClipScore [60] and EMScore [163] are proposed. These methods evaluate the similarity of text and visual content and can effectively assess the multi-modal relevance of textual story and visual story.",
                "cite_spans": [
                    {
                        "start": 56,
                        "end": 61,
                        "text": "[115,",
                        "ref_id": "BIBREF114"
                    },
                    {
                        "start": 62,
                        "end": 66,
                        "text": "116]",
                        "ref_id": "BIBREF115"
                    },
                    {
                        "start": 361,
                        "end": 365,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 378,
                        "end": 383,
                        "text": "[163]",
                        "ref_id": "BIBREF162"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Tasks Evaluation",
                "sec_num": "5.5"
            },
            {
                "text": "The former metrics separately measure each sentence's relevance to the correlated visual content. To measure the global relevance of the whole story and the visual content, Maharana et al. [116] train a Hierarchical-DAMSM model that extracts global representations for both the story and the image sequence, using their similarity to compute the retrieval-based R-Precision. Ning et al. [130] apply the earth mover's score [156] to measure the distance between the distribution of the album images and the generated stories in the CLIP embedding space.",
                "cite_spans": [
                    {
                        "start": 189,
                        "end": 194,
                        "text": "[116]",
                        "ref_id": "BIBREF115"
                    },
                    {
                        "start": 387,
                        "end": 392,
                        "text": "[130]",
                        "ref_id": "BIBREF129"
                    },
                    {
                        "start": 423,
                        "end": 428,
                        "text": "[156]",
                        "ref_id": "BIBREF155"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Tasks Evaluation",
                "sec_num": "5.5"
            },
            {
                "text": "Some metrics [171, 180] specifically address the global multi-modal relevance in visual storytelling. RoViST [180] proposes three evaluation metrics for visual grounding, coherence, and non-redundancy, achieving good overall evaluation performance. However, its RoViST-VG metric, which measures multi-modal relevance, overlooks temporal alignmentit doesn't consider whether the order of entities in the story matches their order in the image sequence. To address this, Surikuchi et al. [171] conduct an extended analysis of existing metrics and propose a novel metric, GROOViST, which is robust to temporal misalignments and correlates with human intuitions about grounding. Specifically, they utilize CLIP [142] features to improve visual grounding abilities. To address temporal misalignments, they penalize noun phrases (NPs) that are poorly grounded with temporally relevant images. Although GROOViST performs better in assessing temporal misalignment, simply assigning negative values for poorly grounded NPs isn't entirely reasonable. These NPs might represent a review of previously appeared visual content or could be logically correlated with current visual components. Further exploration is needed.",
                "cite_spans": [
                    {
                        "start": 13,
                        "end": 18,
                        "text": "[171,",
                        "ref_id": "BIBREF170"
                    },
                    {
                        "start": 19,
                        "end": 23,
                        "text": "180]",
                        "ref_id": "BIBREF179"
                    },
                    {
                        "start": 109,
                        "end": 114,
                        "text": "[180]",
                        "ref_id": "BIBREF179"
                    },
                    {
                        "start": 486,
                        "end": 491,
                        "text": "[171]",
                        "ref_id": "BIBREF170"
                    },
                    {
                        "start": 707,
                        "end": 712,
                        "text": "[142]",
                        "ref_id": "BIBREF141"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multi-modal Tasks Evaluation",
                "sec_num": "5.5"
            },
            {
                "text": ". FID [61] and FVD [176] are commonly used to assess the quality of visual outputs.",
                "cite_spans": [
                    {
                        "start": 6,
                        "end": 10,
                        "text": "[61]",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 19,
                        "end": 24,
                        "text": "[176]",
                        "ref_id": "BIBREF175"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Output Evaluation",
                "sec_num": "5.5.2"
            },
            {
                "text": "They measure the similarity between the features of the visual content and those of real-world images and videos. These features can be extracted by any model, such as standard features from Inception-V3 (image) [173] and I3D (video) [14] ,",
                "cite_spans": [
                    {
                        "start": 212,
                        "end": 217,
                        "text": "[173]",
                        "ref_id": "BIBREF172"
                    },
                    {
                        "start": 234,
                        "end": 238,
                        "text": "[14]",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Output Evaluation",
                "sec_num": "5.5.2"
            },
            {
                "text": "or from more powerful models like CLIP (image) [142] and InternVideo-MM-L-14 (video) [184] . DOVER [188] can be used to measure the perceptual quality of visual outputs. It predicts the average human subjective perception of a video.",
                "cite_spans": [
                    {
                        "start": 47,
                        "end": 52,
                        "text": "[142]",
                        "ref_id": "BIBREF141"
                    },
                    {
                        "start": 85,
                        "end": 90,
                        "text": "[184]",
                        "ref_id": "BIBREF183"
                    },
                    {
                        "start": 99,
                        "end": 104,
                        "text": "[188]",
                        "ref_id": "BIBREF187"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Output Evaluation",
                "sec_num": "5.5.2"
            },
            {
                "text": "Besides the quality of the output visual stories, the coherence of the visual scenes is also important. When evaluating the consistency of a generated video or an image sequence, two factors are typically considered: character and background consistency [115, 144] , usually measured by the accuracy and F1-Score.",
                "cite_spans": [
                    {
                        "start": 254,
                        "end": 259,
                        "text": "[115,",
                        "ref_id": "BIBREF114"
                    },
                    {
                        "start": 260,
                        "end": 264,
                        "text": "144]",
                        "ref_id": "BIBREF143"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Output Evaluation",
                "sec_num": "5.5.2"
            },
            {
                "text": "In tasks of continuous visual storytelling [12, 117] , the generated video or images are constrained by the visual prompt, leading to less randomness. For example, a car wouldn't have a random visual appearance. Therefore, we can directly compare these visual contents with the ground truth by calculating the cosine similarity of their normalized visual features.",
                "cite_spans": [
                    {
                        "start": 43,
                        "end": 47,
                        "text": "[12,",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 48,
                        "end": 52,
                        "text": "117]",
                        "ref_id": "BIBREF116"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Output Evaluation",
                "sec_num": "5.5.2"
            },
            {
                "text": "The development of large language models (LLMs) has led to significant advancements in automatic comprehension and generation. This also encourages several LLM-based evaluation methods [42, 102] . As verified in Wang et al. [181] ,",
                "cite_spans": [
                    {
                        "start": 185,
                        "end": 189,
                        "text": "[42,",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 190,
                        "end": 194,
                        "text": "102]",
                        "ref_id": "BIBREF101"
                    },
                    {
                        "start": 224,
                        "end": 229,
                        "text": "[181]",
                        "ref_id": "BIBREF180"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "LLM-BASED EVALUATION",
                "sec_num": "6"
            },
            {
                "text": "LLM has a much higher correlation with humans than traditional metrics. Additionally, it is capable of providing a human-like reasoning process, demonstrating much stronger reliability and interpretability. We discuss existing LLM-based evaluation methods from Section 6.1 to 6.4, along with their pros and cons in Section 6.5.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "LLM-BASED EVALUATION",
                "sec_num": "6"
            },
            {
                "text": "Similar to the traditional embedding-based methods mentioned in Section 5.2, researchers can utilize embeddings calculated by more advanced LLMs to achieve better performance [37] . Clearly, these methods have limitations -they lack explainability and struggle to encompass various evaluation aspects.",
                "cite_spans": [
                    {
                        "start": 175,
                        "end": 179,
                        "text": "[37]",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Embedding-based Metrics",
                "sec_num": "6.1"
            },
            {
                "text": "Such methods [19, 41, 192] count the evaluation score using the generation probability of LLMs. By designing different inputs and outputs, these probability-based metrics can address various aspects. The Implicit Score [19] simply forms the evaluation (overall quality or specific aspect) as a binary Yes or No question, and calculates based on the generation probability of answering \"yes\".",
                "cite_spans": [
                    {
                        "start": 13,
                        "end": 17,
                        "text": "[19,",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 18,
                        "end": 21,
                        "text": "41,",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 22,
                        "end": 26,
                        "text": "192]",
                        "ref_id": "BIBREF191"
                    },
                    {
                        "start": 219,
                        "end": 223,
                        "text": "[19]",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probability-based Metrics",
                "sec_num": "6.2"
            },
            {
                "text": "GPTScore [41] calculates the generation probability of the target story, based on the source input, task specification, and criteria definition. Since the probability of a sentence can vary due to superficial differences such as word order and sentence structure, such probability-based methods are susceptible to likelihood bias, particularly in aspects like relevance. Ohi et al. [131] quantify and explore the impact of this bias. Additionally, they propose a Likelihood-Bias-Mitigation method to mitigate likelihood bias by using highly biased instances as few-shot examples for in-context learning.",
                "cite_spans": [
                    {
                        "start": 9,
                        "end": 13,
                        "text": "[41]",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 382,
                        "end": 387,
                        "text": "[131]",
                        "ref_id": "BIBREF130"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probability-based Metrics",
                "sec_num": "6.2"
            },
            {
                "text": "Inspired by the idea that higher quality stories are more affected by perturbation than lower quality ones (for example, introducing typos could impact a fluent story more than a non-fluent one), Xie et al. [192] propose a novel method named DELTASCORE. This method evaluates the likelihood difference between stories before and after applying perturbation strategies related to specific aspects.",
                "cite_spans": [
                    {
                        "start": 207,
                        "end": 212,
                        "text": "[192]",
                        "ref_id": "BIBREF191"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Probability-based Metrics",
                "sec_num": "6.2"
            },
            {
                "text": "Such methods can also be defined as prompt-based methods, which attempt to prompt strong LLMs to automatically evaluate the results. In other words, they let LLMs serve as human annotators. One of the biggest advantages of generative-based metrics is interpretability, by generating the reasoning process for the evaluation result. The key challenge of these methods is to design proper prompts and effective frameworks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generative-based Metrics",
                "sec_num": "6.3"
            },
            {
                "text": "Aggregating evaluations through more detailed and easier sub-tasks can lead to better performance than directly assessing the overall quality. Saha et al. [157] propose the BRANCH-SOLVE-MERGE (BSM) method, which breaks down the overall evaluation into several sub-tasks. LLM generates results for each sub-evaluation and then aggregates them.",
                "cite_spans": [
                    {
                        "start": 155,
                        "end": 160,
                        "text": "[157]",
                        "ref_id": "BIBREF156"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generative-based Metrics",
                "sec_num": "6.3"
            },
            {
                "text": "Zhang et al. [206] propose WideDeep, a multi-layer LLM evaluator. In its first layer, each neuron handles a specific task of quality evaluation. In higher layers, each neuron integrates and abstracts the previously learned local evaluation information to generate a more comprehensive evaluation result. This multi-layer comprehension leads to improved performance. COAScore [48] prompts the LLM to generate a chain of aspects (such as those shown in Section 3.1) for evaluation. It scores each generated aspect and uses the chain-of-aspect knowledge (definitions and scores) to achieve the final evaluation result. CheckEval [90] breaks down each evaluation aspect into more detailed sub-aspects, develops a checklist for each dimension, and achieves better performance.",
                "cite_spans": [
                    {
                        "start": 13,
                        "end": 18,
                        "text": "[206]",
                        "ref_id": "BIBREF205"
                    },
                    {
                        "start": 375,
                        "end": 379,
                        "text": "[48]",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 626,
                        "end": 630,
                        "text": "[90]",
                        "ref_id": "BIBREF89"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generative-based Metrics",
                "sec_num": "6.3"
            },
            {
                "text": "LLMs can further enhance evaluation performance, consistency, and robustness by merging or debating on multiple evaluation results. FairEval [182] generates multiple pieces of evidence before assigning final ratings. G-Eval [107] calculates the final score as a probability-weighted summation of different Likert-type scales. In ChatEval [15] , unique personas are assigned to an LLM, leading to multiple agents engaging in the debate process. These agents can reach a conclusion after one-by-one or simultaneous debates. An additional summarizer can summarize each iteration of a simultaneous debate to add high-level messages. Through multi-agent discussion, more reliable and robust evaluation is achieved. MATEval [99] further incorporates the Chain-of-Thought [187] and self-reflection [114] strategies into the multi-agent discussion, improving the performance in open-ended story evaluation. As different LLMs naturally have differences and can comment on the results of others, this omits the process of persona assignment. Bai et al.",
                "cite_spans": [
                    {
                        "start": 141,
                        "end": 146,
                        "text": "[182]",
                        "ref_id": "BIBREF181"
                    },
                    {
                        "start": 224,
                        "end": 229,
                        "text": "[107]",
                        "ref_id": "BIBREF106"
                    },
                    {
                        "start": 338,
                        "end": 342,
                        "text": "[15]",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 718,
                        "end": 722,
                        "text": "[99]",
                        "ref_id": "BIBREF98"
                    },
                    {
                        "start": 765,
                        "end": 770,
                        "text": "[187]",
                        "ref_id": "BIBREF186"
                    },
                    {
                        "start": 791,
                        "end": 796,
                        "text": "[114]",
                        "ref_id": "BIBREF113"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generative-based Metrics",
                "sec_num": "6.3"
            },
            {
                "text": "[6] propose a peer-examination method that lets LLMs evaluate the results of other LLMs and combine the ranking results. SCALEEVAL [20] proposes an agent-debate-assisted meta-evaluation framework. It allows different LLM agents to engage in multi-round debates, resulting in more reasonable evaluation results that correlate highly with human annotators. It also provides the process of discussions to assist humans. SCALEEVAL supports any new user-defined scenarios and criteria, aiding automatic evaluation and new benchmark annotation.",
                "cite_spans": [
                    {
                        "start": 131,
                        "end": 135,
                        "text": "[20]",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generative-based Metrics",
                "sec_num": "6.3"
            },
            {
                "text": "PORTIA [101] splits the long answers into multiple segments, aligns similar content, and then merges them back into a single prompt for evaluation. Its main idea is to divide the comparison of long contexts into several comparisons between similar shorter segments, making the evaluation easier and reducing the problem of position bias. This might also be applied to story evaluation, for instance, dividing the evaluation into comparisons of the opening, progression, and ending.",
                "cite_spans": [
                    {
                        "start": 7,
                        "end": 12,
                        "text": "[101]",
                        "ref_id": "BIBREF100"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generative-based Metrics",
                "sec_num": "6.3"
            },
            {
                "text": "Due to the high cost and potential irreproducibility of generative-based metrics, some methods focus on fine-tuning open-source LLMs to create expert models for evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Trained Metrics",
                "sec_num": "6.4"
            },
            {
                "text": "Most of them access general evaluation tasks (with creative writing as a sub-domain), including PandaLM [186] ,",
                "cite_spans": [
                    {
                        "start": 104,
                        "end": 109,
                        "text": "[186]",
                        "ref_id": "BIBREF185"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Trained Metrics",
                "sec_num": "6.4"
            },
            {
                "text": "Prometheus [81, 82] , Shepherd [183] , Auto-J [95] , CritiqueLLM [76] , JudgeLM [211] and TIGERScore [74] . They collect large-scale evaluation benchmarks and fine-tune pre-trained LLMs through instruction tuning [204] . More details of their instruction datasets are summarized in Gao et al. [42] . Among these trained models, Auto-J [95] proposes several referable criteria for each task, while TIGERScore [74] provides detailed error annotations of various aspects. They demonstrate the potential to handle unseen tasks when instructions are built using predefined criteria.",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 15,
                        "text": "[81,",
                        "ref_id": "BIBREF80"
                    },
                    {
                        "start": 16,
                        "end": 19,
                        "text": "82]",
                        "ref_id": "BIBREF81"
                    },
                    {
                        "start": 31,
                        "end": 36,
                        "text": "[183]",
                        "ref_id": "BIBREF182"
                    },
                    {
                        "start": 46,
                        "end": 50,
                        "text": "[95]",
                        "ref_id": "BIBREF94"
                    },
                    {
                        "start": 65,
                        "end": 69,
                        "text": "[76]",
                        "ref_id": "BIBREF75"
                    },
                    {
                        "start": 80,
                        "end": 85,
                        "text": "[211]",
                        "ref_id": "BIBREF210"
                    },
                    {
                        "start": 101,
                        "end": 105,
                        "text": "[74]",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 213,
                        "end": 218,
                        "text": "[204]",
                        "ref_id": "BIBREF203"
                    },
                    {
                        "start": 293,
                        "end": 297,
                        "text": "[42]",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 335,
                        "end": 339,
                        "text": "[95]",
                        "ref_id": "BIBREF94"
                    },
                    {
                        "start": 408,
                        "end": 412,
                        "text": "[74]",
                        "ref_id": "BIBREF73"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Trained Metrics",
                "sec_num": "6.4"
            },
            {
                "text": "PERSE [179] and COHESENTIA [118] specifically focus on story evaluation. The former emphasizes personalized story evaluation, while the latter concentrates on coherence evaluation.",
                "cite_spans": [
                    {
                        "start": 6,
                        "end": 11,
                        "text": "[179]",
                        "ref_id": "BIBREF178"
                    },
                    {
                        "start": 27,
                        "end": 32,
                        "text": "[118]",
                        "ref_id": "BIBREF117"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Trained Metrics",
                "sec_num": "6.4"
            },
            {
                "text": "Although LLM-based evaluations achieve much better results than traditional methods, there is still a long way to go in realizing reliable and robust evaluation. Here we would like to summarize some useful strategies and existing limitations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Takeaways",
                "sec_num": "6.5"
            },
            {
                "text": "Helpful Strategies. We summarize the generally useful strategies in LLM-based evaluation: (1) In the evaluation prompt, clear and concise instructions are better than complex ones [80] . (2) Generating the reasoning process or detailed error analysis can aid the final evaluation [23, 74, 80, 95] . (3) Decomposing a complicated task into simpler, clearer sub-tasks is helpful [48, 101, 157, 206] . (4) Aggregating multiple evaluations [182] , incorporating a debating process [15, 20, 99] , and using multi-agent assessment [15] can all improve the performance and robustness of the results. (5) In-context learning could be helpful [72] , especially for personalized evaluation [179] .",
                "cite_spans": [
                    {
                        "start": 180,
                        "end": 184,
                        "text": "[80]",
                        "ref_id": "BIBREF79"
                    },
                    {
                        "start": 280,
                        "end": 284,
                        "text": "[23,",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 285,
                        "end": 288,
                        "text": "74,",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 289,
                        "end": 292,
                        "text": "80,",
                        "ref_id": "BIBREF79"
                    },
                    {
                        "start": 293,
                        "end": 296,
                        "text": "95]",
                        "ref_id": "BIBREF94"
                    },
                    {
                        "start": 377,
                        "end": 381,
                        "text": "[48,",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 382,
                        "end": 386,
                        "text": "101,",
                        "ref_id": "BIBREF100"
                    },
                    {
                        "start": 387,
                        "end": 391,
                        "text": "157,",
                        "ref_id": "BIBREF156"
                    },
                    {
                        "start": 392,
                        "end": 396,
                        "text": "206]",
                        "ref_id": "BIBREF205"
                    },
                    {
                        "start": 436,
                        "end": 441,
                        "text": "[182]",
                        "ref_id": "BIBREF181"
                    },
                    {
                        "start": 477,
                        "end": 481,
                        "text": "[15,",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 482,
                        "end": 485,
                        "text": "20,",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 486,
                        "end": 489,
                        "text": "99]",
                        "ref_id": "BIBREF98"
                    },
                    {
                        "start": 525,
                        "end": 529,
                        "text": "[15]",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 593,
                        "end": 596,
                        "text": "(5)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 634,
                        "end": 638,
                        "text": "[72]",
                        "ref_id": "BIBREF71"
                    },
                    {
                        "start": 680,
                        "end": 685,
                        "text": "[179]",
                        "ref_id": "BIBREF178"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Takeaways",
                "sec_num": "6.5"
            },
            {
                "text": "Limitations. There are four types of discovered biases: (1) Format Bias, which means the optimal performance is only sentence can vary due to superficial differences, especially in aspects like relevance [131] .",
                "cite_spans": [
                    {
                        "start": 204,
                        "end": 209,
                        "text": "[131]",
                        "ref_id": "BIBREF130"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Takeaways",
                "sec_num": "6.5"
            },
            {
                "text": "There are also concerns about the substantial expense and non-reproducibility of methods based on commercial LLMs. This calls for more research based on open-source models. Proposing evaluation benchmarks for developing stronger evaluation expert models is also valuable.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Takeaways",
                "sec_num": "6.5"
            },
            {
                "text": "In this section, we present the evaluation performancefoot_2 of existing automatic metrics on the story evaluation benchmark, from collected experimental results and our additional experiments. We focus on the most commonly used metrics and those demonstrating exceptional performance. We discuss their advantages and disadvantages, and suggest potential uses for them.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "EVALUATING METRICS ON STORY EVALUATION BENCHMARK",
                "sec_num": "7"
            },
            {
                "text": "The evaluation results of automatic metrics on the story evaluation benchmark dataset OpenMEVAfoot_3 are displayed in Figure 4 . The representative evaluation metrics involve: (a) Reference-based Metrics, including widely used lexicalbased metrics (BLEU [132] , ROUGE [103] , METEOR [103] ), embedding-based and probability-based metrics (BERTScore [205] , BARTScore_ref [201] ), trained metrics (BLEURT [160] , COMET22 [148] , UniEval [208] ), and LLM-based scores (InstructScore [194] , GPTScore_ref [41] ). (b) Reference-free metrics, including commonly used probability-based metrics (PPL [5] , BARTScore_src [201] ), trained metrics (Union [53] , COMETKiwi [150] ), and strong LLM-based scores (Llama2-13B [175] , GPTScore_src [41] , Auto-J [95] , TigerScore [74] , Implicit/Explicit Score [19] ).",
                "cite_spans": [
                    {
                        "start": 254,
                        "end": 259,
                        "text": "[132]",
                        "ref_id": "BIBREF131"
                    },
                    {
                        "start": 268,
                        "end": 273,
                        "text": "[103]",
                        "ref_id": "BIBREF102"
                    },
                    {
                        "start": 283,
                        "end": 288,
                        "text": "[103]",
                        "ref_id": "BIBREF102"
                    },
                    {
                        "start": 349,
                        "end": 354,
                        "text": "[205]",
                        "ref_id": "BIBREF204"
                    },
                    {
                        "start": 371,
                        "end": 376,
                        "text": "[201]",
                        "ref_id": "BIBREF200"
                    },
                    {
                        "start": 404,
                        "end": 409,
                        "text": "[160]",
                        "ref_id": "BIBREF159"
                    },
                    {
                        "start": 420,
                        "end": 425,
                        "text": "[148]",
                        "ref_id": "BIBREF147"
                    },
                    {
                        "start": 436,
                        "end": 441,
                        "text": "[208]",
                        "ref_id": "BIBREF207"
                    },
                    {
                        "start": 481,
                        "end": 486,
                        "text": "[194]",
                        "ref_id": "BIBREF193"
                    },
                    {
                        "start": 502,
                        "end": 506,
                        "text": "[41]",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 593,
                        "end": 596,
                        "text": "[5]",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 613,
                        "end": 618,
                        "text": "[201]",
                        "ref_id": "BIBREF200"
                    },
                    {
                        "start": 645,
                        "end": 649,
                        "text": "[53]",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 662,
                        "end": 667,
                        "text": "[150]",
                        "ref_id": "BIBREF149"
                    },
                    {
                        "start": 711,
                        "end": 716,
                        "text": "[175]",
                        "ref_id": "BIBREF174"
                    },
                    {
                        "start": 732,
                        "end": 736,
                        "text": "[41]",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 746,
                        "end": 750,
                        "text": "[95]",
                        "ref_id": "BIBREF94"
                    },
                    {
                        "start": 764,
                        "end": 768,
                        "text": "[74]",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 795,
                        "end": 799,
                        "text": "[19]",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 125,
                        "end": 126,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation of Overall Quality",
                "sec_num": "7.1"
            },
            {
                "text": "Our observations and discussions, which could also be applied to other creative generation tasks, are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation of Overall Quality",
                "sec_num": "7.1"
            },
            {
                "text": "\u2022 While accessing the overall performance of a textual story, although we have stated that there are no standard answers for creative story generation, reference-based evaluation results remain significant. Metrics such as COMET22 and UniEval, which have been trained on human evaluation benchmarks, can show good correlations with human judgments. It's advantageous to present both reference-based and reference-free metrics. However, for longer stories, reference-based metrics may be more suitable for plot-level matching rather than evaluating the entire story. \u2022 Regarding reference-based methods, widely used lexical-based metrics such as BLEU and ROUGE show very low correlation with human evaluation (Figure 4 ). Embedding-based and probability-based metrics demonstrate stronger performance due to their improved semantic comprehension. However, to better align with human judgment, training on human evaluation benchmarks is necessary. This is verified by the performance of UniEval and COMET22, which show even stronger results than some reference-based LLM metrics.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 716,
                        "end": 717,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation of Overall Quality",
                "sec_num": "7.1"
            },
            {
                "text": "\u2022 Regarding reference-free methods, although the metrics based on powerful ChatGPT achieve high performance, they suffer from non-reproducibility and high cost. Open-source metrics that show comparable results are preferable alternatives. Specifically, metrics trained for story evaluation such as Union [53] , or evaluation expertise LLMs like TigerScore [74] and AUTO-J [95] are worth considering. We can further enhance performance by applying additional strategies (Section 6.5) such as the debating process.",
                "cite_spans": [
                    {
                        "start": 304,
                        "end": 308,
                        "text": "[53]",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 356,
                        "end": 360,
                        "text": "[74]",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 372,
                        "end": 376,
                        "text": "[95]",
                        "ref_id": "BIBREF94"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation of Overall Quality",
                "sec_num": "7.1"
            },
            {
                "text": "\u2022 Existing automatic evaluations, even the effective LLM-based methods, have their limitations. Thus human evaluation and collaborative evaluation (Section 8) still have significant value.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation of Overall Quality",
                "sec_num": "7.1"
            },
            {
                "text": "Regarding the evaluation of various aspects, our discoveries and discussions are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation of Sub-Aspects",
                "sec_num": "7.2"
            },
            {
                "text": "\u2022 We select powerful metrics capable of evaluating specific aspects and present their performance on the benchmark proposed by Xie et al. [191] , a dataset covering commonly explored aspects. As shown in Figure 5 , evaluation expert models like UniEval and AUTO-J perform comparably or better than ChatGPTfoot_4 . Detailed instructions (including criteria definitions and scoring standards) [22] or fine-grained evaluations (such as DeltaScore [192] ) can improve the evaluation of specific aspects. Note that this analysis may require validation on more robust benchmarks.",
                "cite_spans": [
                    {
                        "start": 138,
                        "end": 143,
                        "text": "[191]",
                        "ref_id": "BIBREF190"
                    },
                    {
                        "start": 391,
                        "end": 395,
                        "text": "[22]",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 444,
                        "end": 449,
                        "text": "[192]",
                        "ref_id": "BIBREF191"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 211,
                        "end": 212,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation of Sub-Aspects",
                "sec_num": "7.2"
            },
            {
                "text": "\u2022 According to the experimental results shown in Figure 5 , as well as existing research works [21, 22, 192] , LLM-based methods are currently the best proxy for human evaluation of stories. However, they still face Fig. 5 . The Kendall Correlation between powerful metrics and multi-aspect human ratings proposed by Xie et al. [191] .",
                "cite_spans": [
                    {
                        "start": 95,
                        "end": 99,
                        "text": "[21,",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 100,
                        "end": 103,
                        "text": "22,",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 104,
                        "end": 108,
                        "text": "192]",
                        "ref_id": "BIBREF191"
                    },
                    {
                        "start": 328,
                        "end": 333,
                        "text": "[191]",
                        "ref_id": "BIBREF190"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 56,
                        "end": 57,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 221,
                        "end": 222,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation of Sub-Aspects",
                "sec_num": "7.2"
            },
            {
                "text": "challenges in aspects like interestingness, which require further exploration in both general and personalized evaluation [179] . \u2022 Especially regarding coherence, existing research such as COHENSENTIA [118] and NOV_COHERENCE [119] can be applied. However, these approaches are not that effective for evaluating long stories.",
                "cite_spans": [
                    {
                        "start": 122,
                        "end": 127,
                        "text": "[179]",
                        "ref_id": "BIBREF178"
                    },
                    {
                        "start": 202,
                        "end": 207,
                        "text": "[118]",
                        "ref_id": "BIBREF117"
                    },
                    {
                        "start": 226,
                        "end": 231,
                        "text": "[119]",
                        "ref_id": "BIBREF118"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation of Sub-Aspects",
                "sec_num": "7.2"
            },
            {
                "text": "This section discusses the research on human-AI collaborative evaluation and the evaluating considerations for collaborative writing frameworks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "COLLABORATIVE STORY EVALUATION AND GENERATION",
                "sec_num": "8"
            },
            {
                "text": "Collaborative Evaluation. Both human evaluation and automatic evaluation have their own advantages and limitations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "COLLABORATIVE STORY EVALUATION AND GENERATION",
                "sec_num": "8"
            },
            {
                "text": "Human evaluation is considered the gold standard, but it can be time-consuming, expensive, subjective, and inconsistent [24] . Automatic evaluations, particularly those based on open-source LLMs, can be cost-effective and produce fewer outlier values. However, they still need improvement to better correlate with human standards. Recent studies have explored to combine these advantages through collaborative evaluation. Li et al. [97] introduce COEVAL, a two-stage approach. First, it generates a checklist of task-specific criteria, then conducts instance evaluation. Both stages begin with LLM-generated results, which are subsequently scrutinized by humans. Through this human scrutiny, COEVAL revises approximately 20% of evaluation scores for ultimate reliability. Kim et al. [83] propose EvalLM, an interactive platform that provides automatic, detailed evaluation outputs based on user-defined criteria. This enables users to efficiently refine task prompts and evaluation criteria. In sum, the main issue lies in designing effective interactive strategies for efficient collaboration between humans and AI systems.",
                "cite_spans": [
                    {
                        "start": 120,
                        "end": 124,
                        "text": "[24]",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 432,
                        "end": 436,
                        "text": "[97]",
                        "ref_id": "BIBREF96"
                    },
                    {
                        "start": 783,
                        "end": 787,
                        "text": "[83]",
                        "ref_id": "BIBREF82"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "COLLABORATIVE STORY EVALUATION AND GENERATION",
                "sec_num": "8"
            },
            {
                "text": "Evaluation of Collaborative Story Generation. Today's Large Language Models, particularly commercial ones like GPT-4 [1] and Claude [189] , can match or slightly surpass human writers in most areas [190, 191] . However, they may still fall short in aspects such as creativity and interestingness [47, 89, 191] . Nevertheless, this advancement enhances the feasibility of collaborative story generation -a process where a person works with model outputs to jointly create a story [11, 26, 71] . Evaluating collaborative writing requires consideration of both story quality and user experience.",
                "cite_spans": [
                    {
                        "start": 117,
                        "end": 120,
                        "text": "[1]",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 132,
                        "end": 137,
                        "text": "[189]",
                        "ref_id": "BIBREF188"
                    },
                    {
                        "start": 198,
                        "end": 203,
                        "text": "[190,",
                        "ref_id": "BIBREF189"
                    },
                    {
                        "start": 204,
                        "end": 208,
                        "text": "191]",
                        "ref_id": "BIBREF190"
                    },
                    {
                        "start": 296,
                        "end": 300,
                        "text": "[47,",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 301,
                        "end": 304,
                        "text": "89,",
                        "ref_id": "BIBREF88"
                    },
                    {
                        "start": 305,
                        "end": 309,
                        "text": "191]",
                        "ref_id": "BIBREF190"
                    },
                    {
                        "start": 479,
                        "end": 483,
                        "text": "[11,",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 484,
                        "end": 487,
                        "text": "26,",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 488,
                        "end": 491,
                        "text": "71]",
                        "ref_id": "BIBREF70"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "COLLABORATIVE STORY EVALUATION AND GENERATION",
                "sec_num": "8"
            },
            {
                "text": "Quantitative evaluations of human experience include edit distance [35] , percentage of accepted suggestions or applied model-generated stories, generation productivity (words written per unit time) [35] , and story completion time [172] .",
                "cite_spans": [
                    {
                        "start": 67,
                        "end": 71,
                        "text": "[35]",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 199,
                        "end": 203,
                        "text": "[35]",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 232,
                        "end": 237,
                        "text": "[172]",
                        "ref_id": "BIBREF171"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "COLLABORATIVE STORY EVALUATION AND GENERATION",
                "sec_num": "8"
            },
            {
                "text": "Lee et al. [89] apply the concepts from Storch [168] , redefining equality as the even distribution of writing events between humans and AI, and mutuality as the proportion of user-system interactions among all operations. Table 6 . The common aspects used for evaluating collaborative writing.",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 15,
                        "text": "[89]",
                        "ref_id": "BIBREF88"
                    },
                    {
                        "start": 47,
                        "end": 52,
                        "text": "[168]",
                        "ref_id": "BIBREF167"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 229,
                        "end": 230,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "COLLABORATIVE STORY EVALUATION AND GENERATION",
                "sec_num": "8"
            },
            {
                "text": "Helpfulness I found the AI system helpful.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aspect Definition",
                "sec_num": null
            },
            {
                "text": "Collaboration I felt like I was collaborating with the AI system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aspect Definition",
                "sec_num": null
            },
            {
                "text": "Ease I found it easy to write with the AI system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aspect Definition",
                "sec_num": null
            },
            {
                "text": "Enjoyment I enjoyed writing with the AI system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aspect Definition",
                "sec_num": null
            },
            {
                "text": "Expression I was able to express my creative goals while writing with the AI system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Aspect Definition",
                "sec_num": null
            },
            {
                "text": "The script(s) written with the AI system feel unique.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unique",
                "sec_num": null
            },
            {
                "text": "Ownership I feel I have ownership over the created script(s).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unique",
                "sec_num": null
            },
            {
                "text": "I was surprised by the responses from the AI system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Surprise",
                "sec_num": null
            },
            {
                "text": "Proud I'm proud of the final outputs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Surprise",
                "sec_num": null
            },
            {
                "text": "User feedback might be more important for evaluating collaborative writing systems. Several studies [46, 100, 125, 167, 198, 200] have developed questionnaires to gather this feedback. These questions either assess the overall performance (whether the framework is a good platform and if they would use it again); or assess multiple aspects, focusing on the helpfulness, user-friendliness, user experience, and effectiveness of the collaborative writing framework. Detailed questions can be located in each paper, where Table 6 shows the example questions from Mirowski et al. [125] . Future research may adapt these questions to align with the specific goals of proposed systems.",
                "cite_spans": [
                    {
                        "start": 100,
                        "end": 104,
                        "text": "[46,",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 105,
                        "end": 109,
                        "text": "100,",
                        "ref_id": "BIBREF99"
                    },
                    {
                        "start": 110,
                        "end": 114,
                        "text": "125,",
                        "ref_id": "BIBREF124"
                    },
                    {
                        "start": 115,
                        "end": 119,
                        "text": "167,",
                        "ref_id": "BIBREF166"
                    },
                    {
                        "start": 120,
                        "end": 124,
                        "text": "198,",
                        "ref_id": "BIBREF197"
                    },
                    {
                        "start": 125,
                        "end": 129,
                        "text": "200]",
                        "ref_id": "BIBREF199"
                    },
                    {
                        "start": 577,
                        "end": 582,
                        "text": "[125]",
                        "ref_id": "BIBREF124"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 526,
                        "end": 527,
                        "text": "6",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Surprise",
                "sec_num": null
            },
            {
                "text": "Based on the above survey, we would like to make following recommendations for future research explorations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RECOMMENDATIONS FOR FUTURE EXPLORATIONS",
                "sec_num": "9"
            },
            {
                "text": "Standardized Evaluation Criteria. Although we have analyzed the evaluation criteria in previous research, as shown in Section 3.1, we hope to encourage further exploration of standardized evaluation criteria, especially for subjective aspects like interestingness that are difficult to evaluate automatically. Specifically for LLM-based evaluation, it's valuable to explore suitable criteria for various LLMs, as different models may favor distinct definitions. For example, we can iteratively refine initial criteria through human feedback [83] or system self-improvement [108] , based on issues identified in evaluation results using the current criteria.",
                "cite_spans": [
                    {
                        "start": 541,
                        "end": 545,
                        "text": "[83]",
                        "ref_id": "BIBREF82"
                    },
                    {
                        "start": 573,
                        "end": 578,
                        "text": "[108]",
                        "ref_id": "BIBREF107"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RECOMMENDATIONS FOR FUTURE EXPLORATIONS",
                "sec_num": "9"
            },
            {
                "text": "Story Evaluation Benchmark. Most existing story evaluation benchmarks (Section 3.2) are limited to stories generated from prompts in ROCStories [127] and WritingPrompts [38] . Future benchmarks should encompass more diverse domains, incorporate more complex aspects, and include longer stories. Additionally, since human evaluation is expensive and time-consuming, it is usually conducted on a subset of a large-scale dataset. Therefore, exploring methods for effective subset sampling is also valuable [154] .",
                "cite_spans": [
                    {
                        "start": 144,
                        "end": 149,
                        "text": "[127]",
                        "ref_id": "BIBREF126"
                    },
                    {
                        "start": 169,
                        "end": 173,
                        "text": "[38]",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 503,
                        "end": 508,
                        "text": "[154]",
                        "ref_id": "BIBREF153"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RECOMMENDATIONS FOR FUTURE EXPLORATIONS",
                "sec_num": "9"
            },
            {
                "text": "Long Story Evaluation. With the development of Large Language Models, it is more possible for automatic evaluation and generation of long story [89, 195, 199, 210] . However, evaluating long narratives presents numerous challenges.",
                "cite_spans": [
                    {
                        "start": 144,
                        "end": 148,
                        "text": "[89,",
                        "ref_id": "BIBREF88"
                    },
                    {
                        "start": 149,
                        "end": 153,
                        "text": "195,",
                        "ref_id": "BIBREF194"
                    },
                    {
                        "start": 154,
                        "end": 158,
                        "text": "199,",
                        "ref_id": "BIBREF198"
                    },
                    {
                        "start": 159,
                        "end": 163,
                        "text": "210]",
                        "ref_id": "BIBREF209"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RECOMMENDATIONS FOR FUTURE EXPLORATIONS",
                "sec_num": "9"
            },
            {
                "text": "Firstly, obtaining human annotations for lengthy stories is difficult. Secondly, encoding and comprehending long contexts presents a challenging task. While existing works have made progress in 0-10K story processing, the automatic generation and evaluation of human-like long stories, such as a Harry Potter fanfic fiction of at least 40K words [122] ,",
                "cite_spans": [
                    {
                        "start": 346,
                        "end": 351,
                        "text": "[122]",
                        "ref_id": "BIBREF121"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RECOMMENDATIONS FOR FUTURE EXPLORATIONS",
                "sec_num": "9"
            },
            {
                "text": "is still challenging.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RECOMMENDATIONS FOR FUTURE EXPLORATIONS",
                "sec_num": "9"
            },
            {
                "text": "We convert a few original aspects into our summarized aspects ( \u00a73.1). For the original aspect names and definitions, please refer to the related papers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Reasoning process, on the other hand, provides a more coarse-grained explanation of the evaluation result.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The correlation of these metrics with the human-annotated evaluation dataset can be calculated using Pearson[135], Spearman[165], or Kendall-Tau[78] correlations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "To the best of our knowledge, it is the story evaluation benchmark that has the highest citation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We use OpenAI API with the GPT-3.5-turbo model",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "Story Type Format Reasoning Criteria Aspects #Stories #Samples OpenMEVA [54] Model-Generated Story Likert Scale (1-5) No Overall REL, FLU, COH, COMM 2,000 2,000 HANNA [21] Model-Generated Story Likert Scale (1-5) No Single REL, COH, EMP, SUR, INT, INF 1,056 19,008 VHED [65] Model-Generated Visual Story Comparison No Overall FLU, COH, CLA, REL 4,500 13,875 StoryER-Rank [16] Human-Written Story Comparison No Overall -63,929 116,971 StoryER-Scale [16] Human-Written Story Likert Scale (1-5) Yes Single COH, END, STY, CHA, EMP 12,669 45,948 Per-MPST [179] Human-Written Movie Plot Comparison Yes Overall -981 69,947 Per-DOC [179] Human-Written Novel Plot Likert Scale (1-5) Yes Single INT, ADAP, SUR, CHA, END 596 8,922 Xie [190] Model-generated Story Likert Scale (1-5) No Single REL, FLU, COH, COMM, INT 200 1,000 COHESENTIA [118] Model-generated Story Likert Scale (1-5) Yes Single COH 500 500",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": null
            },
            {
                "text": "Multi-modal Story Evaluation. As discussed in Section 2.2, in multi-modal story evaluation, we should consider not only spatial relevance but also temporal relevance and more complex logical relevance. Although some efforts [171, 180] have been made to access these challenges, this is still a under-explored domain.Personalized Evaluation. Human evaluation might suffer from low inter-annotator agreement, particularly for subjective aspects like interestingness and character development [13] . It's worthwhile to explore personalized evaluation, which is valuable for applications like recommendation systems. Wang et al. [179] explore to address the problem of personalized evaluation. They propose two personalized story evaluation datasets, fine-tuning open-source LLM with instructions and few-shot reviewer preference. Further research into more benchmarks and personalized evaluation methods is encouraged.Fairness Improvement. Fairness improvement (or debiasing) is a crucial issue in LLM-based evaluation. Common problems include position bias, where LLMs favor the first option in a comparison pair; format bias, which refers to their sensitivity to prompts; and knowledge bias, which implies a preference for memorized or generated stories. Despite efforts to mitigate these biases, further research is needed.Robustness Analysis. He et al. [58] has developed various stress tests to examine the robustness of metrics derived from medium-size pre-trained language models. They identify several blind spots in metrics such as BERTScore [205] and UniEval [208] . Although newer metrics based on more powerful LLMs show increased robustness, assessing the reliability of these automatic metrics remains essential.Reliability Exploration. Improving reliability helps ensure people trust and use automatic evaluation results. To address this, existing metrics primarily provide a more human-like rational process [80] or conduct more reliable error analysis [74] . Future works could also explore ways to mitigate overconfidence and reduce inconsistencies.Explore the Difference between Human-written and AI-generated Stories. Here we have two objectives: first, to explore the gap between AI-generated and human-written stories in order to enhance the quality of generated content.Second, to develop methods for distinguishing between AI-generated and human-written stories, with the aim of preventing potential harm, such as the spread of fake news stories.Collaborative Evaluation. Collaborative evaluation can leverage the effectiveness of both human evaluation and automatic evaluation. Callan and Foster [13] found that automatic metrics can perform at near-human levels, except for aspects like interestingness. As such, it is reasonable to conduct an automatic evaluation first, providing the result and reasoning process. Based on these outputs, humans can then focus on assessing the aspects that machines seem to struggle with.",
                "cite_spans": [
                    {
                        "start": 224,
                        "end": 229,
                        "text": "[171,",
                        "ref_id": "BIBREF170"
                    },
                    {
                        "start": 230,
                        "end": 234,
                        "text": "180]",
                        "ref_id": "BIBREF179"
                    },
                    {
                        "start": 490,
                        "end": 494,
                        "text": "[13]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 625,
                        "end": 630,
                        "text": "[179]",
                        "ref_id": "BIBREF178"
                    },
                    {
                        "start": 1354,
                        "end": 1358,
                        "text": "[58]",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 1548,
                        "end": 1553,
                        "text": "[205]",
                        "ref_id": "BIBREF204"
                    },
                    {
                        "start": 1566,
                        "end": 1571,
                        "text": "[208]",
                        "ref_id": "BIBREF207"
                    },
                    {
                        "start": 1921,
                        "end": 1925,
                        "text": "[80]",
                        "ref_id": "BIBREF79"
                    },
                    {
                        "start": 1966,
                        "end": 1970,
                        "text": "[74]",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 2618,
                        "end": 2622,
                        "text": "[13]",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "annex",
                "sec_num": null
            },
            {
                "text": "In this survey, we summarize and discuss the evaluation of human-written or automatically generated stories. We first progress with various types of story generation tasks, the story evaluation criteria, and benchmark datasets. We then introduce a taxonomy to categorize existing metrics that are proposed or can be adopted for story evaluation, discussing them in detail. Additionally, we carry out experiments, report their quantitative performance on story evaluation, and provide recommendations. Finally, we propose future directions for story evaluation, which are suitable for general evaluations as well. We hope our survey will help readers understand the developments in story generation and automatic evaluations, while inspiring future research directions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CONCLUSION",
                "sec_num": "10"
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Shyamal Anadkat, et al. 2024. Gpt-4 technical report",
                "authors": [
                    {
                        "first": "Josh",
                        "middle": [],
                        "last": "Achiam",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Adler",
                        "suffix": ""
                    },
                    {
                        "first": "Sandhini",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Lama",
                        "middle": [],
                        "last": "Ahmad",
                        "suffix": ""
                    },
                    {
                        "first": "Ilge",
                        "middle": [],
                        "last": "Akkaya",
                        "suffix": ""
                    },
                    {
                        "first": "Florencia",
                        "middle": [],
                        "last": "Leoni Aleman",
                        "suffix": ""
                    },
                    {
                        "first": "Diogo",
                        "middle": [],
                        "last": "Almeida",
                        "suffix": ""
                    },
                    {
                        "first": "Janko",
                        "middle": [],
                        "last": "Altenschmidt",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Altman",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2303.08774"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2024. Gpt-4 technical report. (2024). arXiv:2303.08774 https://arxiv.org/abs/2303.08774",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation",
                "authors": [
                    {
                        "first": "Nader",
                        "middle": [],
                        "last": "Akoury",
                        "suffix": ""
                    },
                    {
                        "first": "Shufan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Josh",
                        "middle": [],
                        "last": "Whiting",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Hood",
                        "suffix": ""
                    },
                    {
                        "first": "Nanyun",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "6470--6484",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.EMNLP-MAIN.525"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, and Mohit Iyyer. 2020. STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 6470-6484. https://doi.org/10.18653/V1/2020.EMNLP-MAIN.525",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Bringing Stories Alive: Generating Interactive Fiction Worlds",
                "authors": [
                    {
                        "first": "Prithviraj",
                        "middle": [],
                        "last": "Ammanabrolu",
                        "suffix": ""
                    },
                    {
                        "first": "Wesley",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Tu",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Broniec",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [
                            "O"
                        ],
                        "last": "Riedl",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the Sixteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2020, virtual",
                "volume": "",
                "issue": "",
                "pages": "3--9",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Prithviraj Ammanabrolu, Wesley Cheung, Dan Tu, William Broniec, and Mark O. Riedl. 2020. Bringing Stories Alive: Generating Interactive Fiction Worlds. In Proceedings of the Sixteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2020, virtual, October 19-23, 2020, Levi Lelis and David Thue (Eds.). AAAI Press, 3-9. https://ojs.aaai.org/index.php/AIIDE/article/view/7400",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "SPICE: Semantic Propositional Image Caption Evaluation",
                "authors": [
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Anderson",
                        "suffix": ""
                    },
                    {
                        "first": "Basura",
                        "middle": [],
                        "last": "Fernando",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Gould",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Computer Vision -ECCV 2016 -14th European Conference",
                "volume": "9909",
                "issue": "",
                "pages": "382--398",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-319-46454-1_24"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. SPICE: Semantic Propositional Image Caption Evaluation. In Computer Vision -ECCV 2016 -14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V (Lecture Notes in Computer Science, Vol. 9909), Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (Eds.). Springer, 382-398. https://doi.org/10.1007/978-3-319-46454-1_24",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "A Maximum Likelihood Approach to Continuous Speech Recognition",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Lalit",
                        "suffix": ""
                    },
                    {
                        "first": "Frederick",
                        "middle": [],
                        "last": "Bahl",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "L"
                        ],
                        "last": "Jelinek",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mercer",
                        "suffix": ""
                    }
                ],
                "year": 1983,
                "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
                "volume": "5",
                "issue": "",
                "pages": "179--190",
                "other_ids": {
                    "DOI": [
                        "10.1109/TPAMI.1983.4767370"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. 1983. A Maximum Likelihood Approach to Continuous Speech Recognition. IEEE Trans. Pattern Anal. Mach. Intell. 5, 2 (1983), 179-190. https://doi.org/10.1109/TPAMI.1983.4767370",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
                "authors": [
                    {
                        "first": "Yushi",
                        "middle": [],
                        "last": "Bai",
                        "suffix": ""
                    },
                    {
                        "first": "Jiahao",
                        "middle": [],
                        "last": "Ying",
                        "suffix": ""
                    },
                    {
                        "first": "Yixin",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Lv",
                        "suffix": ""
                    },
                    {
                        "first": "Yuze",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaozhi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jifan",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Kaisheng",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Yijia",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Haozhe",
                        "middle": [],
                        "last": "Lyu",
                        "suffix": ""
                    },
                    {
                        "first": "Jiayin",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Juanzi",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. 2023. Benchmarking Foundation Models with Language-Model-as-an-Examiner. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 -16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/ f64e55d03e2fe61aa4114e49cb654acb-Abstract-Datasets_and_Benchmarks.html",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Condensed movies: Story based retrieval with contextual embeddings",
                "authors": [
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Bain",
                        "suffix": ""
                    },
                    {
                        "first": "Arsha",
                        "middle": [],
                        "last": "Nagrani",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zisserman",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the Asian Conference on Computer Vision",
                "volume": "12626",
                "issue": "",
                "pages": "460--479",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-030-69541-5_28"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Max Bain, Arsha Nagrani, Andrew Brown, and Andrew Zisserman. 2020. Condensed movies: Story based retrieval with contextual embeddings. In Proceedings of the Asian Conference on Computer Vision, Kyoto, Japan, November 30 -December 4, 2020, Revised Selected Papers, Part V (Lecture Notes in Computer Science, Vol. 12626). Springer, 460-479. https://doi.org/10.1007/978-3-030-69541-5_28",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments",
                "authors": [
                    {
                        "first": "Satanjeev",
                        "middle": [],
                        "last": "Banerjee",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005",
                "volume": "",
                "issue": "",
                "pages": "65--72",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005, Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare R. Voss (Eds.). Association for Computational Linguistics, 65-72. https://aclanthology.org/W05-0909/",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Trustworthiness of Children Stories Generated by Large Language Models",
                "authors": [
                    {
                        "first": "Prabin",
                        "middle": [],
                        "last": "Bhandari",
                        "suffix": ""
                    },
                    {
                        "first": "Hannah",
                        "middle": [
                            "Marie"
                        ],
                        "last": "Brennan",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 16th International Natural Language Generation Conference, INLG 2023",
                "volume": "",
                "issue": "",
                "pages": "352--361",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.INLG-MAIN.24"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Prabin Bhandari and Hannah Marie Brennan. 2023. Trustworthiness of Children Stories Generated by Large Language Models. In Proceedings of the 16th International Natural Language Generation Conference, INLG 2023, Prague, Czechia, September 11 -15, 2023, C. Maria Keet, Hung-Yi Lee, and Sina Zarrie\u00df (Eds.). Association for Computational Linguistics, 352-361. https://doi.org/10.18653/V1/2023.INLG-MAIN.24",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Modeling Protagonist Emotions for Emotion-Aware Storytelling",
                "authors": [
                    {
                        "first": "Faeze",
                        "middle": [],
                        "last": "Brahman",
                        "suffix": ""
                    },
                    {
                        "first": "Snigdha",
                        "middle": [],
                        "last": "Chaturvedi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "5277--5294",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.EMNLP-MAIN.426"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Faeze Brahman and Snigdha Chaturvedi. 2020. Modeling Protagonist Emotions for Emotion-Aware Storytelling. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 5277-5294. https://doi.org/10.18653/V1/2020.EMNLP-MAIN.426",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Collaborative Storytelling with Human Actors and AI Narrators",
                "authors": [
                    {
                        "first": "Boyd",
                        "middle": [],
                        "last": "Branch",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Mirowski",
                        "suffix": ""
                    },
                    {
                        "first": "Kory",
                        "middle": [
                            "W"
                        ],
                        "last": "Mathewson",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the Twelfth International Conference on Computational Creativity, M\u00e9xico City, M\u00e9xico (Virtual)",
                "volume": "",
                "issue": "",
                "pages": "96--101",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Boyd Branch, Piotr Mirowski, and Kory W. Mathewson. 2021. Collaborative Storytelling with Human Actors and AI Narrators. In Proceedings of the Twelfth International Conference on Computational Creativity, M\u00e9xico City, M\u00e9xico (Virtual), September 14-18, 2021, Andr\u00e9s G\u00f3mez de Silva Garza, Tony Veale, Wendy Aguilar, and Rafael P\u00e9rez y P\u00e9rez (Eds.). Association for Computational Creativity (ACC), 96-101. https://computationalcreativity. net/iccc21/wp-content/uploads/2021/09/ICCC_2021_paper_118.pdf",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "StoryBench: A Multifaceted Benchmark for Continuous Story Visualization",
                "authors": [
                    {
                        "first": "Emanuele",
                        "middle": [],
                        "last": "Bugliarello",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Hernan Moraldo",
                        "suffix": ""
                    },
                    {
                        "first": "Ruben",
                        "middle": [],
                        "last": "Villegas",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Babaeizadeh",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Taghi Saffar",
                        "suffix": ""
                    },
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Dumitru",
                        "middle": [],
                        "last": "Erhan",
                        "suffix": ""
                    },
                    {
                        "first": "Vittorio",
                        "middle": [],
                        "last": "Ferrari",
                        "suffix": ""
                    },
                    {
                        "first": "Pieter-Jan",
                        "middle": [],
                        "last": "Kindermans",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Voigtlaender",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "37",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emanuele Bugliarello, H Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan, Vittorio Ferrari, Pieter-Jan Kindermans, and Paul Voigtlaender. 2023. StoryBench: A Multifaceted Benchmark for Continuous Story Visualization. In Advances in Neural Information Processing Systems, Vol. 37. Curran Associates, Inc. https://arxiv.org/pdf/2308.11606.pdf",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "How interesting and coherent are the stories generated by a large-scale neural language model? Comparing human and automatic evaluations of machine-generated text",
                "authors": [
                    {
                        "first": "Dominic",
                        "middle": [],
                        "last": "Callan",
                        "suffix": ""
                    },
                    {
                        "first": "Jennifer",
                        "middle": [],
                        "last": "Foster",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Expert Systems",
                "volume": "40",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1111/exsy.13292"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dominic Callan and Jennifer Foster. 2023. How interesting and coherent are the stories generated by a large-scale neural language model? Comparing human and automatic evaluations of machine-generated text. Expert Systems 40, 6 (2023), e13292. https://doi.org/10.1111/exsy.13292",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
                "authors": [
                    {
                        "first": "Jo\u00e3o",
                        "middle": [],
                        "last": "Carreira",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zisserman",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "4724--4733",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2017.502"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jo\u00e3o Carreira and Andrew Zisserman. 2017. Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017. IEEE Computer Society, 4724-4733. https://doi.org/10. 1109/CVPR.2017.502",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Chateval: Towards better llm-based evaluators through multi-agent debate",
                "authors": [
                    {
                        "first": "Chi-Min",
                        "middle": [],
                        "last": "Chan",
                        "suffix": ""
                    },
                    {
                        "first": "Weize",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yusheng",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Jianxuan",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Xue",
                        "suffix": ""
                    },
                    {
                        "first": "Shanghang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2308.07201"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv:2308.07201 https://arxiv.org/abs/2308.07201",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "StoryER: Automatic Story Evaluation via Ranking, Rating and Reasoning",
                "authors": [
                    {
                        "first": "Hong",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Minh",
                        "middle": [],
                        "last": "Duc",
                        "suffix": ""
                    },
                    {
                        "first": "Hiroya",
                        "middle": [],
                        "last": "Vo",
                        "suffix": ""
                    },
                    {
                        "first": "Yusuke",
                        "middle": [],
                        "last": "Takamura",
                        "suffix": ""
                    },
                    {
                        "first": "Hideki",
                        "middle": [],
                        "last": "Miyao",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Nakayama",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1739--1753",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2022.EMNLP-MAIN.114"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hong Chen, Duc Minh Vo, Hiroya Takamura, Yusuke Miyao, and Hideki Nakayama. 2022. StoryER: Automatic Story Evaluation via Ranking, Rating and Reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 1739-1753. https://doi.org/10.18653/V1/2022.EMNLP-MAIN.114",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "TVStoryGen: A Dataset for Generating Stories with Character Descriptions",
                "authors": [
                    {
                        "first": "Mingda",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Gimpel",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2109.08833"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mingda Chen and Kevin Gimpel. 2022. TVStoryGen: A Dataset for Generating Stories with Character Descriptions. (2022). arXiv:2109.08833 https://arxiv.org/abs/2109.08833",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Neural storyboard artist: Visualizing stories with coherent image sequences",
                "authors": [
                    {
                        "first": "Shizhe",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Bei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Jianlong",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Ruihua",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Qin",
                        "middle": [],
                        "last": "Jin",
                        "suffix": ""
                    },
                    {
                        "first": "Pingping",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyu",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Chunting",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jin",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 27th ACM International Conference on Multimedia, MM 2019",
                "volume": "",
                "issue": "",
                "pages": "2236--2244",
                "other_ids": {
                    "DOI": [
                        "10.1145/3343031.3350571"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shizhe Chen, Bei Liu, Jianlong Fu, Ruihua Song, Qin Jin, Pingping Lin, Xiaoyu Qi, Chunting Wang, and Jin Zhou. 2019. Neural storyboard artist: Visualizing stories with coherent image sequences. In Proceedings of the 27th ACM International Conference on Multimedia, MM 2019, Nice, France, October 21-25, 2019. ACM, 2236-2244. https://doi.org/10.1145/3343031.3350571",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
                "authors": [
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Haiyun",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Shuming",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Ruifeng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 -Findings",
                "volume": "",
                "issue": "",
                "pages": "361--374",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.FINDINGS-IJCNLP.32"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. 2023. Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study. In Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 -Findings, Nusa Dua, Bali, November 1-4, 2023, Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, and Adila Alfa Krisnadhi (Eds.). Association for Computational Linguistics, 361-374. https://doi.org/10.18653/V1/2023.FINDINGS-IJCNLP.32",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate",
                "authors": [
                    {
                        "first": "Steffi",
                        "middle": [],
                        "last": "Chern",
                        "suffix": ""
                    },
                    {
                        "first": "Ethan",
                        "middle": [],
                        "last": "Chern",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2401.16788"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Steffi Chern, Ethan Chern, Graham Neubig, and Pengfei Liu. 2024. Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate. arXiv:2401.16788 https://arxiv.org/abs/2401.16788",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation",
                "authors": [
                    {
                        "first": "Cyril",
                        "middle": [],
                        "last": "Chhun",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Colombo",
                        "suffix": ""
                    },
                    {
                        "first": "Fabian",
                        "middle": [
                            "M"
                        ],
                        "last": "Suchanek",
                        "suffix": ""
                    },
                    {
                        "first": "Chlo\u00e9",
                        "middle": [],
                        "last": "Clavel",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [],
                        "last": "Nicoletta Calzolari",
                        "suffix": ""
                    },
                    {
                        "first": "Chu-Ren",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Hansaem",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Pustejovsky",
                        "suffix": ""
                    },
                    {
                        "first": "Leo",
                        "middle": [],
                        "last": "Wanner",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Key-Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Pum-Mo",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Hsin-Hsi",
                        "middle": [],
                        "last": "Ryu",
                        "suffix": ""
                    },
                    {
                        "first": "Lucia",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Donatelli",
                        "suffix": ""
                    },
                    {
                        "first": "Sadao",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kurohashi",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022",
                "volume": "",
                "issue": "",
                "pages": "5794--5836",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cyril Chhun, Pierre Colombo, Fabian M. Suchanek, and Chlo\u00e9 Clavel. 2022. Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation. In Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na (Eds.). International Committee on Computational Linguistics, 5794-5836. https://aclanthology.org/2022.coling-1.509",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation",
                "authors": [
                    {
                        "first": "Cyril",
                        "middle": [],
                        "last": "Chhun",
                        "suffix": ""
                    },
                    {
                        "first": "Fabian",
                        "middle": [
                            "M"
                        ],
                        "last": "Suchanek",
                        "suffix": ""
                    },
                    {
                        "first": "Chlo\u00e9",
                        "middle": [],
                        "last": "Clavel",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2405.13769"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Cyril Chhun, Fabian M. Suchanek, and Chlo\u00e9 Clavel. 2024. Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation. arXiv:2405.13769 https://arxiv.org/abs/2405.13769",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "A Closer Look into Using Large Language Models for Automatic Evaluation",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Cheng-Han",
                        "suffix": ""
                    },
                    {
                        "first": "Hung-Yi",
                        "middle": [],
                        "last": "Chiang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
                "volume": "",
                "issue": "",
                "pages": "8928--8942",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.FINDINGS-EMNLP.599"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "David Cheng-Han Chiang and Hung-yi Lee. 2023. A Closer Look into Using Large Language Models for Automatic Evaluation. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 8928-8942. https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.599",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text",
                "authors": [
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Tal",
                        "middle": [],
                        "last": "August",
                        "suffix": ""
                    },
                    {
                        "first": "Sofia",
                        "middle": [],
                        "last": "Serrano",
                        "suffix": ""
                    },
                    {
                        "first": "Nikita",
                        "middle": [],
                        "last": "Haduong",
                        "suffix": ""
                    },
                    {
                        "first": "Suchin",
                        "middle": [],
                        "last": "Gururangan",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
                "volume": "1",
                "issue": "",
                "pages": "7282--7296",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2021.ACL-LONG.565"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 7282-7296. https://doi.org/10.18653/V1/2021.ACL- LONG.565",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Sentence Mover's Similarity: Automatic Evaluation for Multi-Sentence Texts",
                "authors": [
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Asli",
                        "middle": [],
                        "last": "Celikyilmaz",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "2748--2760",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/P19-1264"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Elizabeth Clark, Asli Celikyilmaz, and Noah A. Smith. 2019. Sentence Mover's Similarity: Automatic Evaluation for Multi-Sentence Texts. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez (Eds.). Association for Computational Linguistics, 2748-2760. https: //doi.org/10.18653/V1/P19-1264",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Creative Writing with a Machine in the Loop: Case Studies on Slogans and Stories",
                "authors": [
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Anne",
                        "middle": [],
                        "last": "Spencer Ross",
                        "suffix": ""
                    },
                    {
                        "first": "Chenhao",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Yangfeng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 23rd International Conference on Intelligent User Interfaces, IUI 2018",
                "volume": "",
                "issue": "",
                "pages": "329--340",
                "other_ids": {
                    "DOI": [
                        "10.1145/3172944.3172983"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, and Noah A. Smith. 2018. Creative Writing with a Machine in the Loop: Case Studies on Slogans and Stories. In Proceedings of the 23rd International Conference on Intelligent User Interfaces, IUI 2018, Tokyo, Japan, March 07-11, 2018, Shlomo Berkovsky, Yoshinori Hijikata, Jun Rekimoto, Margaret M. Burnett, Mark Billinghurst, and Aaron Quigley (Eds.). ACM, 329-340. https://doi.org/10.1145/3172944.3172983",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Automatic Text Evaluation through the Lens of Wasserstein Barycenters",
                "authors": [
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Colombo",
                        "suffix": ""
                    },
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Staerman",
                        "suffix": ""
                    },
                    {
                        "first": "Chlo\u00e9",
                        "middle": [],
                        "last": "Clavel",
                        "suffix": ""
                    },
                    {
                        "first": "Pablo",
                        "middle": [],
                        "last": "Piantanida",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta Cana",
                "volume": "",
                "issue": "",
                "pages": "10450--10466",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.emnlp-main.817"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pierre Colombo, Guillaume Staerman, Chlo\u00e9 Clavel, and Pablo Piantanida. 2021. Automatic Text Evaluation through the Lens of Wasserstein Barycenters. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta Cana, Dominican Republic, 10450-10466. https://doi.org/10.18653/v1/2021.emnlp-main.817",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation",
                "authors": [
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Jean",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Colombo",
                        "suffix": ""
                    },
                    {
                        "first": "Chlo\u00e9",
                        "middle": [],
                        "last": "Clavel",
                        "suffix": ""
                    },
                    {
                        "first": "Pablo",
                        "middle": [],
                        "last": "Piantanida",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event",
                "volume": "",
                "issue": "",
                "pages": "10554--10562",
                "other_ids": {
                    "DOI": [
                        "10.1609/AAAI.V36I10.21299"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pierre Jean A. Colombo, Chlo\u00e9 Clavel, and Pablo Piantanida. 2022. InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 -March 1, 2022. AAAI Press, 10554-10562. https://doi.org/10.1609/AAAI.V36I10.21299",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Cross-lingual Language Model Pretraining",
                "authors": [
                    {
                        "first": "Alexis",
                        "middle": [],
                        "last": "Conneau",
                        "suffix": ""
                    },
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Lample",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Hanna",
                        "middle": [
                            "M"
                        ],
                        "last": "Wallach",
                        "suffix": ""
                    },
                    {
                        "first": "Hugo",
                        "middle": [],
                        "last": "Larochelle",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "7057--7067",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexis Conneau and Guillaume Lample. 2019. Cross-lingual Language Model Pretraining. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (Eds.). 7057-7067. https://proceedings.neurips.cc/ paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Relevance Feedback and Personalization: A Language Modeling Perspective",
                "authors": [
                    {
                        "first": "W",
                        "middle": [
                            "Bruce"
                        ],
                        "last": "Croft",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Cronen-Townsend",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Lavrenko",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the Second DELOS Network of Excellence Workshop on Personalisation and Recommender Systems in Digital Libraries",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "W. Bruce Croft, Stephen Cronen-Townsend, and Victor Lavrenko. 2001. Relevance Feedback and Personalization: A Language Modeling Perspective. In Proceedings of the Second DELOS Network of Excellence Workshop on Personalisation and Recommender Systems in Digital Libraries, DELOS 2001, Dublin, Ireland, June 18-20, 2001 (ERCIM Workshop Proceedings, Vol. 01/W03), Alan F. Smeaton and Jamie Callan (Eds.). ERCIM. http://www.ercim.org/publication/ws-proceedings/DelNoe02/croft-delos.pdf",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "The Tool for the Automatic Analysis of Cohesion 2.0: Integrating semantic similarity and text overlap",
                "authors": [
                    {
                        "first": "Kristopher",
                        "middle": [],
                        "last": "Scott A Crossley",
                        "suffix": ""
                    },
                    {
                        "first": "Mihai",
                        "middle": [],
                        "last": "Kyle",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dascalu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Behavior research methods",
                "volume": "51",
                "issue": "",
                "pages": "14--27",
                "other_ids": {
                    "DOI": [
                        "10.3758/s13428-018-1142-4"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Scott A Crossley, Kristopher Kyle, and Mihai Dascalu. 2019. The Tool for the Automatic Analysis of Cohesion 2.0: Integrating semantic similarity and text overlap. Behavior research methods 51 (2019), 14-27. https://doi.org/10.3758/s13428-018-1142-4",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "The tool for the automatic analysis of text cohesion (TAACO): Automatic assessment of local, global, and text cohesion",
                "authors": [
                    {
                        "first": "Kristopher",
                        "middle": [],
                        "last": "Scott A Crossley",
                        "suffix": ""
                    },
                    {
                        "first": "Danielle",
                        "middle": [
                            "S"
                        ],
                        "last": "Kyle",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mcnamara",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Behavior research methods",
                "volume": "48",
                "issue": "",
                "pages": "1227--1237",
                "other_ids": {
                    "DOI": [
                        "10.3758/s13428-015-0651-7"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Scott A Crossley, Kristopher Kyle, and Danielle S McNamara. 2016. The tool for the automatic analysis of text cohesion (TAACO): Automatic assessment of local, global, and text cohesion. Behavior research methods 48 (2016), 1227-1237. https://doi.org/10.3758/s13428-015-0651-7",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Google news personalization: scalable online collaborative filtering",
                "authors": [
                    {
                        "first": "Abhinandan",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Mayur",
                        "middle": [],
                        "last": "Datar",
                        "suffix": ""
                    },
                    {
                        "first": "Ashutosh",
                        "middle": [],
                        "last": "Garg",
                        "suffix": ""
                    },
                    {
                        "first": "Shyamsundar",
                        "middle": [],
                        "last": "Rajaram",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 16th International Conference on World Wide Web, WWW 2007",
                "volume": "",
                "issue": "",
                "pages": "271--280",
                "other_ids": {
                    "DOI": [
                        "10.1145/1242572.1242610"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyamsundar Rajaram. 2007. Google news personalization: scalable online collaborative filtering. In Proceedings of the 16th International Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007, Carey L. Williamson, Mary Ellen Zurko, Peter F. Patel-Schneider, and Prashant J. Shenoy (Eds.). ACM, 271-280. https://doi.org/10.1145/1242572.1242610",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "volume": "1",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/N19-1423"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). Association for Computational Linguistics, 4171-4186. https://doi.org/10.18653/V1/N19-1423",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models",
                "authors": [
                    {
                        "first": "Somayeh",
                        "middle": [],
                        "last": "Paramveer S Dhillon",
                        "suffix": ""
                    },
                    {
                        "first": "Jiaqi",
                        "middle": [],
                        "last": "Molaei",
                        "suffix": ""
                    },
                    {
                        "first": "Maximilian",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Shaochun",
                        "middle": [],
                        "last": "Golub",
                        "suffix": ""
                    },
                    {
                        "first": "Lionel",
                        "middle": [
                            "P"
                        ],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Robert",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2402.11723"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Paramveer S Dhillon, Somayeh Molaei, Jiaqi Li, Maximilian Golub, Shaochun Zheng, and Lionel P Robert. 2024. Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models. arXiv:2402.11723 https://arxiv.org/abs/2402.11723",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Enabling Language Models to Fill in the Blanks",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Donahue",
                        "suffix": ""
                    },
                    {
                        "first": "Mina",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "2492--2501",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.225"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chris Donahue, Mina Lee, and Percy Liang. 2020. Enabling Language Models to Fill in the Blanks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 2492-2501. https://doi.org/10.18653/v1/2020.acl-main.225",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "RAGAs: Automated Evaluation of Retrieval Augmented Generation",
                "authors": [
                    {
                        "first": "E",
                        "middle": [
                            "S"
                        ],
                        "last": "Shahul",
                        "suffix": ""
                    },
                    {
                        "first": "Jithin",
                        "middle": [],
                        "last": "James",
                        "suffix": ""
                    },
                    {
                        "first": "Luis",
                        "middle": [],
                        "last": "Espinosa Anke",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Schockaert",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Proceedings of the 18th Conference of the European Chapter",
                "volume": "",
                "issue": "",
                "pages": "150--158",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shahul ES, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. RAGAs: Automated Evaluation of Retrieval Augmented Generation. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 -System Demonstrations, St. Julians, Malta, March 17-22, 2024, Nikolaos Aletras and Orph\u00e9e De Clercq (Eds.). Association for Computational Linguistics, 150-158. https: //aclanthology.org/2024.eacl-demo.16",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Hierarchical neural story generation",
                "authors": [
                    {
                        "first": "Angela",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Yann",
                        "middle": [],
                        "last": "Dauphin",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018",
                "volume": "1",
                "issue": "",
                "pages": "889--898",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/P18-1082"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers. Association for Computational Linguistics, 889-898. https://doi.org/10.18653/V1/P18-1082",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Strategies for Structuring Story Generation",
                "authors": [
                    {
                        "first": "Angela",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Yann",
                        "middle": [
                            "N"
                        ],
                        "last": "Dauphin",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
                "volume": "1",
                "issue": "",
                "pages": "2650--2660",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/P19-1254"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Angela Fan, Mike Lewis, and Yann N. Dauphin. 2019. Strategies for Structuring Story Generation. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez (Eds.). Association for Computational Linguistics, 2650-2660. https://doi.org/10.18653/V1/P19-1254",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "BLEU might be Guilty but References are not Innocent",
                "authors": [
                    {
                        "first": "Markus",
                        "middle": [],
                        "last": "Freitag",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    },
                    {
                        "first": "Isaac",
                        "middle": [],
                        "last": "Caswell",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "61--71",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.EMNLP-MAIN.5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Markus Freitag, David Grangier, and Isaac Caswell. 2020. BLEU might be Guilty but References are not Innocent. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 61-71. https://doi.org/10.18653/V1/2020.EMNLP-MAIN.5",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Gptscore: Evaluate as you desire",
                "authors": [
                    {
                        "first": "Jinlan",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "See-Kiong",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengbao",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2302.04166"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. (2023). arXiv:2302.04166 https://arxiv.org/abs/ 2302.04166",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "LLM-based NLG Evaluation: Current Status and Challenges",
                "authors": [
                    {
                        "first": "Mingqi",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Xinyu",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Ruan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Pu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2402.01383"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun Wan. 2024. LLM-based NLG Evaluation: Current Status and Challenges. arXiv:2402.01383 https://arxiv.org/abs/2402.01383",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation",
                "authors": [
                    {
                        "first": "Zixi",
                        "middle": [],
                        "last": "Sarik Ghazarian",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "M"
                        ],
                        "last": "Akash",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [
                            "M"
                        ],
                        "last": "Weischedel",
                        "suffix": ""
                    },
                    {
                        "first": "Aram",
                        "middle": [],
                        "last": "Galstyan",
                        "suffix": ""
                    },
                    {
                        "first": "Nanyun",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
                "volume": "",
                "issue": "",
                "pages": "4334--4344",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2021.NAACL-MAIN.343"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sarik Ghazarian, Zixi Liu, Akash SM, Ralph M. Weischedel, Aram Galstyan, and Nanyun Peng. 2021. Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T\u00fcr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, 4334-4344. https://doi.org/10.18653/V1/2021.NAACL-MAIN.343",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Better automatic evaluation of open-domain dialogue systems with contextualized embeddings",
                "authors": [
                    {
                        "first": "Johnny Tian-Zheng",
                        "middle": [],
                        "last": "Sarik Ghazarian",
                        "suffix": ""
                    },
                    {
                        "first": "Aram",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Nanyun",
                        "middle": [],
                        "last": "Galstyan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.10635"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sarik Ghazarian, Johnny Tian-Zheng Wei, Aram Galstyan, and Nanyun Peng. 2019. Better automatic evaluation of open-domain dialogue systems with contextualized embeddings. arXiv:1904.10635 https://arxiv.org/abs/1904.10635",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Content Planning for Neural Story Generation with Aristotelian Rescoring",
                "authors": [
                    {
                        "first": "Seraphina",
                        "middle": [],
                        "last": "Goldfarb-Tarrant",
                        "suffix": ""
                    },
                    {
                        "first": "Tuhin",
                        "middle": [],
                        "last": "Chakrabarty",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [
                            "M"
                        ],
                        "last": "Weischedel",
                        "suffix": ""
                    },
                    {
                        "first": "Nanyun",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "4319--4338",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.EMNLP-MAIN.351"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph M. Weischedel, and Nanyun Peng. 2020. Content Planning for Neural Story Generation with Aristotelian Rescoring. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 4319-4338. https://doi.org/10.18653/V1/2020.EMNLP-MAIN.351",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation",
                "authors": [
                    {
                        "first": "Seraphina",
                        "middle": [],
                        "last": "Goldfarb-Tarrant",
                        "suffix": ""
                    },
                    {
                        "first": "Haining",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Nanyun",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "volume": "",
                "issue": "",
                "pages": "89--97",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/N19-4016"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Seraphina Goldfarb-Tarrant, Haining Feng, and Nanyun Peng. 2019. Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Demonstrations, Waleed Ammar, Annie Louis, and Nasrin Mostafazadeh (Eds.). Association for Computational Linguistics, 89-97. https://doi.org/10.18653/V1/N19-4016",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing",
                "authors": [
                    {
                        "first": "Carlos",
                        "middle": [],
                        "last": "G\u00f3mez",
                        "suffix": ""
                    },
                    {
                        "first": "-Rodr\u00edguez",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Williams",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
                "volume": "",
                "issue": "",
                "pages": "14504--14528",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.FINDINGS-EMNLP.966"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Carlos G\u00f3mez-Rodr\u00edguez and Paul Williams. 2023. A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 14504-14528. https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.966",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "CoAScore: Chain-of-Aspects Prompting for NLG Evaluation",
                "authors": [
                    {
                        "first": "Peiyuan",
                        "middle": [],
                        "last": "Gong",
                        "suffix": ""
                    },
                    {
                        "first": "Jiaxin",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2312.10355"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Peiyuan Gong and Jiaxin Mao. 2023. CoAScore: Chain-of-Aspects Prompting for NLG Evaluation. (2023). arXiv:2312.10355 https://arxiv.org/abs/ 2312.10355",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Generative Adversarial Nets",
                "authors": [
                    {
                        "first": "Ian",
                        "middle": [
                            "J"
                        ],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [],
                        "last": "Pouget-Abadie",
                        "suffix": ""
                    },
                    {
                        "first": "Mehdi",
                        "middle": [],
                        "last": "Mirza",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Warde-Farley",
                        "suffix": ""
                    },
                    {
                        "first": "Sherjil",
                        "middle": [],
                        "last": "Ozair",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [
                            "C"
                        ],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "2672--2680",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (Eds.). 2672-2680. https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "TeViS: Translating Text Synopses to Video Storyboards",
                "authors": [
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuchong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Feiyue",
                        "middle": [],
                        "last": "Ni",
                        "suffix": ""
                    },
                    {
                        "first": "Shizhe",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Xihua",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ruihua",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Boyuan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Xiang",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 31st ACM International Conference on Multimedia, MM 2023",
                "volume": "",
                "issue": "",
                "pages": "4968--4979",
                "other_ids": {
                    "DOI": [
                        "10.1145/3581783.3612417"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xu Gu, Yuchong Sun, Feiyue Ni, Shizhe Chen, Xihua Wang, Ruihua Song, Boyuan Li, and Xiang Cao. 2023. TeViS: Translating Text Synopses to Video Storyboards. In Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 2023-3 November 2023. ACM, 4968-4979. https://doi.org/10.1145/3581783.3612417",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation",
                "authors": [
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Guan",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoer",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Yamei",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ruilin",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoxi",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Changjie",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Trans. Assoc. Comput. Linguistics",
                "volume": "10",
                "issue": "",
                "pages": "434--451",
                "other_ids": {
                    "DOI": [
                        "10.1162/TACL_A_00469"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jian Guan, Zhuoer Feng, Yamei Chen, Ruilin He, Xiaoxi Mao, Changjie Fan, and Minlie Huang. 2022. LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation. Trans. Assoc. Comput. Linguistics 10 (2022), 434-451. https://doi.org/10.1162/TACL_A_00469",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation",
                "authors": [
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Guan",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhihao",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Trans. Assoc. Comput. Linguistics",
                "volume": "8",
                "issue": "",
                "pages": "93--108",
                "other_ids": {
                    "DOI": [
                        "10.1162/TACL_A_00302"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jian Guan, Fei Huang, Minlie Huang, Zhihao Zhao, and Xiaoyan Zhu. 2020. A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation. Trans. Assoc. Comput. Linguistics 8 (2020), 93-108. https://doi.org/10.1162/TACL_A_00302",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation",
                "authors": [
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Guan",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "9157--9166",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.EMNLP-MAIN.736"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jian Guan and Minlie Huang. 2020. UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 9157-9166. https://doi.org/10.18653/V1/2020.EMNLP-MAIN.736",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics",
                "authors": [
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Guan",
                        "suffix": ""
                    },
                    {
                        "first": "Zhexin",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoer",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Zitao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Wenbiao",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoxi",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Changjie",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
                "volume": "1",
                "issue": "",
                "pages": "6394--6407",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2021.ACL-LONG.500"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie Huang. 2021. OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 6394-6407. https: //doi.org/10.18653/V1/2021.ACL-LONG.500",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Imagine This! Scripts to Compositions to Videos",
                "authors": [
                    {
                        "first": "Tanmay",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Dustin",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Farhadi",
                        "suffix": ""
                    },
                    {
                        "first": "Derek",
                        "middle": [],
                        "last": "Hoiem",
                        "suffix": ""
                    },
                    {
                        "first": "Aniruddha",
                        "middle": [],
                        "last": "Kembhavi",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Computer Vision -ECCV 2018 -15th European Conference",
                "volume": "11212",
                "issue": "",
                "pages": "610--626",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-030-01237-3_37"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. 2018. Imagine This! Scripts to Compositions to Videos. In Computer Vision -ECCV 2018 -15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VIII (Lecture Notes in Computer Science, Vol. 11212), Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (Eds.). Springer, 610-626. https://doi.org/10.1007/978-3- 030-01237-3_37",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Envisioning Narrative Intelligence: A Creative Visual Storytelling Anthology",
                "authors": [
                    {
                        "first": "Brett",
                        "middle": [
                            "A"
                        ],
                        "last": "Halperin",
                        "suffix": ""
                    },
                    {
                        "first": "Stephanie",
                        "middle": [
                            "M"
                        ],
                        "last": "Lukin",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
                "volume": "240",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1145/3544548.3580744"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Brett A. Halperin and Stephanie M. Lukin. 2023. Envisioning Narrative Intelligence: A Creative Visual Storytelling Anthology. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI '23). Association for Computing Machinery, New York, NY, USA, Article 240, 21 pages. https://doi.org/10.1145/3544548.3580744",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Hayes",
                        "suffix": ""
                    },
                    {
                        "first": "Songyang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xi",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Guan",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Sasha",
                        "middle": [],
                        "last": "Sheng",
                        "suffix": ""
                    },
                    {
                        "first": "Harry",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Songwei",
                        "middle": [],
                        "last": "Ge",
                        "suffix": ""
                    },
                    {
                        "first": "Qiyuan",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Computer Vision -ECCV 2022 -17th European Conference",
                "volume": "13668",
                "issue": "",
                "pages": "431--449",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-031-20074-8_25"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Thomas Hayes, Songyang Zhang, Xi Yin, Guan Pang, Sasha Sheng, Harry Yang, Songwei Ge, Qiyuan Hu, and Devi Parikh. 2022. MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration. In Computer Vision -ECCV 2022 -17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII (Lecture Notes in Computer Science, Vol. 13668), Shai Avidan, Gabriel J. Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, and Tal Hassner (Eds.). Springer, 431-449. https://doi.org/10.1007/978-3-031-20074-8_25",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
                "authors": [
                    {
                        "first": "Tianxing",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jingyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Tianle",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Sachin",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "R"
                        ],
                        "last": "Glass",
                        "suffix": ""
                    },
                    {
                        "first": "Yulia",
                        "middle": [],
                        "last": "Tsvetkov",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "12067--12097",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.ACL-LONG.674"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James R. Glass, and Yulia Tsvetkov. 2023. On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 12067-12097. https://doi.org/10.18653/V1/2023.ACL-LONG.674",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Teaching Machines to Read and Comprehend",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Moritz Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Tom\u00e1s",
                        "middle": [],
                        "last": "Kocisk\u00fd",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Lasse",
                        "middle": [],
                        "last": "Espeholt",
                        "suffix": ""
                    },
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Kay",
                        "suffix": ""
                    },
                    {
                        "first": "Mustafa",
                        "middle": [],
                        "last": "Suleyman",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "1693--1701",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karl Moritz Hermann, Tom\u00e1s Kocisk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching Machines to Read and Comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (Eds.). 1693-1701. https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
                "authors": [
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Hessel",
                        "suffix": ""
                    },
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Holtzman",
                        "suffix": ""
                    },
                    {
                        "first": "Maxwell",
                        "middle": [],
                        "last": "Forbes",
                        "suffix": ""
                    },
                    {
                        "first": "Ronan",
                        "middle": [],
                        "last": "Le Bras",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana",
                "volume": "",
                "issue": "",
                "pages": "7514--7528",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2021.EMNLP-MAIN.595"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image Captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, 7514-7528. https://doi.org/10.18653/V1/2021.EMNLP-MAIN.595",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                "authors": [
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Heusel",
                        "suffix": ""
                    },
                    {
                        "first": "Hubert",
                        "middle": [],
                        "last": "Ramsauer",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Unterthiner",
                        "suffix": ""
                    },
                    {
                        "first": "Bernhard",
                        "middle": [],
                        "last": "Nessler",
                        "suffix": ""
                    },
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "6626--6637",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 6626-6637. https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d- Abstract.html",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "The goldilocks principle: Reading children's books with explicit memory representations",
                "authors": [
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "4th International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2016. The goldilocks principle: Reading children's books with explicit memory representations. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. http://arxiv.org/abs/1511.02301",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences",
                "authors": [
                    {
                        "first": "Xudong",
                        "middle": [],
                        "last": "Hong",
                        "suffix": ""
                    },
                    {
                        "first": "Asad",
                        "middle": [
                            "B"
                        ],
                        "last": "Sayeed",
                        "suffix": ""
                    },
                    {
                        "first": "Khushboo",
                        "middle": [],
                        "last": "Mehra",
                        "suffix": ""
                    },
                    {
                        "first": "Vera",
                        "middle": [],
                        "last": "Demberg",
                        "suffix": ""
                    },
                    {
                        "first": "Bernt",
                        "middle": [],
                        "last": "Schiele",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Trans. Assoc. Comput. Linguistics",
                "volume": "11",
                "issue": "",
                "pages": "565--581",
                "other_ids": {
                    "DOI": [
                        "10.1162/TACL_A_00553"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xudong Hong, Asad B. Sayeed, Khushboo Mehra, Vera Demberg, and Bernt Schiele. 2023. Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences. Trans. Assoc. Comput. Linguistics 11 (2023), 565-581. https://doi.org/10.1162/TACL_A_00553",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions",
                "authors": [
                    {
                        "first": "Anya",
                        "middle": [],
                        "last": "David M Howcroft",
                        "suffix": ""
                    },
                    {
                        "first": "Miruna",
                        "middle": [],
                        "last": "Belz",
                        "suffix": ""
                    },
                    {
                        "first": "Dimitra",
                        "middle": [],
                        "last": "Clinciu",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gkatzia",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Sadid",
                        "suffix": ""
                    },
                    {
                        "first": "Saad",
                        "middle": [],
                        "last": "Hasan",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Mahamood",
                        "suffix": ""
                    },
                    {
                        "first": "Emiel",
                        "middle": [],
                        "last": "Mille",
                        "suffix": ""
                    },
                    {
                        "first": "Sashank",
                        "middle": [],
                        "last": "Van Miltenburg",
                        "suffix": ""
                    },
                    {
                        "first": "Verena",
                        "middle": [],
                        "last": "Santhanam",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Rieser",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 13th International Conference on Natural Language Generation, INLG 2020",
                "volume": "",
                "issue": "",
                "pages": "169--182",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.INLG-1.23"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "David M Howcroft, Anya Belz, Miruna Clinciu, Dimitra Gkatzia, Sadid A Hasan, Saad Mahamood, Simon Mille, Emiel Van Miltenburg, Sashank Santhanam, and Verena Rieser. 2020. Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions. In Proceedings of the 13th International Conference on Natural Language Generation, INLG 2020, Dublin, Ireland, December 15-18, 2020. Association for Computational Linguistics, 169-182. https://doi.org/10.18653/V1/2020.INLG-1.23",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "Learning to Rank Visual Stories From Human Ranking Data",
                "authors": [
                    {
                        "first": "Chi-Yang",
                        "middle": [],
                        "last": "Hsu",
                        "suffix": ""
                    },
                    {
                        "first": "Yun-Wei",
                        "middle": [],
                        "last": "Chu",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Kuan-Chieh",
                        "middle": [],
                        "last": "Lo",
                        "suffix": ""
                    },
                    {
                        "first": "Chacha",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ting-Hao",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Lun-Wei",
                        "middle": [],
                        "last": "Ku",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "6365--6378",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2022.ACL-LONG.441"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chi-Yang Hsu, Yun-Wei Chu, Vincent Chen, Kuan-Chieh Lo, Chacha Chen, Ting-Hao Huang, and Lun-Wei Ku. 2022. Learning to Rank Visual Stories From Human Ranking Data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 6365-6378. https://doi.org/10.18653/V1/2022.ACL-LONG.441",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "Are LLM-based Evaluators Confusing NLG Quality Criteria? (2024)",
                "authors": [
                    {
                        "first": "Xinyu",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Mingqi",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Sen",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yicheng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Teng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2402.12055"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, and Xiaojun Wan. 2024. Are LLM-based Evaluators Confusing NLG Quality Criteria? (2024). arXiv:2402.12055 https://arxiv.org/abs/2402.12055",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4",
                "authors": [
                    {
                        "first": "Yebowen",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Kaiqiang",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Sangwoo",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Hassan",
                        "middle": [],
                        "last": "Foroosh",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023",
                "volume": "",
                "issue": "",
                "pages": "8344--8357",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.EMNLP-MAIN.519"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, and Fei Liu. 2023. DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 8344-8357. https://doi.org/10.18653/V1/2023.EMNLP-MAIN.519",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
                "authors": [
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Weijiang",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Weitao",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Weihong",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Zhangyin",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Haotian",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Qianglong",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Weihua",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaocheng",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2311.05232"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv:2311.05232 https://arxiv.org/abs/2311.05232",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "Visual Storytelling",
                "authors": [
                    {
                        "first": "Ting-Hao ;",
                        "middle": [],
                        "last": "Kenneth",
                        "suffix": ""
                    },
                    {
                        "first": ")",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Francis",
                        "middle": [],
                        "last": "Ferraro",
                        "suffix": ""
                    },
                    {
                        "first": "Nasrin",
                        "middle": [],
                        "last": "Mostafazadeh",
                        "suffix": ""
                    },
                    {
                        "first": "Ishan",
                        "middle": [],
                        "last": "Misra",
                        "suffix": ""
                    },
                    {
                        "first": "Aishwarya",
                        "middle": [],
                        "last": "Agrawal",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ross",
                        "middle": [
                            "B"
                        ],
                        "last": "Girshick",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Pushmeet",
                        "middle": [],
                        "last": "Kohli",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "Lawrence"
                        ],
                        "last": "Zitnick",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    },
                    {
                        "first": "Lucy",
                        "middle": [],
                        "last": "Vanderwende",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Margaret",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "1233--1239",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/N16-1147"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ting-Hao (Kenneth) Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross B. Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel Galley, and Margaret Mitchell. 2016. Visual Storytelling. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, Kevin Knight, Ani Nenkova, and Owen Rambow (Eds.). The Association for Computational Linguistics, 1233-1239. https://doi.org/10.18653/V1/N16-1147",
                "links": null
            },
            "BIBREF69": {
                "ref_id": "b69",
                "title": "Unsupervised hierarchical story infilling",
                "authors": [
                    {
                        "first": "Daphne",
                        "middle": [],
                        "last": "Ippolito",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Douglas",
                        "middle": [],
                        "last": "Eck",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the First Workshop on Narrative Understanding",
                "volume": "",
                "issue": "",
                "pages": "37--43",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W19-2405"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Daphne Ippolito, David Grangier, Chris Callison-Burch, and Douglas Eck. 2019. Unsupervised hierarchical story infilling. In Proceedings of the First Workshop on Narrative Understanding. Association for Computational Linguistics, Minneapolis, Minnesota, 37-43. https://doi.org/10.18653/v1/W19- 2405",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "Creative Writing with an AI-Powered Writing Assistant: Perspectives from Professional Writers",
                "authors": [
                    {
                        "first": "Daphne",
                        "middle": [],
                        "last": "Ippolito",
                        "suffix": ""
                    },
                    {
                        "first": "Ann",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Coenen",
                        "suffix": ""
                    },
                    {
                        "first": "Sehmon",
                        "middle": [],
                        "last": "Burnam",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.48550/ARXIV.2211.05030"
                    ],
                    "arXiv": [
                        "arXiv:2211.05030"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Daphne Ippolito, Ann Yuan, Andy Coenen, and Sehmon Burnam. 2022. Creative Writing with an AI-Powered Writing Assistant: Perspectives from Professional Writers. CoRR abs/2211.05030 (2022). https://doi.org/10.48550/ARXIV.2211.05030 arXiv:2211.05030",
                "links": null
            },
            "BIBREF71": {
                "ref_id": "b71",
                "title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning",
                "authors": [
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Jain",
                        "suffix": ""
                    },
                    {
                        "first": "Vaishakh",
                        "middle": [],
                        "last": "Keshava",
                        "suffix": ""
                    },
                    {
                        "first": "Mysore",
                        "middle": [],
                        "last": "Swarnashree",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Sathyendra",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Fernandes",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Chunting",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
                "volume": "",
                "issue": "",
                "pages": "8487--8495",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.FINDINGS-ACL.537"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. 2023. Multi-Dimensional Evaluation of Text Summarization with In-Context Learning. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 8487-8495. https://doi.org/10.18653/V1/2023.FINDINGS-ACL.537",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "Transmedia storytelling and entertainment: An annotated syllabus",
                "authors": [
                    {
                        "first": "Henry",
                        "middle": [],
                        "last": "Jenkins",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Entertainment Industries. Routledge",
                "volume": "",
                "issue": "",
                "pages": "145--160",
                "other_ids": {
                    "DOI": [
                        "10.1080/10304312.2010.510599"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Henry Jenkins. 2014. Transmedia storytelling and entertainment: An annotated syllabus. In Entertainment Industries. Routledge, 145-160. https://doi.org/10.1080/10304312.2010.510599",
                "links": null
            },
            "BIBREF73": {
                "ref_id": "b73",
                "title": "TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks",
                "authors": [
                    {
                        "first": "Dongfu",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Yishan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Ge",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhao",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Yuchen Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.48550/ARXIV.2310.00752"
                    ],
                    "arXiv": [
                        "arXiv:2310.00752"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. 2023. TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks. CoRR abs/2310.00752 (2023). https://doi.org/10.48550/ARXIV.2310.00752 arXiv:2310.00752",
                "links": null
            },
            "BIBREF74": {
                "ref_id": "b74",
                "title": "The role of storytelling in advertising: Consumer emotion, narrative engagement level, and word-of-mouth intention",
                "authors": [
                    {
                        "first": "Jin-Ae",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    },
                    {
                        "first": "Sookyeong",
                        "middle": [],
                        "last": "Hong",
                        "suffix": ""
                    },
                    {
                        "first": "Glenn",
                        "middle": [],
                        "last": "Hubbard",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Journal of Consumer Behaviour",
                "volume": "19",
                "issue": "01 2020",
                "pages": "47--56",
                "other_ids": {
                    "DOI": [
                        "10.1002/cb.1793"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jin-Ae Kang, Sookyeong Hong, and Glenn Hubbard. 2020. The role of storytelling in advertising: Consumer emotion, narrative engagement level, and word-of-mouth intention. Journal of Consumer Behaviour 19 (01 2020), 47-56. https://doi.org/10.1002/cb.1793",
                "links": null
            },
            "BIBREF75": {
                "ref_id": "b75",
                "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large misc Model Generation",
                "authors": [
                    {
                        "first": "Pei",
                        "middle": [],
                        "last": "Ke",
                        "suffix": ""
                    },
                    {
                        "first": "Bosi",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoer",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Xuanyu",
                        "middle": [],
                        "last": "Lei",
                        "suffix": ""
                    },
                    {
                        "first": "Jiale",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Shengyuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Aohan",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxiao",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Hongning",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2311.18702"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. 2024. CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large misc Model Generation. arXiv:2311.18702 https://arxiv.org/abs/2311.18702",
                "links": null
            },
            "BIBREF76": {
                "ref_id": "b76",
                "title": "CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation",
                "authors": [
                    {
                        "first": "Pei",
                        "middle": [],
                        "last": "Ke",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Yankai",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "2306--2319",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2022.ACL-LONG.164"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pei Ke, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou, Xiaoyan Zhu, and Minlie Huang. 2022. CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 2306-2319. https://doi.org/10.18653/V1/2022.ACL-LONG.164",
                "links": null
            },
            "BIBREF77": {
                "ref_id": "b77",
                "title": "A new measure of rank correlation",
                "authors": [
                    {
                        "first": "Maurice G",
                        "middle": [],
                        "last": "Kendall",
                        "suffix": ""
                    }
                ],
                "year": 1938,
                "venue": "Biometrika",
                "volume": "30",
                "issue": "",
                "pages": "81--93",
                "other_ids": {
                    "DOI": [
                        "10.2307/2332226"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika 30, 1/2 (1938), 81-93. https://doi.org/10.2307/2332226",
                "links": null
            },
            "BIBREF78": {
                "ref_id": "b78",
                "title": "OpenKiwi: An Open Source Framework for Quality Estimation",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "F\u00e1bio",
                        "suffix": ""
                    },
                    {
                        "first": "Jonay",
                        "middle": [],
                        "last": "Kepler",
                        "suffix": ""
                    },
                    {
                        "first": "Marcos",
                        "middle": [
                            "V"
                        ],
                        "last": "Tr\u00e9nous",
                        "suffix": ""
                    },
                    {
                        "first": "Miguel",
                        "middle": [],
                        "last": "Treviso",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Vera",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [
                            "T"
                        ],
                        "last": "Andr\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Martins",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
                "volume": "3",
                "issue": "",
                "pages": "117--122",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/P19-3020"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "F\u00e1bio N. Kepler, Jonay Tr\u00e9nous, Marcos V. Treviso, Miguel Vera, and Andr\u00e9 F. T. Martins. 2019. OpenKiwi: An Open Source Framework for Quality Estimation. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28 -August 2, 2019, Volume 3: System Demonstrations, Marta R. Costa-juss\u00e0 and Enrique Alfonseca (Eds.). Association for Computational Linguistics, 117-122. https://doi.org/10.18653/V1/P19-3020",
                "links": null
            },
            "BIBREF79": {
                "ref_id": "b79",
                "title": "Which is better? Exploring Prompting Strategy For LLM-based Metrics",
                "authors": [
                    {
                        "first": "Joonghoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Sangmin",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Seung",
                        "middle": [],
                        "last": "Hun Han",
                        "suffix": ""
                    },
                    {
                        "first": "Saeran",
                        "middle": [],
                        "last": "Park",
                        "suffix": ""
                    },
                    {
                        "first": "Jiyoon",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kiyoon",
                        "middle": [],
                        "last": "Jeong",
                        "suffix": ""
                    },
                    {
                        "first": "Pilsung",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, Eval4NLP 2023",
                "volume": "",
                "issue": "",
                "pages": "164--183",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.EVAL4NLP-1.14"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Joonghoon Kim, Sangmin Lee, Seung Hun Han, Saeran Park, Jiyoon Lee, Kiyoon Jeong, and Pilsung Kang. 2023. Which is better? Exploring Prompting Strategy For LLM-based Metrics. In Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, Eval4NLP 2023, Bali, Indonesia, November 1, 2023, Daniel Deutsch, Rotem Dror, Steffen Eger, Yang Gao, Christoph Leiter, Juri Opitz, and Andreas R\u00fcckl\u00e9 (Eds.). Association for Computational Linguistics, 164-183. https://doi.org/10.18653/V1/2023.EVAL4NLP-1.14",
                "links": null
            },
            "BIBREF80": {
                "ref_id": "b80",
                "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
                "authors": [
                    {
                        "first": "Seungone",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Jamin",
                        "middle": [],
                        "last": "Shin",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Joel",
                        "middle": [],
                        "last": "Jang",
                        "suffix": ""
                    },
                    {
                        "first": "Shayne",
                        "middle": [],
                        "last": "Longpre",
                        "suffix": ""
                    },
                    {
                        "first": "Hwaran",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sangdoo",
                        "middle": [],
                        "last": "Yun",
                        "suffix": ""
                    },
                    {
                        "first": "Seongjin",
                        "middle": [],
                        "last": "Shin",
                        "suffix": ""
                    },
                    {
                        "first": "Sungdong",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Thorne",
                        "suffix": ""
                    },
                    {
                        "first": "Minjoon",
                        "middle": [],
                        "last": "Seo",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.48550/ARXIV.2310.08491"
                    ],
                    "arXiv": [
                        "arXiv:2310.08491"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2023. Prometheus: Inducing Fine-grained Evaluation Capability in Language Models. CoRR abs/2310.08491 (2023). https: //doi.org/10.48550/ARXIV.2310.08491 arXiv:2310.08491",
                "links": null
            },
            "BIBREF81": {
                "ref_id": "b81",
                "title": "Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
                "authors": [
                    {
                        "first": "Seungone",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Juyoung",
                        "middle": [],
                        "last": "Suk",
                        "suffix": ""
                    },
                    {
                        "first": "Shayne",
                        "middle": [],
                        "last": "Longpre",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Yuchen Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Jamin",
                        "middle": [],
                        "last": "Shin",
                        "suffix": ""
                    },
                    {
                        "first": "Sean",
                        "middle": [],
                        "last": "Welleck",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    },
                    {
                        "first": "Moontae",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kyungjae",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Minjoon",
                        "middle": [],
                        "last": "Seo",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2405.01535[cs.CL"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024. Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models. arXiv:2405.01535 [cs.CL] https://arxiv.org/abs/2405.01535",
                "links": null
            },
            "BIBREF82": {
                "ref_id": "b82",
                "title": "EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria",
                "authors": [
                    {
                        "first": "Tae",
                        "middle": [],
                        "last": "Soo",
                        "suffix": ""
                    },
                    {
                        "first": "Kim",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Yoonjoo",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Jamin",
                        "middle": [],
                        "last": "Shin",
                        "suffix": ""
                    },
                    {
                        "first": "Young-Ho",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Juho",
                        "middle": [],
                        "last": "Kim ; Florian 'floyd' Mueller",
                        "suffix": ""
                    },
                    {
                        "first": "Penny",
                        "middle": [],
                        "last": "Kyburz",
                        "suffix": ""
                    },
                    {
                        "first": "Julie",
                        "middle": [
                            "R"
                        ],
                        "last": "Williamson",
                        "suffix": ""
                    },
                    {
                        "first": "Corina",
                        "middle": [],
                        "last": "Sas",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [
                            "L"
                        ],
                        "last": "Wilson",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024",
                "volume": "306",
                "issue": "",
                "pages": "1--306",
                "other_ids": {
                    "DOI": [
                        "10.1145/3613904.3642216"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, and Juho Kim. 2024. EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024, Honolulu, HI, USA, May 11-16, 2024, Florian 'Floyd' Mueller, Penny Kyburz, Julie R. Williamson, Corina Sas, Max L. Wilson, Phoebe O. Toups Dugas, and Irina Shklovski (Eds.). ACM, 306:1-306:21. https://doi.org/10.1145/3613904.3642216",
                "links": null
            },
            "BIBREF83": {
                "ref_id": "b83",
                "title": "Auto-Encoding Variational Bayes",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Diederik",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Kingma",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "2nd International Conference on Learning Representations, ICLR 2014",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1312.6114",
                "links": null
            },
            "BIBREF84": {
                "ref_id": "b84",
                "title": "Stylized story generation with style-guided planning",
                "authors": [
                    {
                        "first": "Xiangzhe",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Jialiang",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Ziquan",
                        "middle": [],
                        "last": "Tung",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Guan",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021",
                "volume": "",
                "issue": "",
                "pages": "2430--2436",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2021.FINDINGS-ACL.215"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xiangzhe Kong, Jialiang Huang, Ziquan Tung, Jian Guan, and Minlie Huang. 2021. Stylized story generation with style-guided planning. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 (Findings of ACL, Vol. ACL/IJCNLP 2021). Association for Computational Linguistics, 2430-2436. https://doi.org/10.18653/V1/2021.FINDINGS-ACL.215",
                "links": null
            },
            "BIBREF85": {
                "ref_id": "b85",
                "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Krause",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Ranjay",
                        "middle": [],
                        "last": "Krishna",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Fei-Fei",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "3337--3345",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2017.356"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. 2017. A Hierarchical Approach for Generating Descriptive Image Paragraphs. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017. IEEE Computer Society, 3337-3345. https://doi.org/10.1109/CVPR.2017.356",
                "links": null
            },
            "BIBREF86": {
                "ref_id": "b86",
                "title": "From Word Embeddings To Document Distances",
                "authors": [
                    {
                        "first": "Matt",
                        "middle": [
                            "J"
                        ],
                        "last": "Kusner",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [
                            "I"
                        ],
                        "last": "Kolkin",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [
                            "Q"
                        ],
                        "last": "Weinberger",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015",
                "volume": "37",
                "issue": "",
                "pages": "957--966",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From Word Embeddings To Document Distances. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 (JMLR Workshop and Conference Proceedings, Vol. 37), Francis R. Bach and David M. Blei (Eds.). JMLR.org, 957-966. http://proceedings.mlr.press/v37/kusnerb15.html",
                "links": null
            },
            "BIBREF87": {
                "ref_id": "b87",
                "title": "Probabilistic text structuring: Experiments with sentence ordering",
                "authors": [
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "545--552",
                "other_ids": {
                    "DOI": [
                        "10.3115/1075096.1075165"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics. 545-552. https://doi.org/10.3115/1075096.1075165",
                "links": null
            },
            "BIBREF88": {
                "ref_id": "b88",
                "title": "CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities",
                "authors": [
                    {
                        "first": "Mina",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Qian",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "CHI '22: CHI Conference on Human Factors in Computing Systems",
                "volume": "388",
                "issue": "",
                "pages": "1--388",
                "other_ids": {
                    "DOI": [
                        "10.1145/3491102.3502030"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mina Lee, Percy Liang, and Qian Yang. 2022. CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities. In CHI '22: CHI Conference on Human Factors in Computing Systems, New Orleans, LA, USA, 29 April 2022 -5 May 2022, Simone D. J. Barbosa, Cliff Lampe, Caroline Appert, David A. Shamma, Steven Mark Drucker, Julie R. Williamson, and Koji Yatani (Eds.). ACM, 388:1-388:19. https://doi.org/10.1145/3491102.3502030",
                "links": null
            },
            "BIBREF89": {
                "ref_id": "b89",
                "title": "CheckEval: Robust Evaluation Framework using Large Language Model via Checklist",
                "authors": [
                    {
                        "first": "Yukyung",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Joonghoon",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Jaehee",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Hyowon",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Pilsung",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2403.18771"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, and Pilsung Kang. 2024. CheckEval: Robust Evaluation Framework using Large Language Model via Checklist. (2024). arXiv:2403.18771 https://arxiv.org/abs/2403.18771",
                "links": null
            },
            "BIBREF90": {
                "ref_id": "b90",
                "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Marjan",
                        "middle": [],
                        "last": "Ghazvininejad",
                        "suffix": ""
                    },
                    {
                        "first": "Abdelrahman",
                        "middle": [],
                        "last": "Mohamed",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "volume": "",
                "issue": "",
                "pages": "7871--7880",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.ACL-MAIN.703"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 7871-7880. https://doi.org/10.18653/V1/2020.ACL-MAIN.703",
                "links": null
            },
            "BIBREF91": {
                "ref_id": "b91",
                "title": "Story Generation with Crowdsourced Plot Graphs",
                "authors": [
                    {
                        "first": "Boyang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Lee-Urban",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Johnston",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [
                            "O"
                        ],
                        "last": "Riedl",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "598--604",
                "other_ids": {
                    "DOI": [
                        "10.1609/AAAI.V27I1.8649"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Boyang Li, Stephen Lee-Urban, George Johnston, and Mark O. Riedl. 2013. Story Generation with Crowdsourced Plot Graphs. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, July 14-18, 2013, Bellevue, Washington, USA, Marie desJardins and Michael L. Littman (Eds.). AAAI Press, 598-604. https://doi.org/10.1609/AAAI.V27I1.8649",
                "links": null
            },
            "BIBREF92": {
                "ref_id": "b92",
                "title": "Story Generation with Crowdsourced Plot Graphs",
                "authors": [
                    {
                        "first": "Boyang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Lee-Urban",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Johnston",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [
                            "O"
                        ],
                        "last": "Riedl",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "598--604",
                "other_ids": {
                    "DOI": [
                        "10.1609/AAAI.V27I1.8649"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Boyang Li, Stephen Lee-Urban, George Johnston, and Mark O. Riedl. 2013. Story Generation with Crowdsourced Plot Graphs. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, July 14-18, 2013, Bellevue, Washington, USA, Marie desJardins and Michael L. Littman (Eds.). AAAI Press, 598-604. https://doi.org/10.1609/AAAI.V27I1.8649",
                "links": null
            },
            "BIBREF93": {
                "ref_id": "b93",
                "title": "A Diversity-Promoting Objective Function for Neural Conversation Models",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Dolan",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "110--119",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/N16-1014"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A Diversity-Promoting Objective Function for Neural Conversation Models. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, Kevin Knight, Ani Nenkova, and Owen Rambow (Eds.). The Association for Computational Linguistics, 110-119. https://doi.org/10.18653/V1/N16-1014",
                "links": null
            },
            "BIBREF94": {
                "ref_id": "b94",
                "title": "Generative judge for evaluating alignment",
                "authors": [
                    {
                        "first": "Junlong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Shichao",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Weizhe",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Run-Ze",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Hai",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2310.05470"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023. Generative judge for evaluating alignment. (2023). arXiv:2310.05470 https://arxiv.org/abs/2310.05470",
                "links": null
            },
            "BIBREF95": {
                "ref_id": "b95",
                "title": "Video Storytelling: Textual Summaries for Events",
                "authors": [
                    {
                        "first": "Junnan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yongkang",
                        "middle": [],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Mohan",
                        "middle": [
                            "S"
                        ],
                        "last": "Kankanhalli",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "IEEE Trans. Multim",
                "volume": "22",
                "issue": "",
                "pages": "554--565",
                "other_ids": {
                    "DOI": [
                        "10.1109/TMM.2019.2930041"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S. Kankanhalli. 2020. Video Storytelling: Textual Summaries for Events. IEEE Trans. Multim. 22, 2 (2020), 554-565. https://doi.org/10.1109/TMM.2019.2930041",
                "links": null
            },
            "BIBREF96": {
                "ref_id": "b96",
                "title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
                "authors": [
                    {
                        "first": "Qintong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Leyang",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Lingpeng",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Bi",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.48550/ARXIV.2310.19740"
                    ],
                    "arXiv": [
                        "arXiv:2310.19740"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Qintong Li, Leyang Cui, Lingpeng Kong, and Wei Bi. 2023. Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation. CoRR abs/2310.19740 (2023). https://doi.org/10.48550/ARXIV.2310.19740 arXiv:2310.19740",
                "links": null
            },
            "BIBREF97": {
                "ref_id": "b97",
                "title": "StoryGAN: A Sequential Conditional GAN for Story Visualization",
                "authors": [
                    {
                        "first": "Yitong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    },
                    {
                        "first": "Yelong",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Yuexin",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Lawrence",
                        "middle": [],
                        "last": "Carin",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "E"
                        ],
                        "last": "Carlson",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "6329--6338",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2019.00649"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David E. Carlson, and Jianfeng Gao. 2019. StoryGAN: A Sequential Conditional GAN for Story Visualization. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. Computer Vision Foundation / IEEE, 6329-6338. https://doi.org/10.1109/CVPR.2019.00649",
                "links": null
            },
            "BIBREF98": {
                "ref_id": "b98",
                "title": "MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation",
                "authors": [
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Shenyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiutian",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Yongrui",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhao",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Guilin",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Dehai",
                        "middle": [],
                        "last": "Min",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2403.19305"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yu Li, Shenyu Zhang, Rui Wu, Xiutian Huang, Yongrui Chen, Wenhao Xu, Guilin Qi, and Dehai Min. 2024. MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation. (2024). arXiv:2403.19305 https://arxiv.org/abs/2403.19305",
                "links": null
            },
            "BIBREF99": {
                "ref_id": "b99",
                "title": "The Value, Benefits, and Concerns of Generative AI-Powered Assistance in Writing",
                "authors": [
                    {
                        "first": "Zhuoyan",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [],
                        "last": "Florian",
                        "suffix": ""
                    },
                    {
                        "first": "'",
                        "middle": [],
                        "last": "Floyd' Mueller",
                        "suffix": ""
                    },
                    {
                        "first": "Penny",
                        "middle": [],
                        "last": "Kyburz",
                        "suffix": ""
                    },
                    {
                        "first": "Julie",
                        "middle": [
                            "R"
                        ],
                        "last": "Williamson",
                        "suffix": ""
                    },
                    {
                        "first": "Corina",
                        "middle": [],
                        "last": "Sas",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [
                            "L"
                        ],
                        "last": "Wilson",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024",
                "volume": "1048",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1145/3613904.3642625"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhuoyan Li, Chen Liang, Jing Peng, and Ming Yin. 2024. The Value, Benefits, and Concerns of Generative AI-Powered Assistance in Writing. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024, Honolulu, HI, USA, May 11-16, 2024, Florian 'Floyd' Mueller, Penny Kyburz, Julie R. Williamson, Corina Sas, Max L. Wilson, Phoebe O. Toups Dugas, and Irina Shklovski (Eds.). ACM, 1048:1-1048:25. https://doi.org/10.1145/3613904.3642625",
                "links": null
            },
            "BIBREF100": {
                "ref_id": "b100",
                "title": "Split and merge: Aligning position biases in large language model based evaluators",
                "authors": [
                    {
                        "first": "Zongjie",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Chaozheng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Pingchuan",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Daoyuan",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Shuai",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Cuiyun",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2310.01432"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2023. Split and merge: Aligning position biases in large language model based evaluators. (2023). arXiv:2310.01432 https://arxiv.org/abs/2310.01432",
                "links": null
            },
            "BIBREF101": {
                "ref_id": "b101",
                "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                "authors": [
                    {
                        "first": "Zhen",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaohan",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Can",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jia-Chen",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxuan",
                        "middle": [],
                        "last": "Lai",
                        "suffix": ""
                    },
                    {
                        "first": "Chongyang",
                        "middle": [],
                        "last": "Tao",
                        "suffix": ""
                    },
                    {
                        "first": "Shuai",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2401.07103"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, and Shuai Ma. 2024. Leveraging Large Language Models for NLG Evaluation: Advances and Challenges. arXiv:2401.07103 https://arxiv.org/abs/2401.07103",
                "links": null
            },
            "BIBREF102": {
                "ref_id": "b102",
                "title": "Rouge: A package for automatic evaluation of summaries",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Text Summarization Branches Out",
                "volume": "",
                "issue": "",
                "pages": "74--81",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain. 74-81. https:/www.aclweb.org/W04-1013",
                "links": null
            },
            "BIBREF103": {
                "ref_id": "b103",
                "title": "Intelligent Grimm-Open-ended Visual Storytelling via Latent Diffusion Models",
                "authors": [
                    {
                        "first": "Chang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Haoning",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Yujie",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyun",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Weidi",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2306.00973"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, and Weidi Xie. 2024. Intelligent Grimm-Open-ended Visual Storytelling via Latent Diffusion Models. (2024). arXiv:2306.00973 https://arxiv.org/abs/2306.00973",
                "links": null
            },
            "BIBREF104": {
                "ref_id": "b104",
                "title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
                "authors": [
                    {
                        "first": "Chia-Wei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Lowe",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Iulian V Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Noseworthy",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Charlin",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1603.08023"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. arXiv preprint arXiv:1603.08023 (2016).",
                "links": null
            },
            "BIBREF105": {
                "ref_id": "b105",
                "title": "Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality",
                "authors": [
                    {
                        "first": "Yongkang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Shi",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Daling",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yifei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2305.14658"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang, and Hinrich Sch\u00fctze. 2024. Evaluate What You Can't Evaluate: Unassessable Generated Responses Quality. (2024). arXiv:2305.14658 https://arxiv.org/abs/2305.14658",
                "links": null
            },
            "BIBREF106": {
                "ref_id": "b106",
                "title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Iter",
                        "suffix": ""
                    },
                    {
                        "first": "Yichong",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Shuohang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ruochen",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Chenguang",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2303.16634"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg evaluation using gpt-4 with better human alignment. (2023). arXiv:2303.16634 https://arxiv.org/abs/2303.16634",
                "links": null
            },
            "BIBREF107": {
                "ref_id": "b107",
                "title": "Calibrating LLM-Based Evaluator",
                "authors": [
                    {
                        "first": "Yuxuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Tianchi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Shaohan",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Zihan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Haizhen",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Weiwei",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Feng",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024",
                "volume": "",
                "issue": "",
                "pages": "2638--2656",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2024. Calibrating LLM-Based Evaluator. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, Nicoletta Calzolari, Min-Yen Kan, V\u00e9ronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, 2638-2656. https://aclanthology.org/2024.lrec-main.237",
                "links": null
            },
            "BIBREF108": {
                "ref_id": "b108",
                "title": "LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models",
                "authors": [
                    {
                        "first": "Adian",
                        "middle": [],
                        "last": "Liusie",
                        "suffix": ""
                    },
                    {
                        "first": "Potsawee",
                        "middle": [],
                        "last": "Manakul",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [
                            "J F"
                        ],
                        "last": "Gales",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Proceedings of the 18th Conference of the European Chapter",
                "volume": "1",
                "issue": "",
                "pages": "139--151",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adian Liusie, Potsawee Manakul, and Mark J. F. Gales. 2024. LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 -Volume 1: Long Papers, St. Julian's, Malta, March 17-22, 2024, Yvette Graham and Matthew Purver (Eds.). Association for Computational Linguistics, 139-151. https://aclanthology.org/2024.eacl-long.8",
                "links": null
            },
            "BIBREF109": {
                "ref_id": "b109",
                "title": "Sentence ordering and coherence modeling using recurrent neural networks",
                "authors": [
                    {
                        "first": "Lajanugen",
                        "middle": [],
                        "last": "Logeswaran",
                        "suffix": ""
                    },
                    {
                        "first": "Honglak",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Dragomir",
                        "middle": [],
                        "last": "Radev",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "32",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.inlg-1.23"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lajanugen Logeswaran, Honglak Lee, and Dragomir Radev. 2018. Sentence ordering and coherence modeling using recurrent neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32. New Orleans, Louisiana, USA. https://doi.org/10.18653/v1/2020.inlg-1.23",
                "links": null
            },
            "BIBREF110": {
                "ref_id": "b110",
                "title": "Show Me a Video: A Large-Scale Narrated Video Dataset for Coherent Story Illustration",
                "authors": [
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Feiyue",
                        "middle": [],
                        "last": "Ni",
                        "suffix": ""
                    },
                    {
                        "first": "Haofan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaofeng",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Linchao",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Zongxin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Ruihua",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Lele",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "IEEE Transactions on Multimedia",
                "volume": "26",
                "issue": "",
                "pages": "2456--2466",
                "other_ids": {
                    "DOI": [
                        "10.1109/TMM.2023.3296944"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yu Lu, Feiyue Ni, Haofan Wang, Xiaofeng Guo, Linchao Zhu, Zongxin Yang, Ruihua Song, Lele Cheng, and Yi Yang. 2024. Show Me a Video: A Large-Scale Narrated Video Dataset for Coherent Story Illustration. IEEE Transactions on Multimedia 26 (2024), 2456-2466. https://doi.org/10.1109/ TMM.2023.3296944",
                "links": null
            },
            "BIBREF111": {
                "ref_id": "b111",
                "title": "Learning to Control the Fine-grained Sentiment for Story Ending Generation",
                "authors": [
                    {
                        "first": "Fuli",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "Damai",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Pengcheng",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Baobao",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifang",
                        "middle": [],
                        "last": "Sui",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
                "volume": "1",
                "issue": "",
                "pages": "6020--6026",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/P19-1603"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Fuli Luo, Damai Dai, Pengcheng Yang, Tianyu Liu, Baobao Chang, Zhifang Sui, and Xu Sun. 2019. Learning to Control the Fine-grained Sentiment for Story Ending Generation. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez (Eds.). Association for Computational Linguistics, 6020-6026. https://doi.org/10.18653/V1/P19-1603",
                "links": null
            },
            "BIBREF112": {
                "ref_id": "b112",
                "title": "MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation",
                "authors": [
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Qiao",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2406.05690"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yan Ma, Yu Qiao, and Pengfei Liu. 2024. MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation. arXiv:2406.05690 https://arxiv.org/abs/2406.05690",
                "links": null
            },
            "BIBREF113": {
                "ref_id": "b113",
                "title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "authors": [
                    {
                        "first": "Aman",
                        "middle": [],
                        "last": "Madaan",
                        "suffix": ""
                    },
                    {
                        "first": "Niket",
                        "middle": [],
                        "last": "Tandon",
                        "suffix": ""
                    },
                    {
                        "first": "Prakhar",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Skyler",
                        "middle": [],
                        "last": "Hallinan",
                        "suffix": ""
                    },
                    {
                        "first": "Luyu",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Sarah",
                        "middle": [],
                        "last": "Wiegreffe",
                        "suffix": ""
                    },
                    {
                        "first": "Uri",
                        "middle": [],
                        "last": "Alon",
                        "suffix": ""
                    },
                    {
                        "first": "Nouha",
                        "middle": [],
                        "last": "Dziri",
                        "suffix": ""
                    },
                    {
                        "first": "Shrimai",
                        "middle": [],
                        "last": "Prabhumoye",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Shashank",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Bodhisattwa",
                        "middle": [],
                        "last": "Prasad Majumder",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Sean",
                        "middle": [],
                        "last": "Welleck",
                        "suffix": ""
                    },
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Yazdanbakhsh",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with Self-Feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 -16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html",
                "links": null
            },
            "BIBREF114": {
                "ref_id": "b114",
                "title": "Integrating Visuospatial, Linguistic, and Commonsense Structure into Story Visualization",
                "authors": [
                    {
                        "first": "Adyasha",
                        "middle": [],
                        "last": "Maharana",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "6772--6786",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2021.EMNLP-MAIN.543"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Adyasha Maharana and Mohit Bansal. 2021. Integrating Visuospatial, Linguistic, and Commonsense Structure into Story Visualization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, 6772-6786. https://doi.org/10.18653/V1/2021.EMNLP-MAIN.543",
                "links": null
            },
            "BIBREF115": {
                "ref_id": "b115",
                "title": "Improving Generation and Evaluation of Visual Stories via Semantic Consistency",
                "authors": [
                    {
                        "first": "Adyasha",
                        "middle": [],
                        "last": "Maharana",
                        "suffix": ""
                    },
                    {
                        "first": "Darryl",
                        "middle": [],
                        "last": "Hannan",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
                "volume": "",
                "issue": "",
                "pages": "2427--2442",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2021.NAACL-MAIN.194"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Adyasha Maharana, Darryl Hannan, and Mohit Bansal. 2021. Improving Generation and Evaluation of Visual Stories via Semantic Consistency. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T\u00fcr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, 2427-2442. https://doi.org/10.18653/V1/ 2021.NAACL-MAIN.194",
                "links": null
            },
            "BIBREF116": {
                "ref_id": "b116",
                "title": "StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation",
                "authors": [
                    {
                        "first": "Adyasha",
                        "middle": [],
                        "last": "Maharana",
                        "suffix": ""
                    },
                    {
                        "first": "Darryl",
                        "middle": [],
                        "last": "Hannan",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Computer Vision -ECCV 2022 -17th European Conference",
                "volume": "XXXVII",
                "issue": "",
                "pages": "70--87",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-031-19836-6_5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Adyasha Maharana, Darryl Hannan, and Mohit Bansal. 2022. StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation. In Computer Vision -ECCV 2022 -17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVII (Lecture Notes in Computer Science, Vol. 13697), Shai Avidan, Gabriel J. Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, and Tal Hassner (Eds.). Springer, 70-87. https://doi.org/10.1007/978-3-031-19836-6_5",
                "links": null
            },
            "BIBREF117": {
                "ref_id": "b117",
                "title": "COHESENTIA: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts",
                "authors": [
                    {
                        "first": "Aviya",
                        "middle": [],
                        "last": "Maimon",
                        "suffix": ""
                    },
                    {
                        "first": "Reut",
                        "middle": [],
                        "last": "Tsarfaty",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "5328--5343",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2023.emnlp-main.324"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aviya Maimon and Reut Tsarfaty. 2023. COHESENTIA: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 5328-5343. https://doi.org/10.18653/v1/2023.emnlp-main.324",
                "links": null
            },
            "BIBREF118": {
                "ref_id": "b118",
                "title": "A Novel Computational and Modeling Foundation for Automatic Coherence Assessment",
                "authors": [
                    {
                        "first": "Aviya",
                        "middle": [],
                        "last": "Maimon",
                        "suffix": ""
                    },
                    {
                        "first": "Reut",
                        "middle": [],
                        "last": "Tsarfaty",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2310.00598"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aviya Maimon and Reut Tsarfaty. 2023. A Novel Computational and Modeling Foundation for Automatic Coherence Assessment. arXiv:2310.00598 https://arxiv.org/abs/2310.00598",
                "links": null
            },
            "BIBREF119": {
                "ref_id": "b119",
                "title": "Event Representations for Automated Story Generation with Deep Neural Nets",
                "authors": [
                    {
                        "first": "Lara",
                        "middle": [
                            "J"
                        ],
                        "last": "Martin",
                        "suffix": ""
                    },
                    {
                        "first": "Prithviraj",
                        "middle": [],
                        "last": "Ammanabrolu",
                        "suffix": ""
                    },
                    {
                        "first": "Xinyu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Hancock",
                        "suffix": ""
                    },
                    {
                        "first": "Shruti",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Brent",
                        "middle": [],
                        "last": "Harrison",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [
                            "O"
                        ],
                        "last": "Riedl",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)",
                "volume": "",
                "issue": "",
                "pages": "868--875",
                "other_ids": {
                    "DOI": [
                        "10.1609/AAAI.V32I1.11430"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lara J. Martin, Prithviraj Ammanabrolu, Xinyu Wang, William Hancock, Shruti Singh, Brent Harrison, and Mark O. Riedl. 2018. Event Represen- tations for Automated Story Generation with Deep Neural Nets. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q. Weinberger (Eds.). AAAI Press, 868-875. https://doi.org/10.1609/AAAI.V32I1.11430",
                "links": null
            },
            "BIBREF120": {
                "ref_id": "b120",
                "title": "Learning through storytelling higher education: Using reflection & experience to improve learning",
                "authors": [
                    {
                        "first": "Janice",
                        "middle": [],
                        "last": "Mcdrury",
                        "suffix": ""
                    },
                    {
                        "first": "Maxine",
                        "middle": [],
                        "last": "Alterio",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "1--199",
                "other_ids": {
                    "DOI": [
                        "10.4324/9780203416655"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Janice McDrury and Maxine Alterio. 2003. Learning through storytelling higher education: Using reflection & experience to improve learning. (12 2003), 1-199. https://doi.org/10.4324/9780203416655",
                "links": null
            },
            "BIBREF121": {
                "ref_id": "b121",
                "title": "Long Story Generation Challenge",
                "authors": [
                    {
                        "first": "Nikolay",
                        "middle": [],
                        "last": "Mikhaylovskiy",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 16th International Natural Language Generation Conference, INLG 2023 -Generation Challenges",
                "volume": "",
                "issue": "",
                "pages": "10--16",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nikolay Mikhaylovskiy. 2023. Long Story Generation Challenge. In Proceedings of the 16th International Natural Language Generation Conference, INLG 2023 -Generation Challenges, Prague, Czechia, September 11 -15, 2023, Simon Mille (Ed.). Association for Computational Linguistics, 10-16. https://aclanthology.org/2023.inlg-genchal.2",
                "links": null
            },
            "BIBREF122": {
                "ref_id": "b122",
                "title": "Efficient Estimation of Word Representations in Vector Space",
                "authors": [
                    {
                        "first": "Tom\u00e1s",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "1st International Conference on Learning Representations, ICLR 2013",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom\u00e1s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. In 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1301.3781",
                "links": null
            },
            "BIBREF123": {
                "ref_id": "b123",
                "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "authors": [
                    {
                        "first": "Sewon",
                        "middle": [],
                        "last": "Min",
                        "suffix": ""
                    },
                    {
                        "first": "Kalpesh",
                        "middle": [],
                        "last": "Krishna",
                        "suffix": ""
                    },
                    {
                        "first": "Xinxi",
                        "middle": [],
                        "last": "Lyu",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Wen-Tau",
                        "middle": [],
                        "last": "Yih",
                        "suffix": ""
                    },
                    {
                        "first": "Pang",
                        "middle": [],
                        "last": "Wei Koh",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023",
                "volume": "",
                "issue": "",
                "pages": "12076--12100",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.EMNLP-MAIN.741"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 12076-12100. https://doi.org/10.18653/V1/2023.EMNLP-MAIN.741",
                "links": null
            },
            "BIBREF124": {
                "ref_id": "b124",
                "title": "Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals",
                "authors": [
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Mirowski",
                        "suffix": ""
                    },
                    {
                        "first": "Kory",
                        "middle": [
                            "W"
                        ],
                        "last": "Mathewson",
                        "suffix": ""
                    },
                    {
                        "first": "Jaylen",
                        "middle": [],
                        "last": "Pittman",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Evans",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023",
                "volume": "355",
                "issue": "",
                "pages": "1--355",
                "other_ids": {
                    "DOI": [
                        "10.1145/3544548.3581225"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, and Richard Evans. 2023. Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023, Albrecht Schmidt, Kaisa V\u00e4\u00e4n\u00e4nen, Tesh Goyal, Per Ola Kristensson, Anicia Peters, Stefanie Mueller, Julie R. Williamson, and Max L. Wilson (Eds.). ACM, 355:1-355:34. https://doi.org/10.1145/3544548.3581225",
                "links": null
            },
            "BIBREF125": {
                "ref_id": "b125",
                "title": "Jointly measuring diversity and quality in text generation models",
                "authors": [
                    {
                        "first": "Ehsan",
                        "middle": [],
                        "last": "Montahaei",
                        "suffix": ""
                    },
                    {
                        "first": "Danial",
                        "middle": [],
                        "last": "Alihosseini",
                        "suffix": ""
                    },
                    {
                        "first": "Mahdieh",
                        "middle": [],
                        "last": "Soleymani",
                        "suffix": ""
                    },
                    {
                        "first": "Baghshah",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.03971"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ehsan Montahaei, Danial Alihosseini, and Mahdieh Soleymani Baghshah. 2019. Jointly measuring diversity and quality in text generation models. (2019). arXiv:1904.03971 https://arxiv.org/abs/1904.03971",
                "links": null
            },
            "BIBREF126": {
                "ref_id": "b126",
                "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories",
                "authors": [
                    {
                        "first": "Nasrin",
                        "middle": [],
                        "last": "Mostafazadeh",
                        "suffix": ""
                    },
                    {
                        "first": "Nathanael",
                        "middle": [],
                        "last": "Chambers",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Lucy",
                        "middle": [],
                        "last": "Vanderwende",
                        "suffix": ""
                    },
                    {
                        "first": "Pushmeet",
                        "middle": [],
                        "last": "Kohli",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "F"
                        ],
                        "last": "Allen",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "839--849",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/N16-1098"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James F. Allen. 2016. A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, Kevin Knight, Ani Nenkova, and Owen Rambow (Eds.). The Association for Computational Linguistics, 839-849. https://doi.org/10.18653/V1/N16-1098",
                "links": null
            },
            "BIBREF127": {
                "ref_id": "b127",
                "title": "Stylized text generation: Approaches and applications",
                "authors": [
                    {
                        "first": "Lili",
                        "middle": [],
                        "last": "Mou",
                        "suffix": ""
                    },
                    {
                        "first": "Olga",
                        "middle": [],
                        "last": "Vechtomova",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, ACL 2020",
                "volume": "",
                "issue": "",
                "pages": "19--22",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.ACL-TUTORIALS.5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lili Mou and Olga Vechtomova. 2020. Stylized text generation: Approaches and applications. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, ACL 2020, Online, July 5, 2020. Association for Computational Linguistics, 19-22. https://doi.org/10.18653/V1/2020.ACL-TUTORIALS.5",
                "links": null
            },
            "BIBREF128": {
                "ref_id": "b128",
                "title": "Better Summarization Evaluation with Word Embeddings for ROUGE",
                "authors": [
                    {
                        "first": "Jun-Ping",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Viktoria",
                        "middle": [],
                        "last": "Abrecht",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1925--1930",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/D15-1222"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jun-Ping Ng and Viktoria Abrecht. 2015. Better Summarization Evaluation with Word Embeddings for ROUGE. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, Llu\u00eds M\u00e0rquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton (Eds.). The Association for Computational Linguistics, 1925-1930. https://doi.org/10.18653/V1/D15-1222",
                "links": null
            },
            "BIBREF129": {
                "ref_id": "b129",
                "title": "Album Storytelling with Iterative Story-aware Captioning and Large Language Models",
                "authors": [
                    {
                        "first": "Munan",
                        "middle": [],
                        "last": "Ning",
                        "suffix": ""
                    },
                    {
                        "first": "Yujia",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Dongdong",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Zeyin",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Lu",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Yonghong",
                        "middle": [],
                        "last": "Tian",
                        "suffix": ""
                    },
                    {
                        "first": "Qixiang",
                        "middle": [],
                        "last": "Ye",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.48550/ARXIV.2305.12943"
                    ],
                    "arXiv": [
                        "arXiv:2305.12943"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Munan Ning, Yujia Xie, Dongdong Chen, Zeyin Song, Lu Yuan, Yonghong Tian, Qixiang Ye, and Li Yuan. 2023. Album Storytelling with Iterative Story-aware Captioning and Large Language Models. CoRR abs/2305.12943 (2023). https://doi.org/10.48550/ARXIV.2305.12943 arXiv:2305.12943",
                "links": null
            },
            "BIBREF130": {
                "ref_id": "b130",
                "title": "Likelihood-based Mitigation of Evaluation Bias in Large Language Models",
                "authors": [
                    {
                        "first": "Masanari",
                        "middle": [],
                        "last": "Ohi",
                        "suffix": ""
                    },
                    {
                        "first": "Masahiro",
                        "middle": [],
                        "last": "Kaneko",
                        "suffix": ""
                    },
                    {
                        "first": "Ryuto",
                        "middle": [],
                        "last": "Koike",
                        "suffix": ""
                    },
                    {
                        "first": "Mengsay",
                        "middle": [],
                        "last": "Loem",
                        "suffix": ""
                    },
                    {
                        "first": "Naoaki",
                        "middle": [],
                        "last": "Okazaki",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2402.15987"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Masanari Ohi, Masahiro Kaneko, Ryuto Koike, Mengsay Loem, and Naoaki Okazaki. 2024. Likelihood-based Mitigation of Evaluation Bias in Large Language Models. (2024). arXiv:2402.15987 https://arxiv.org/abs/2402.15987",
                "links": null
            },
            "BIBREF131": {
                "ref_id": "b131",
                "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {
                    "DOI": [
                        "10.3115/1073083.1073135"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA. ACL, 311-318. https://doi.org/10.3115/1073083.1073135",
                "links": null
            },
            "BIBREF132": {
                "ref_id": "b132",
                "title": "Expressing an Image Stream with a Sequence of Natural Sentences",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Cesc",
                        "suffix": ""
                    },
                    {
                        "first": "Gunhee",
                        "middle": [],
                        "last": "Park",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "73--81",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cesc C. Park and Gunhee Kim. 2015. Expressing an Image Stream with a Sequence of Natural Sentences. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (Eds.). 73-81. https://proceedings.neurips.cc/paper/2015/hash/ 17e62166fc8586dfa4d1bc0e1742c08b-Abstract.html",
                "links": null
            },
            "BIBREF133": {
                "ref_id": "b133",
                "title": "LongStory: Coherent, Complete and Length Controlled Long story Generation",
                "authors": [
                    {
                        "first": "Kyeongman",
                        "middle": [],
                        "last": "Park",
                        "suffix": ""
                    },
                    {
                        "first": "Nakyeong",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Kyomin",
                        "middle": [],
                        "last": "Jung",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Advances in Knowledge Discovery and Data Mining -28th Pacific-Asia Conference on Knowledge Discovery and Data Mining",
                "volume": "2024",
                "issue": "",
                "pages": "184--196",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-981-97-2253-2_15"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kyeongman Park, Nakyeong Yang, and Kyomin Jung. 2024. LongStory: Coherent, Complete and Length Controlled Long story Generation. In Advances in Knowledge Discovery and Data Mining -28th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2024, Taipei, Taiwan, May 7-10, 2024, Proceedings, Part II (Lecture Notes in Computer Science, Vol. 14646). Springer, 184-196. https://doi.org/10.1007/978-981-97- 2253-2_15",
                "links": null
            },
            "BIBREF134": {
                "ref_id": "b134",
                "title": "Note on regression and inheritance in the case of two parents",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Pearson",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "proceedings of the royal society of London",
                "volume": "58",
                "issue": "",
                "pages": "240--242",
                "other_ids": {
                    "DOI": [
                        "10.1098/rspl.1895.0041"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Karl Pearson. 1895. VII. Note on regression and inheritance in the case of two parents. proceedings of the royal society of London 58, 347-352 (1895), 240-242. https://doi.org/10.1098/rspl.1895.0041",
                "links": null
            },
            "BIBREF135": {
                "ref_id": "b135",
                "title": "A Survey of Useful LLM Evaluation",
                "authors": [
                    {
                        "first": "Ji-Lun",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Sijia",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Egil",
                        "middle": [],
                        "last": "Diau",
                        "suffix": ""
                    },
                    {
                        "first": "Yung-Yu",
                        "middle": [],
                        "last": "Shih",
                        "suffix": ""
                    },
                    {
                        "first": "Po-Heng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yen-Ting",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Yun-Nung",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2406.00936"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, and Yun-Nung Chen. 2024. A Survey of Useful LLM Evaluation. arXiv:2406.00936 https://arxiv.org/abs/2406.00936",
                "links": null
            },
            "BIBREF136": {
                "ref_id": "b136",
                "title": "Towards controllable story generation",
                "authors": [
                    {
                        "first": "Nanyun",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Marjan",
                        "middle": [],
                        "last": "Ghazvininejad",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "May",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the First Workshop on Storytelling",
                "volume": "",
                "issue": "",
                "pages": "43--49",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nanyun Peng, Marjan Ghazvininejad, Jonathan May, and Kevin Knight. 2018. Towards controllable story generation. In Proceedings of the First Workshop on Storytelling. New Orleans, Louisiana, 43-49. https://aclanthology.org/W18-1505",
                "links": null
            },
            "BIBREF137": {
                "ref_id": "b137",
                "title": "MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers",
                "authors": [
                    {
                        "first": "Krishna",
                        "middle": [],
                        "last": "Pillutla",
                        "suffix": ""
                    },
                    {
                        "first": "Swabha",
                        "middle": [],
                        "last": "Swayamdipta",
                        "suffix": ""
                    },
                    {
                        "first": "Rowan",
                        "middle": [],
                        "last": "Zellers",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Thickstun",
                        "suffix": ""
                    },
                    {
                        "first": "Sean",
                        "middle": [],
                        "last": "Welleck",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Za\u00efd",
                        "middle": [],
                        "last": "Harchaoui",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021",
                "volume": "",
                "issue": "",
                "pages": "4816--4828",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Za\u00efd Harchaoui. 2021. MAUVE: Mea- suring the Gap Between Neural Text and Human Text using Divergence Frontiers. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (Eds.). 4816-4828. https://proceedings.neurips.cc/paper/2021/hash/ 260c2432a0eecc28ce03c10dadc078a4-Abstract.html",
                "links": null
            },
            "BIBREF138": {
                "ref_id": "b138",
                "title": "My Way of Telling a Story\": Persona based Grounded Story Generation",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Shrimai Prabhumoye",
                        "suffix": ""
                    },
                    {
                        "first": "Raghavi",
                        "middle": [],
                        "last": "Khyathi",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Chandu",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [
                            "W"
                        ],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Black",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the Second Workshop on Storytelling",
                "volume": "",
                "issue": "",
                "pages": "11--21",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W19-3402"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shrimai Prabhumoye, Khyathi Raghavi Chandu, Ruslan Salakhutdinov, and Alan W Black. 2019. \" My Way of Telling a Story\": Persona based Grounded Story Generation. In Proceedings of the Second Workshop on Storytelling. Association for Computational Linguistics, Florence, Italy, 11-21. https://doi.org/10.18653/v1/W19-3402",
                "links": null
            },
            "BIBREF139": {
                "ref_id": "b139",
                "title": "Topological Sort for Sentence Ordering",
                "authors": [
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Shrimai Prabhumoye",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [
                            "W"
                        ],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Black",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "volume": "",
                "issue": "",
                "pages": "2783--2792",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.ACL-MAIN.248"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shrimai Prabhumoye, Ruslan Salakhutdinov, and Alan W. Black. 2020. Topological Sort for Sentence Ordering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020. Association for Computational Linguistics, 2783-2792. https://doi.org/10.18653/V1/2020.ACL-MAIN.248",
                "links": null
            },
            "BIBREF140": {
                "ref_id": "b140",
                "title": "T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics",
                "authors": [
                    {
                        "first": "Yiwei",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Weizhe",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
                "volume": "",
                "issue": "",
                "pages": "15185--15202",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.FINDINGS-EMNLP.1014"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yiwei Qin, Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2023. T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 15185-15202. https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.1014",
                "links": null
            },
            "BIBREF141": {
                "ref_id": "b141",
                "title": "Learning Transferable Visual Models From Natural Language Supervision",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jong",
                        "middle": [
                            "Wook"
                        ],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Hallacy",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Ramesh",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Goh",
                        "suffix": ""
                    },
                    {
                        "first": "Sandhini",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Girish",
                        "middle": [],
                        "last": "Sastry",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    },
                    {
                        "first": "Pamela",
                        "middle": [],
                        "last": "Mishkin",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Gretchen",
                        "middle": [],
                        "last": "Krueger",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021",
                "volume": "139",
                "issue": "",
                "pages": "8748--8763",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 8748-8763. http://proceedings.mlr.press/v139/radford21a.html",
                "links": null
            },
            "BIBREF142": {
                "ref_id": "b142",
                "title": "Language Models are Unsupervised Multitask Learners",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Rewon",
                        "middle": [],
                        "last": "Child",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019). https://api.semanticscholar.org/CorpusID:160025533",
                "links": null
            },
            "BIBREF143": {
                "ref_id": "b143",
                "title": "Make-A-Story: Visual Memory Conditioned Consistent Story Generation",
                "authors": [
                    {
                        "first": "Tanzila",
                        "middle": [],
                        "last": "Rahman",
                        "suffix": ""
                    },
                    {
                        "first": "Hsin-Ying",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Tulyakov",
                        "suffix": ""
                    },
                    {
                        "first": "Shweta",
                        "middle": [],
                        "last": "Mahajan",
                        "suffix": ""
                    },
                    {
                        "first": "Leonid",
                        "middle": [],
                        "last": "Sigal",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023",
                "volume": "",
                "issue": "",
                "pages": "2493--2502",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR52729.2023.00246"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, and Leonid Sigal. 2023. Make-A-Story: Visual Memory Conditioned Consistent Story Generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023. IEEE, 2493-2502. https://doi.org/10.1109/CVPR52729.2023.00246",
                "links": null
            },
            "BIBREF144": {
                "ref_id": "b144",
                "title": "PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking",
                "authors": [
                    {
                        "first": "Asli",
                        "middle": [],
                        "last": "Hannah Rashkin",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Celikyilmaz",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "4274--4295",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.EMNLP-MAIN.349"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and Jianfeng Gao. 2020. PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 4274-4295. https://doi.org/10.18653/V1/ 2020.EMNLP-MAIN.349",
                "links": null
            },
            "BIBREF145": {
                "ref_id": "b145",
                "title": "AESOP: Abstract Encoding of Stories, Objects, and Pictures",
                "authors": [
                    {
                        "first": "Hareesh",
                        "middle": [],
                        "last": "Ravi",
                        "suffix": ""
                    },
                    {
                        "first": "Kushal",
                        "middle": [],
                        "last": "Kafle",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Brandt",
                        "suffix": ""
                    },
                    {
                        "first": "Mubbasir",
                        "middle": [],
                        "last": "Kapadia",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021",
                "volume": "",
                "issue": "",
                "pages": "2032--2043",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICCV48922.2021.00206"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hareesh Ravi, Kushal Kafle, Scott Cohen, Jonathan Brandt, and Mubbasir Kapadia. 2021. AESOP: Abstract Encoding of Stories, Objects, and Pictures. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021. IEEE, 2032-2043. https://doi.org/10.1109/ICCV48922.2021.00206",
                "links": null
            },
            "BIBREF146": {
                "ref_id": "b146",
                "title": "Show me a story: Towards coherent neural story illustration",
                "authors": [
                    {
                        "first": "Hareesh",
                        "middle": [],
                        "last": "Ravi",
                        "suffix": ""
                    },
                    {
                        "first": "Lezi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Carlos",
                        "middle": [],
                        "last": "Muniz",
                        "suffix": ""
                    },
                    {
                        "first": "Leonid",
                        "middle": [],
                        "last": "Sigal",
                        "suffix": ""
                    },
                    {
                        "first": "Dimitris",
                        "middle": [],
                        "last": "Metaxas",
                        "suffix": ""
                    },
                    {
                        "first": "Mubbasir",
                        "middle": [],
                        "last": "Kapadia",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "2018 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "7613--7621",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2018.00794"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hareesh Ravi, Lezi Wang, Carlos Muniz, Leonid Sigal, Dimitris Metaxas, and Mubbasir Kapadia. 2018. Show me a story: Towards coherent neural story illustration. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. Computer Vision Foundation / IEEE Computer Society, 7613-7621. https://doi.org/10.1109/CVPR.2018.00794",
                "links": null
            },
            "BIBREF147": {
                "ref_id": "b147",
                "title": "COMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task",
                "authors": [
                    {
                        "first": "Ricardo",
                        "middle": [],
                        "last": "Rei",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [
                            "C"
                        ],
                        "last": "Jos\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "De Souza",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Duarte",
                        "suffix": ""
                    },
                    {
                        "first": "Chrysoula",
                        "middle": [],
                        "last": "Alves",
                        "suffix": ""
                    },
                    {
                        "first": "Ana",
                        "middle": [
                            "C"
                        ],
                        "last": "Zerva",
                        "suffix": ""
                    },
                    {
                        "first": "Taisiya",
                        "middle": [],
                        "last": "Farinha",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Glushkova",
                        "suffix": ""
                    },
                    {
                        "first": "Lu\u00edsa",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Coheur",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [
                            "T"
                        ],
                        "last": "Andr\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Lo\u00efc",
                        "middle": [],
                        "last": "Martins ; Philipp Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Ondrej",
                        "middle": [],
                        "last": "Barrault",
                        "suffix": ""
                    },
                    {
                        "first": "Fethi",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Rajen",
                        "middle": [],
                        "last": "Bougares",
                        "suffix": ""
                    },
                    {
                        "first": "Marta",
                        "middle": [
                            "R"
                        ],
                        "last": "Chatterjee",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Costa-Juss\u00e0",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Federmann",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Fishel",
                        "suffix": ""
                    },
                    {
                        "first": "Markus",
                        "middle": [],
                        "last": "Fraser",
                        "suffix": ""
                    },
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Freitag",
                        "suffix": ""
                    },
                    {
                        "first": "Roman",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    },
                    {
                        "first": "Paco",
                        "middle": [],
                        "last": "Grundkiewicz",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Guzman",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Huck",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Jimeno-Yepes",
                        "suffix": ""
                    },
                    {
                        "first": "Andr\u00e9",
                        "middle": [],
                        "last": "Kocmi",
                        "suffix": ""
                    },
                    {
                        "first": "Makoto",
                        "middle": [],
                        "last": "Martins",
                        "suffix": ""
                    },
                    {
                        "first": "Christof",
                        "middle": [],
                        "last": "Morishita",
                        "suffix": ""
                    },
                    {
                        "first": "Masaaki",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    },
                    {
                        "first": "Toshiaki",
                        "middle": [],
                        "last": "Nagata",
                        "suffix": ""
                    },
                    {
                        "first": "Matteo",
                        "middle": [],
                        "last": "Nakazawa",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Negri",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid)",
                "volume": "",
                "issue": "",
                "pages": "578--585",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ricardo Rei, Jos\u00e9 G. C. de Souza, Duarte M. Alves, Chrysoula Zerva, Ana C. Farinha, Taisiya Glushkova, Alon Lavie, Lu\u00edsa Coheur, and Andr\u00e9 F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task. In Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022, Philipp Koehn, Lo\u00efc Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-juss\u00e0, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno-Yepes, Tom Kocmi, Andr\u00e9 Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, Matteo Negri, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Martin Popel, Marco Turchi, and Marcos Zampieri (Eds.). Association for Computational Linguistics, 578-585. https://aclanthology.org/2022.wmt-1.52",
                "links": null
            },
            "BIBREF148": {
                "ref_id": "b148",
                "title": "COMET: A Neural Framework for MT Evaluation",
                "authors": [
                    {
                        "first": "Ricardo",
                        "middle": [],
                        "last": "Rei",
                        "suffix": ""
                    },
                    {
                        "first": "Craig",
                        "middle": [],
                        "last": "Stewart",
                        "suffix": ""
                    },
                    {
                        "first": "Ana",
                        "middle": [
                            "C"
                        ],
                        "last": "Farinha",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "2685--2702",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.EMNLP-MAIN.213"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ricardo Rei, Craig Stewart, Ana C. Farinha, and Alon Lavie. 2020. COMET: A Neural Framework for MT Evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 2685-2702. https://doi.org/10.18653/V1/2020.EMNLP-MAIN.213",
                "links": null
            },
            "BIBREF149": {
                "ref_id": "b149",
                "title": "CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task",
                "authors": [
                    {
                        "first": "Ricardo",
                        "middle": [],
                        "last": "Rei",
                        "suffix": ""
                    },
                    {
                        "first": "Marcos",
                        "middle": [
                            "V"
                        ],
                        "last": "Treviso",
                        "suffix": ""
                    },
                    {
                        "first": "Miguel",
                        "middle": [],
                        "last": "Nuno",
                        "suffix": ""
                    },
                    {
                        "first": "Chrysoula",
                        "middle": [],
                        "last": "Guerreiro",
                        "suffix": ""
                    },
                    {
                        "first": "Ana",
                        "middle": [
                            "C"
                        ],
                        "last": "Zerva",
                        "suffix": ""
                    },
                    {
                        "first": "Christine",
                        "middle": [],
                        "last": "Farinha",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Maroti",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [
                            "C"
                        ],
                        "last": "Jos\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Taisiya",
                        "middle": [],
                        "last": "De Souza",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Glushkova",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Duarte",
                        "suffix": ""
                    },
                    {
                        "first": "Lu\u00edsa",
                        "middle": [],
                        "last": "Alves",
                        "suffix": ""
                    },
                    {
                        "first": "Alon",
                        "middle": [],
                        "last": "Coheur",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lavie",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [
                            "T"
                        ],
                        "last": "Andr\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Lo\u00efc",
                        "middle": [],
                        "last": "Martins ; Philipp Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Ondrej",
                        "middle": [],
                        "last": "Barrault",
                        "suffix": ""
                    },
                    {
                        "first": "Fethi",
                        "middle": [],
                        "last": "Bojar",
                        "suffix": ""
                    },
                    {
                        "first": "Rajen",
                        "middle": [],
                        "last": "Bougares",
                        "suffix": ""
                    },
                    {
                        "first": "Marta",
                        "middle": [
                            "R"
                        ],
                        "last": "Chatterjee",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Costa-Juss\u00e0",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Federmann",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Fishel",
                        "suffix": ""
                    },
                    {
                        "first": "Markus",
                        "middle": [],
                        "last": "Fraser",
                        "suffix": ""
                    },
                    {
                        "first": "Yvette",
                        "middle": [],
                        "last": "Freitag",
                        "suffix": ""
                    },
                    {
                        "first": "Roman",
                        "middle": [],
                        "last": "Graham",
                        "suffix": ""
                    },
                    {
                        "first": "Paco",
                        "middle": [],
                        "last": "Grundkiewicz",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Guzman",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Haddow",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Huck",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Jimeno-Yepes",
                        "suffix": ""
                    },
                    {
                        "first": "Andr\u00e9",
                        "middle": [],
                        "last": "Kocmi",
                        "suffix": ""
                    },
                    {
                        "first": "Makoto",
                        "middle": [],
                        "last": "Martins",
                        "suffix": ""
                    },
                    {
                        "first": "Christof",
                        "middle": [],
                        "last": "Morishita",
                        "suffix": ""
                    },
                    {
                        "first": "Masaaki",
                        "middle": [],
                        "last": "Monz",
                        "suffix": ""
                    },
                    {
                        "first": "Toshiaki",
                        "middle": [],
                        "last": "Nagata",
                        "suffix": ""
                    },
                    {
                        "first": "Matteo",
                        "middle": [],
                        "last": "Nakazawa",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Negri",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid)",
                "volume": "",
                "issue": "",
                "pages": "634--645",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ricardo Rei, Marcos V. Treviso, Nuno Miguel Guerreiro, Chrysoula Zerva, Ana C. Farinha, Christine Maroti, Jos\u00e9 G. C. de Souza, Taisiya Glushkova, Duarte M. Alves, Lu\u00edsa Coheur, Alon Lavie, and Andr\u00e9 F. T. Martins. 2022. CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task. In Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022, Philipp Koehn, Lo\u00efc Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-juss\u00e0, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno-Yepes, Tom Kocmi, Andr\u00e9 Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, Matteo Negri, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Martin Popel, Marco Turchi, and Marcos Zampieri (Eds.). Association for Computational Linguistics, 634-645. https://aclanthology.org/2022.wmt-1.60",
                "links": null
            },
            "BIBREF150": {
                "ref_id": "b150",
                "title": "Likert-type scales, statistical methods, and effect sizes",
                "authors": [
                    {
                        "first": "Judy",
                        "middle": [],
                        "last": "Robertson",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Commun. ACM",
                "volume": "55",
                "issue": "",
                "pages": "6--7",
                "other_ids": {
                    "DOI": [
                        "10.1145/2160718.2160721"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Judy Robertson. 2012. Likert-type scales, statistical methods, and effect sizes. Commun. ACM 55, 5 (2012), 6-7. https://doi.org/10.1145/2160718. 2160721",
                "links": null
            },
            "BIBREF151": {
                "ref_id": "b151",
                "title": "Evaluating story generation systems using automated linguistic analyses",
                "authors": [
                    {
                        "first": "Melissa",
                        "middle": [],
                        "last": "Roemmele",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "S"
                        ],
                        "last": "Gordon",
                        "suffix": ""
                    },
                    {
                        "first": "Reid",
                        "middle": [],
                        "last": "Swanson",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SIGKDD 2017 Workshop on Machine Learning for Creativity",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Melissa Roemmele, Andrew S Gordon, and Reid Swanson. 2017. Evaluating story generation systems using automated linguistic analyses. In SIGKDD 2017 Workshop on Machine Learning for Creativity. 13-17. https://api.semanticscholar.org/CorpusID:4776895",
                "links": null
            },
            "BIBREF152": {
                "ref_id": "b152",
                "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                "authors": [
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Rombach",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Blattmann",
                        "suffix": ""
                    },
                    {
                        "first": "Dominik",
                        "middle": [],
                        "last": "Lorenz",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Esser",
                        "suffix": ""
                    },
                    {
                        "first": "Bj\u00f6rn",
                        "middle": [],
                        "last": "Ommer",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "10674--10685",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR52688.2022.01042"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022. IEEE, 10674-10685. https://doi.org/10.1109/CVPR52688.2022.01042",
                "links": null
            },
            "BIBREF153": {
                "ref_id": "b153",
                "title": "Better than Random: Reliable NLG Human Evaluation with Constrained Active Sampling",
                "authors": [
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Ruan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Pu",
                        "suffix": ""
                    },
                    {
                        "first": "Mingqi",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaojun",
                        "middle": [],
                        "last": "Wan",
                        "suffix": ""
                    },
                    {
                        "first": "Yuesheng",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "18915--18923",
                "other_ids": {
                    "DOI": [
                        "10.1609/AAAI.V38I17.29857"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jie Ruan, Xiao Pu, Mingqi Gao, Xiaojun Wan, and Yuesheng Zhu. 2024. Better than Random: Reliable NLG Human Evaluation with Constrained Active Sampling. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (Eds.). AAAI Press, 18915-18923. https://doi.org/10.1609/AAAI.V38I17.29857",
                "links": null
            },
            "BIBREF154": {
                "ref_id": "b154",
                "title": "A Metric for Distributions with Applications to Image Databases",
                "authors": [
                    {
                        "first": "Yossi",
                        "middle": [],
                        "last": "Rubner",
                        "suffix": ""
                    },
                    {
                        "first": "Carlo",
                        "middle": [],
                        "last": "Tomasi",
                        "suffix": ""
                    },
                    {
                        "first": "Leonidas",
                        "middle": [
                            "J"
                        ],
                        "last": "Guibas",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Proceedings of the Sixth International Conference on Computer Vision (ICCV-98)",
                "volume": "",
                "issue": "",
                "pages": "59--66",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICCV.1998.710701"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas. 1998. A Metric for Distributions with Applications to Image Databases. In Proceedings of the Sixth International Conference on Computer Vision (ICCV-98), Bombay, India, January 4-7, 1998. IEEE Computer Society, 59-66. https: //doi.org/10.1109/ICCV.1998.710701",
                "links": null
            },
            "BIBREF155": {
                "ref_id": "b155",
                "title": "The Earth Mover's Distance as a Metric for Image Retrieval",
                "authors": [
                    {
                        "first": "Yossi",
                        "middle": [],
                        "last": "Rubner",
                        "suffix": ""
                    },
                    {
                        "first": "Carlo",
                        "middle": [],
                        "last": "Tomasi",
                        "suffix": ""
                    },
                    {
                        "first": "Leonidas",
                        "middle": [
                            "J"
                        ],
                        "last": "Guibas",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Int. J. Comput. Vis",
                "volume": "40",
                "issue": "",
                "pages": "99--121",
                "other_ids": {
                    "DOI": [
                        "10.1023/A:1026543900054"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas. 2000. The Earth Mover's Distance as a Metric for Image Retrieval. Int. J. Comput. Vis. 40, 2 (2000), 99-121. https://doi.org/10.1023/A:1026543900054",
                "links": null
            },
            "BIBREF156": {
                "ref_id": "b156",
                "title": "Branch-solve-merge improves large language model evaluation and generation",
                "authors": [
                    {
                        "first": "Swarnadeep",
                        "middle": [],
                        "last": "Saha",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Asli",
                        "middle": [],
                        "last": "Celikyilmaz",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "Xian",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2310.15123"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 2024. Branch-solve-merge improves large language model evaluation and generation. (2024). arXiv:2310.15123 https://arxiv.org/abs/2310.15123",
                "links": null
            },
            "BIBREF157": {
                "ref_id": "b157",
                "title": "The New York Times Annotated Corpus LDC2008T19. Philadelphia: Linguistic Data Consortium",
                "authors": [
                    {
                        "first": "Evang",
                        "middle": [],
                        "last": "Sandhaus",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Evang Sandhaus. 2008. The New York Times Annotated Corpus LDC2008T19. Philadelphia: Linguistic Data Consortium (2008). https://doi.org/ 11272.1/AB2/GZC6PL",
                "links": null
            },
            "BIBREF158": {
                "ref_id": "b158",
                "title": "Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models",
                "authors": [
                    {
                        "first": "Maarten",
                        "middle": [],
                        "last": "Sap",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Horvitz",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "W"
                        ],
                        "last": "Pennebaker",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "volume": "",
                "issue": "",
                "pages": "1970--1978",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.ACL-MAIN.178"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maarten Sap, Eric Horvitz, Yejin Choi, Noah A. Smith, and James W. Pennebaker. 2020. Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 1970-1978. https://doi.org/10.18653/V1/2020.ACL-MAIN.178",
                "links": null
            },
            "BIBREF159": {
                "ref_id": "b159",
                "title": "BLEURT: Learning Robust Metrics for Text Generation",
                "authors": [
                    {
                        "first": "Thibault",
                        "middle": [],
                        "last": "Sellam",
                        "suffix": ""
                    },
                    {
                        "first": "Dipanjan",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Ankur",
                        "middle": [
                            "P"
                        ],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "volume": "",
                "issue": "",
                "pages": "7881--7892",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2020.ACL-MAIN.704"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020. BLEURT: Learning Robust Metrics for Text Generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 7881-7892. https://doi.org/10.18653/V1/2020.ACL-MAIN.704",
                "links": null
            },
            "BIBREF160": {
                "ref_id": "b160",
                "title": "Long and Diverse Text Generation with Planning-based Hierarchical Variational Model",
                "authors": [
                    {
                        "first": "Zhihong",
                        "middle": [],
                        "last": "Shao",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiangtao",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "Wenfei",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",
                "volume": "",
                "issue": "",
                "pages": "3255--3266",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/D19-1321"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. 2019. Long and Diverse Text Generation with Planning-based Hierarchical Variational Model. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 3255-3266. https://doi.org/10.18653/V1/D19-1321",
                "links": null
            },
            "BIBREF161": {
                "ref_id": "b161",
                "title": "A comprehensive survey on image captioning: from handcrafted to deep learning-based techniques, a taxonomy and open research issues",
                "authors": [
                    {
                        "first": "Himanshu",
                        "middle": [],
                        "last": "Sharma",
                        "suffix": ""
                    },
                    {
                        "first": "Devanand",
                        "middle": [],
                        "last": "Padha",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Artif. Intell. Rev",
                "volume": "56",
                "issue": "",
                "pages": "13619--13661",
                "other_ids": {
                    "DOI": [
                        "10.1007/S10462-023-10488-2"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Himanshu Sharma and Devanand Padha. 2023. A comprehensive survey on image captioning: from handcrafted to deep learning-based techniques, a taxonomy and open research issues. Artif. Intell. Rev. 56, 11 (2023), 13619-13661. https://doi.org/10.1007/S10462-023-10488-2",
                "links": null
            },
            "BIBREF162": {
                "ref_id": "b162",
                "title": "EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching",
                "authors": [
                    {
                        "first": "Yaya",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Haiyang",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Chunfeng",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Weiming",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Zheng-Jun",
                        "middle": [],
                        "last": "Zha",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": "2022",
                "issue": "",
                "pages": "17908--17917",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR52688.2022.01740"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yaya Shi, Xu Yang, Haiyang Xu, Chunfeng Yuan, Bing Li, Weiming Hu, and Zheng-Jun Zha. 2022. EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022. IEEE, 17908-17917. https://doi.org/10.1109/CVPR52688.2022.01740",
                "links": null
            },
            "BIBREF163": {
                "ref_id": "b163",
                "title": "Toward Diverse Text Generation with Inverse Reinforcement Learning",
                "authors": [
                    {
                        "first": "Zhan",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Xinchi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Xipeng",
                        "middle": [],
                        "last": "Qiu",
                        "suffix": ""
                    },
                    {
                        "first": "Xuanjing",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "4361--4367",
                "other_ids": {
                    "DOI": [
                        "10.24963/IJCAI.2018/606"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing Huang. 2018. Toward Diverse Text Generation with Inverse Reinforcement Learning. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, J\u00e9r\u00f4me Lang (Ed.). ijcai.org, 4361-4367. https://doi.org/10.24963/IJCAI.2018/606",
                "links": null
            },
            "BIBREF164": {
                "ref_id": "b164",
                "title": "The proof and measurement of association between two things",
                "authors": [
                    {
                        "first": "Charles",
                        "middle": [],
                        "last": "Spearman",
                        "suffix": ""
                    }
                ],
                "year": 1961,
                "venue": "The American Journal of Psychology",
                "volume": "15",
                "issue": "",
                "pages": "72--101",
                "other_ids": {
                    "DOI": [
                        "10.2307/1412159"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Charles Spearman. 1961. The proof and measurement of association between two things. The American Journal of Psychology 15 (1961), 72-101. https://doi.org/10.2307/1412159",
                "links": null
            },
            "BIBREF165": {
                "ref_id": "b165",
                "title": "A pseudo-metric between probability distributions based on depth-trimmed regions",
                "authors": [
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Staerman",
                        "suffix": ""
                    },
                    {
                        "first": "Pavlo",
                        "middle": [],
                        "last": "Mozharovskyi",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Colombo",
                        "suffix": ""
                    },
                    {
                        "first": "St\u00e9phan",
                        "middle": [],
                        "last": "Cl\u00e9men\u00e7on",
                        "suffix": ""
                    },
                    {
                        "first": "Florence D'alch\u00e9",
                        "middle": [],
                        "last": "Buc",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2103.12711"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Guillaume Staerman, Pavlo Mozharovskyi, Pierre Colombo, St\u00e9phan Cl\u00e9men\u00e7on, and Florence d'Alch\u00e9 Buc. 2022. A pseudo-metric between probability distributions based on depth-trimmed regions. arXiv:2103.12711 https://arxiv.org/abs/2103.12711",
                "links": null
            },
            "BIBREF166": {
                "ref_id": "b166",
                "title": "Putting GPT-3's Creativity to the (Alternative Uses) Test",
                "authors": [
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Stevenson",
                        "suffix": ""
                    },
                    {
                        "first": "Iris",
                        "middle": [],
                        "last": "Smal",
                        "suffix": ""
                    },
                    {
                        "first": "Matthijs",
                        "middle": [],
                        "last": "Baas",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [
                            "P P"
                        ],
                        "last": "Raoul",
                        "suffix": ""
                    },
                    {
                        "first": "Han",
                        "middle": [
                            "L J"
                        ],
                        "last": "Grasman",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Van Der Maas",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 13th International Conference on Computational Creativity",
                "volume": "",
                "issue": "",
                "pages": "164--168",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Claire Stevenson, Iris Smal, Matthijs Baas, Raoul P. P. P. Grasman, and Han L. J. van der Maas. 2022. Putting GPT-3's Creativity to the (Alternative Uses) Test. In Proceedings of the 13th International Conference on Computational Creativity, Bozen-Bolzano, Italy, June 27 -July 1, 2022, Maria M. Hedblom, Anna Aurora Kantosalo, Roberto Confalonieri, Oliver Kutz, and Tony Veale (Eds.). Association for Computational Creativity (ACC), 164-168. http://computationalcreativity.net/iccc22/papers/ICCC-2022_paper_140.pdf",
                "links": null
            },
            "BIBREF167": {
                "ref_id": "b167",
                "title": "Patterns of interaction in ESL pair work",
                "authors": [
                    {
                        "first": "Neomy",
                        "middle": [],
                        "last": "Storch",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Language learning",
                "volume": "52",
                "issue": "",
                "pages": "119--158",
                "other_ids": {
                    "DOI": [
                        "10.1111/1467-9922.00179"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Neomy Storch. 2002. Patterns of interaction in ESL pair work. Language learning 52, 1 (2002), 119-158. https://doi.org/10.1111/1467-9922.00179",
                "links": null
            },
            "BIBREF168": {
                "ref_id": "b168",
                "title": "BLEU is Not Suitable for the Evaluation of Text Simplification",
                "authors": [
                    {
                        "first": "Elior",
                        "middle": [],
                        "last": "Sulem",
                        "suffix": ""
                    },
                    {
                        "first": "Omri",
                        "middle": [],
                        "last": "Abend",
                        "suffix": ""
                    },
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Rappoport",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "738--744",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/D18-1081"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Elior Sulem, Omri Abend, and Ari Rappoport. 2018. BLEU is Not Suitable for the Evaluation of Text Simplification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -November 4, 2018, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii (Eds.). Association for Computational Linguistics, 738-744. https://doi.org/10.18653/V1/D18-1081",
                "links": null
            },
            "BIBREF169": {
                "ref_id": "b169",
                "title": "BLEU is Not Suitable for the Evaluation of Text Simplification",
                "authors": [
                    {
                        "first": "Elior",
                        "middle": [],
                        "last": "Sulem",
                        "suffix": ""
                    },
                    {
                        "first": "Omri",
                        "middle": [],
                        "last": "Abend",
                        "suffix": ""
                    },
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Rappoport",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "738--744",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/D18-1081"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Elior Sulem, Omri Abend, and Ari Rappoport. 2018. BLEU is Not Suitable for the Evaluation of Text Simplification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -November 4, 2018, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii (Eds.). Association for Computational Linguistics, 738-744. https://doi.org/10.18653/V1/D18-1081",
                "links": null
            },
            "BIBREF170": {
                "ref_id": "b170",
                "title": "GROOViST: A Metric for Grounding Objects in Visual Storytelling",
                "authors": [
                    {
                        "first": "Aditya",
                        "middle": [
                            "K"
                        ],
                        "last": "Surikuchi",
                        "suffix": ""
                    },
                    {
                        "first": "Sandro",
                        "middle": [],
                        "last": "Pezzelle",
                        "suffix": ""
                    },
                    {
                        "first": "Raquel",
                        "middle": [],
                        "last": "Fern\u00e1ndez",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023",
                "volume": "",
                "issue": "",
                "pages": "3331--3339",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.EMNLP-MAIN.202"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aditya K. Surikuchi, Sandro Pezzelle, and Raquel Fern\u00e1ndez. 2023. GROOViST: A Metric for Grounding Objects in Visual Storytelling. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 3331-3339. https://doi.org/10.18653/V1/2023.EMNLP-MAIN.202",
                "links": null
            },
            "BIBREF171": {
                "ref_id": "b171",
                "title": "Say Anything: A Demonstration of Open Domain Interactive Digital Storytelling",
                "authors": [
                    {
                        "first": "Reid",
                        "middle": [],
                        "last": "Swanson",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "S"
                        ],
                        "last": "Gordon",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Second Joint International Conference on Interactive Digital Storytelling",
                "volume": "5915",
                "issue": "",
                "pages": "324--327",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-642-10643-9_40"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Reid Swanson and Andrew S. Gordon. 2009. Say Anything: A Demonstration of Open Domain Interactive Digital Storytelling. In Interactive Storytelling, Second Joint International Conference on Interactive Digital Storytelling, ICIDS 2009, Guimar\u00e3es, Portugal, December 9-11, 2009. Proceedings (Lecture Notes in Computer Science, Vol. 5915), Ido Iurgel, Nelson Zagalo, and Paolo Petta (Eds.). Springer, 324-327. https://doi.org/10.1007/978-3- 642-10643-9_40",
                "links": null
            },
            "BIBREF172": {
                "ref_id": "b172",
                "title": "Rethinking the Inception Architecture for Computer Vision",
                "authors": [
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Szegedy",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Vanhoucke",
                        "suffix": ""
                    },
                    {
                        "first": "Sergey",
                        "middle": [],
                        "last": "Ioffe",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathon",
                        "middle": [],
                        "last": "Shlens",
                        "suffix": ""
                    },
                    {
                        "first": "Zbigniew",
                        "middle": [],
                        "last": "Wojna",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "2818--2826",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2016.308"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2016. Rethinking the Inception Architecture for Computer Vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 2818-2826. https://doi.org/10.1109/CVPR.2016.308",
                "links": null
            },
            "BIBREF173": {
                "ref_id": "b173",
                "title": "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems",
                "authors": [
                    {
                        "first": "Chongyang",
                        "middle": [],
                        "last": "Tao",
                        "suffix": ""
                    },
                    {
                        "first": "Lili",
                        "middle": [],
                        "last": "Mou",
                        "suffix": ""
                    },
                    {
                        "first": "Dongyan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)",
                "volume": "",
                "issue": "",
                "pages": "722--729",
                "other_ids": {
                    "DOI": [
                        "10.1609/AAAI.V32I1.11321"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chongyang Tao, Lili Mou, Dongyan Zhao, and Rui Yan. 2018. RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q. Weinberger (Eds.). AAAI Press, 722-729. https://doi.org/10.1609/AAAI.V32I1.11321",
                "links": null
            },
            "BIBREF174": {
                "ref_id": "b174",
                "title": "Open Foundation and Fine-Tuned Chat Models",
                "authors": [
                    {
                        "first": "Hugo",
                        "middle": [],
                        "last": "Touvron",
                        "suffix": ""
                    },
                    {
                        "first": "Louis",
                        "middle": [],
                        "last": "Martin",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Stone",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Albert",
                        "suffix": ""
                    },
                    {
                        "first": "Amjad",
                        "middle": [],
                        "last": "Almahairi",
                        "suffix": ""
                    },
                    {
                        "first": "Yasmine",
                        "middle": [],
                        "last": "Babaei",
                        "suffix": ""
                    },
                    {
                        "first": "Nikolay",
                        "middle": [],
                        "last": "Bashlykov",
                        "suffix": ""
                    },
                    {
                        "first": "Soumya",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Prajjwal",
                        "middle": [],
                        "last": "Bhargava",
                        "suffix": ""
                    },
                    {
                        "first": "Shruti",
                        "middle": [],
                        "last": "Bhosale",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Bikel",
                        "suffix": ""
                    },
                    {
                        "first": "Lukas",
                        "middle": [],
                        "last": "Blecher",
                        "suffix": ""
                    },
                    {
                        "first": "Cristian Canton",
                        "middle": [],
                        "last": "Ferrer",
                        "suffix": ""
                    },
                    {
                        "first": "Moya",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Guillem",
                        "middle": [],
                        "last": "Cucurull",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Esiobu",
                        "suffix": ""
                    },
                    {
                        "first": "Jude",
                        "middle": [],
                        "last": "Fernandes",
                        "suffix": ""
                    },
                    {
                        "first": "Jeremy",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Wenyin",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Fuller",
                        "suffix": ""
                    },
                    {
                        "first": "Cynthia",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Vedanuj",
                        "middle": [],
                        "last": "Goswami",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Hartshorn",
                        "suffix": ""
                    },
                    {
                        "first": "Saghar",
                        "middle": [],
                        "last": "Hosseini",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    },
                    {
                        "first": "Hakan",
                        "middle": [],
                        "last": "Inan",
                        "suffix": ""
                    },
                    {
                        "first": "Marcin",
                        "middle": [],
                        "last": "Kardas",
                        "suffix": ""
                    },
                    {
                        "first": "Viktor",
                        "middle": [],
                        "last": "Kerkez",
                        "suffix": ""
                    },
                    {
                        "first": "Madian",
                        "middle": [],
                        "last": "Khabsa",
                        "suffix": ""
                    },
                    {
                        "first": "Isabel",
                        "middle": [],
                        "last": "Kloumann",
                        "suffix": ""
                    },
                    {
                        "first": "Artem",
                        "middle": [],
                        "last": "Korenev",
                        "suffix": ""
                    },
                    {
                        "first": "Punit",
                        "middle": [],
                        "last": "Singh Koura",
                        "suffix": ""
                    },
                    {
                        "first": "Marie-Anne",
                        "middle": [],
                        "last": "Lachaux",
                        "suffix": ""
                    },
                    {
                        "first": "Thibaut",
                        "middle": [],
                        "last": "Lavril",
                        "suffix": ""
                    },
                    {
                        "first": "Jenya",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Diana",
                        "middle": [],
                        "last": "Liskovich",
                        "suffix": ""
                    },
                    {
                        "first": "Yinghai",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuning",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Xavier",
                        "middle": [],
                        "last": "Martinet",
                        "suffix": ""
                    },
                    {
                        "first": "Todor",
                        "middle": [],
                        "last": "Mihaylov",
                        "suffix": ""
                    },
                    {
                        "first": "Pushkar",
                        "middle": [],
                        "last": "Mishra",
                        "suffix": ""
                    },
                    {
                        "first": "Igor",
                        "middle": [],
                        "last": "Molybog",
                        "suffix": ""
                    },
                    {
                        "first": "Yixin",
                        "middle": [],
                        "last": "Nie",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Poulton",
                        "suffix": ""
                    },
                    {
                        "first": "Jeremy",
                        "middle": [],
                        "last": "Reizenstein",
                        "suffix": ""
                    },
                    {
                        "first": "Rashi",
                        "middle": [],
                        "last": "Rungta",
                        "suffix": ""
                    },
                    {
                        "first": "Kalyan",
                        "middle": [],
                        "last": "Saladi",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Schelten",
                        "suffix": ""
                    },
                    {
                        "first": "Ruan",
                        "middle": [],
                        "last": "Silva",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "2",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2307.09288[cs.CL"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL] https://arxiv.org/abs/2307.09288",
                "links": null
            },
            "BIBREF175": {
                "ref_id": "b175",
                "title": "FVD: A new Metric for Video Generation",
                "authors": [
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Unterthiner",
                        "suffix": ""
                    },
                    {
                        "first": "Karol",
                        "middle": [],
                        "last": "Sjoerd Van Steenkiste",
                        "suffix": ""
                    },
                    {
                        "first": "Rapha\u00ebl",
                        "middle": [],
                        "last": "Kurach",
                        "suffix": ""
                    },
                    {
                        "first": "Marcin",
                        "middle": [],
                        "last": "Marinier",
                        "suffix": ""
                    },
                    {
                        "first": "Sylvain",
                        "middle": [],
                        "last": "Michalski",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gelly",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Deep Generative Models for Highly Structured Data, ICLR 2019 Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Rapha\u00ebl Marinier, Marcin Michalski, and Sylvain Gelly. 2019. FVD: A new Metric for Video Generation. In Deep Generative Models for Highly Structured Data, ICLR 2019 Workshop, New Orleans, Louisiana, United States, May 6, 2019. OpenReview.net. https://openreview.net/forum?id=rylgEULtdN",
                "links": null
            },
            "BIBREF176": {
                "ref_id": "b176",
                "title": "CIDEr: Consensus-based image description evaluation",
                "authors": [
                    {
                        "first": "C",
                        "middle": [
                            "Lawrence"
                        ],
                        "last": "Ramakrishna Vedantam",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Zitnick",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015",
                "volume": "",
                "issue": "",
                "pages": "4566--4575",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2015.7299087"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015. IEEE Computer Society, 4566-4575. https://doi.org/10.1109/ CVPR.2015.7299087",
                "links": null
            },
            "BIBREF177": {
                "ref_id": "b177",
                "title": "Towards Inter-character Relationship-driven Story Generation",
                "authors": [
                    {
                        "first": "Anvesh",
                        "middle": [],
                        "last": "Rao Vijjini",
                        "suffix": ""
                    },
                    {
                        "first": "Faeze",
                        "middle": [],
                        "last": "Brahman",
                        "suffix": ""
                    },
                    {
                        "first": "Snigdha",
                        "middle": [],
                        "last": "Chaturvedi",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "8970--8987",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2022.EMNLP-MAIN.613"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Anvesh Rao Vijjini, Faeze Brahman, and Snigdha Chaturvedi. 2022. Towards Inter-character Relationship-driven Story Generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 8970-8987. https://doi.org/10.18653/V1/2022. EMNLP-MAIN.613",
                "links": null
            },
            "BIBREF178": {
                "ref_id": "b178",
                "title": "Learning Personalized Story Evaluation",
                "authors": [
                    {
                        "first": "Danqing",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Hanlin",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaomeng",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yuandong",
                        "middle": [],
                        "last": "Tian",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2310.03304"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei Li, and Yuandong Tian. 2024. Learning Personalized Story Evaluation. arXiv:2310.03304 https://arxiv.org/abs/2310.03304",
                "links": null
            },
            "BIBREF179": {
                "ref_id": "b179",
                "title": "RoViST: Learning Robust Metrics for Visual Storytelling",
                "authors": [
                    {
                        "first": "Eileen",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Soyeon",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Caren",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Josiah",
                        "middle": [],
                        "last": "Poon",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Findings of the Association for Computational Linguistics: NAACL 2022",
                "volume": "",
                "issue": "",
                "pages": "2691--2702",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2022.FINDINGS-NAACL.206"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Eileen Wang, Soyeon Caren Han, and Josiah Poon. 2022. RoViST: Learning Robust Metrics for Visual Storytelling. In Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, Marine Carpuat, Marie-Catherine de Marneffe, and Iv\u00e1n Vladimir Meza Ru\u00edz (Eds.). Association for Computational Linguistics, 2691-2702. https://doi.org/10.18653/V1/2022.FINDINGS-NAACL.206",
                "links": null
            },
            "BIBREF180": {
                "ref_id": "b180",
                "title": "Is chatgpt a good nlg evaluator? a preliminary study",
                "authors": [
                    {
                        "first": "Jiaan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yunlong",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Fandong",
                        "middle": [],
                        "last": "Meng",
                        "suffix": ""
                    },
                    {
                        "first": "Zengkui",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Haoxiang",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Zhixu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jinan",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Qu",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2303.04048"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. [n. d.]. Is chatgpt a good nlg evaluator? a preliminary study. ([n. d.]). arXiv:2303.04048 https://arxiv.org/abs/2303.04048",
                "links": null
            },
            "BIBREF181": {
                "ref_id": "b181",
                "title": "Large language models are not fair evaluators",
                "authors": [
                    {
                        "first": "Peiyi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Dawei",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Binghuai",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Yunbo",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Qi",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyu",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhifang",
                        "middle": [],
                        "last": "Sui",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2305.17926"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. (2023). arXiv:2305.17926 https://arxiv.org/abs/2305.17926",
                "links": null
            },
            "BIBREF182": {
                "ref_id": "b182",
                "title": "A critic for language model generation",
                "authors": [
                    {
                        "first": "Tianlu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Ping",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Ellen",
                        "middle": [],
                        "last": "Xiaoqing",
                        "suffix": ""
                    },
                    {
                        "first": "Sean O'",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Ramakanth",
                        "middle": [],
                        "last": "Brien",
                        "suffix": ""
                    },
                    {
                        "first": "Jane",
                        "middle": [],
                        "last": "Pasunuru",
                        "suffix": ""
                    },
                    {
                        "first": "Olga",
                        "middle": [],
                        "last": "Dwivedi-Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Golovneva",
                        "suffix": ""
                    },
                    {
                        "first": "Maryam",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Asli",
                        "middle": [],
                        "last": "Fazel-Zarandi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Celikyilmaz",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2308.04592"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O'Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023. Shepherd: A critic for language model generation. arXiv:2308.04592 https://arxiv.org/abs/2308.04592",
                "links": null
            },
            "BIBREF183": {
                "ref_id": "b183",
                "title": "Internvideo: General video foundation models via generative and discriminative learning",
                "authors": [
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kunchang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yizhuo",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Yinan",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Bingkun",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyu",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Hongjie",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jilan",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Zun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2212.03191"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. 2022. Internvideo: General video foundation models via generative and discriminative learning. (2022). arXiv:2212.03191 https://arxiv.org/abs/2212.03191",
                "links": null
            },
            "BIBREF184": {
                "ref_id": "b184",
                "title": "Improving Pacing in Long-Form Story Planning",
                "authors": [
                    {
                        "first": "Yichen",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoming",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
                "volume": "",
                "issue": "",
                "pages": "10788--10845",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2023.findings-emnlp.723"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yichen Wang, Kevin Yang, Xiaoming Liu, and Dan Klein. 2023. Improving Pacing in Long-Form Story Planning. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 10788-10845. https://doi.org/10.18653/v1/2023.findings-emnlp.723",
                "links": null
            },
            "BIBREF185": {
                "ref_id": "b185",
                "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
                "authors": [
                    {
                        "first": "Yidong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuohao",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengran",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Linyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Cunxiang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Chaoya",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Jindong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xing",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Ye",
                        "suffix": ""
                    },
                    {
                        "first": "Shikun",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.48550/ARXIV.2306.05087"
                    ],
                    "arXiv": [
                        "arXiv:2306.05087"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. 2023. PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. CoRR abs/2306.05087 (2023). https://doi.org/10.48550/ARXIV.2306.05087 arXiv:2306.05087",
                "links": null
            },
            "BIBREF186": {
                "ref_id": "b186",
                "title": "Chain-ofthought prompting elicits reasoning in large language models",
                "authors": [
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Xuezhi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Dale",
                        "middle": [],
                        "last": "Schuurmans",
                        "suffix": ""
                    },
                    {
                        "first": "Maarten",
                        "middle": [],
                        "last": "Bosma",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Ichter",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Xia",
                        "suffix": ""
                    },
                    {
                        "first": "Ed",
                        "middle": [
                            "H"
                        ],
                        "last": "Chi",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "Denny",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 36th International Conference on Neural Information Processing Systems (NIPS '22)",
                "volume": "14",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2024. Chain-of- thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems (NIPS '22). Curran Associates Inc., Red Hook, NY, USA, Article 1800, 14 pages. http://papers.nips.cc/paper_files/paper/2022/hash/ 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html",
                "links": null
            },
            "BIBREF187": {
                "ref_id": "b187",
                "title": "Disentangling aesthetic and technical effects for video quality assessment of user generated content",
                "authors": [
                    {
                        "first": "Haoning",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Liao",
                        "suffix": ""
                    },
                    {
                        "first": "Chaofeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Jingwen",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    },
                    {
                        "first": "Annan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Wenxiu",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Qiong",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "Weisi",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2211.04894"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Haoning Wu, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. 2023. Disentangling aesthetic and technical effects for video quality assessment of user generated content. (2023). arXiv:2211.04894 https://arxiv.org/abs/2211.04894",
                "links": null
            },
            "BIBREF188": {
                "ref_id": "b188",
                "title": "A comparative study of open-source large language models, gpt-4 and claude 2: Multiple-choice test taking in nephrology",
                "authors": [
                    {
                        "first": "Sean",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Koo",
                        "suffix": ""
                    },
                    {
                        "first": "Lesley",
                        "middle": [],
                        "last": "Blum",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Black",
                        "suffix": ""
                    },
                    {
                        "first": "Liyo",
                        "middle": [],
                        "last": "Kao",
                        "suffix": ""
                    },
                    {
                        "first": "Fabien",
                        "middle": [],
                        "last": "Scalzo",
                        "suffix": ""
                    },
                    {
                        "first": "Ira",
                        "middle": [],
                        "last": "Kurtz",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2308.04709"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sean Wu, Michael Koo, Lesley Blum, Andy Black, Liyo Kao, Fabien Scalzo, and Ira Kurtz. 2023. A comparative study of open-source large language models, gpt-4 and claude 2: Multiple-choice test taking in nephrology. (2023). arXiv:2308.04709 https://arxiv.org/abs/2308.04709",
                "links": null
            },
            "BIBREF189": {
                "ref_id": "b189",
                "title": "Can very large pretrained language models learn storytelling with a few examples",
                "authors": [
                    {
                        "first": "Zhuohan",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Cohn",
                        "suffix": ""
                    },
                    {
                        "first": "Jey",
                        "middle": [
                            "Han"
                        ],
                        "last": "Lau",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2301.09790"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhuohan Xie, Trevor Cohn, and Jey Han Lau. 2023. Can very large pretrained language models learn storytelling with a few examples. (2023). arXiv:2301.09790 https://arxiv.org/abs/2301.09790",
                "links": null
            },
            "BIBREF190": {
                "ref_id": "b190",
                "title": "A Study of Large Language Models in Storytelling",
                "authors": [
                    {
                        "first": "Zhuohan",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Cohn",
                        "suffix": ""
                    },
                    {
                        "first": "Jey",
                        "middle": [
                            "Han"
                        ],
                        "last": "Lau",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 16th International Natural Language Generation Conference, INLG 2023",
                "volume": "",
                "issue": "",
                "pages": "323--351",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.INLG-MAIN.23"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhuohan Xie, Trevor Cohn, and Jey Han Lau. 2023. The Next Chapter: A Study of Large Language Models in Storytelling. In Proceedings of the 16th International Natural Language Generation Conference, INLG 2023, Prague, Czechia, September 11 -15, 2023, C. Maria Keet, Hung-Yi Lee, and Sina Zarrie\u00df (Eds.). Association for Computational Linguistics, 323-351. https://doi.org/10.18653/V1/2023.INLG-MAIN.23",
                "links": null
            },
            "BIBREF191": {
                "ref_id": "b191",
                "title": "Deltascore: Fine-grained story evaluation with perturbations",
                "authors": [
                    {
                        "first": "Zhuohan",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Miao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Cohn",
                        "suffix": ""
                    },
                    {
                        "first": "Jey",
                        "middle": [],
                        "last": "Lau",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
                "volume": "",
                "issue": "",
                "pages": "5317--5331",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2023.findings-emnlp.353"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhuohan Xie, Miao Li, Trevor Cohn, and Jey Lau. 2023. Deltascore: Fine-grained story evaluation with perturbations. In Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore, 5317-5331. https://doi.org/10.18653/v1/2023.findings-emnlp.353",
                "links": null
            },
            "BIBREF192": {
                "ref_id": "b192",
                "title": "Transcript to video: Efficient clip sequencing from texts",
                "authors": [
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Fabian",
                        "middle": [],
                        "last": "Caba Heilbron",
                        "suffix": ""
                    },
                    {
                        "first": "Dahua",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 30th ACM International Conference on Multimedia",
                "volume": "",
                "issue": "",
                "pages": "5407--5416",
                "other_ids": {
                    "DOI": [
                        "10.1145/3503161.3548268"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yu Xiong, Fabian Caba Heilbron, and Dahua Lin. 2022. Transcript to video: Efficient clip sequencing from texts. In Proceedings of the 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 -14, 2022. ACM, 5407-5416. https://doi.org/10.1145/3503161.3548268",
                "links": null
            },
            "BIBREF193": {
                "ref_id": "b193",
                "title": "INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback",
                "authors": [
                    {
                        "first": "Wenda",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Danqing",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Liangming",
                        "middle": [],
                        "last": "Pan",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenqiao",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Markus",
                        "middle": [],
                        "last": "Freitag",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023",
                "volume": "",
                "issue": "",
                "pages": "5967--5994",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.EMNLP-MAIN.365"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li. 2023. INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 5967-5994. https://doi.org/10.18653/V1/2023.EMNLP-MAIN.365",
                "links": null
            },
            "BIBREF194": {
                "ref_id": "b194",
                "title": "DOC: Improving Long Story Coherence With Detailed Outline Control",
                "authors": [
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "Nanyun",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Yuandong",
                        "middle": [],
                        "last": "Tian",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "3378--3465",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2023.ACL-LONG.190"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. 2023. DOC: Improving Long Story Coherence With Detailed Outline Control. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 3378-3465. https: //doi.org/10.18653/V1/2023.ACL-LONG.190",
                "links": null
            },
            "BIBREF195": {
                "ref_id": "b195",
                "title": "Re3: Generating Longer Stories With Recursive Reprompting and Revision",
                "authors": [
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Yuandong",
                        "middle": [],
                        "last": "Tian",
                        "suffix": ""
                    },
                    {
                        "first": "Nanyun",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "4393--4479",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2022.EMNLP-MAIN.296"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. 2022. Re3: Generating Longer Stories With Recursive Reprompting and Revision. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 4393-4479. https: //doi.org/10.18653/V1/2022.EMNLP-MAIN.296",
                "links": null
            },
            "BIBREF196": {
                "ref_id": "b196",
                "title": "Plan-and-Write: Towards Better Automatic Storytelling",
                "authors": [
                    {
                        "first": "Lili",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Nanyun",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [
                            "M"
                        ],
                        "last": "Weischedel",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    },
                    {
                        "first": "Dongyan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "7378--7385",
                "other_ids": {
                    "DOI": [
                        "10.1609/AAAI.V33I01.33017378"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-and-Write: Towards Better Automatic Storytelling. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019. AAAI Press, 7378-7385. https://doi.org/10.1609/AAAI.V33I01.33017378",
                "links": null
            },
            "BIBREF197": {
                "ref_id": "b197",
                "title": "GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency",
                "authors": [
                    {
                        "first": "Catherine",
                        "middle": [],
                        "last": "Yeh",
                        "suffix": ""
                    },
                    {
                        "first": "Gonzalo",
                        "middle": [],
                        "last": "Ramos",
                        "suffix": ""
                    },
                    {
                        "first": "Rachel",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Huntington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Banks",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2402.08855"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Catherine Yeh, Gonzalo Ramos, Rachel Ng, Andy Huntington, and Richard Banks. 2024. GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency. (2024). arXiv:2402.08855 https://arxiv.org/abs/2402.08855",
                "links": null
            },
            "BIBREF198": {
                "ref_id": "b198",
                "title": "EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation",
                "authors": [
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "You",
                        "suffix": ""
                    },
                    {
                        "first": "Wenshan",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Yaobo",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Shaoguang",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Chenfei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Yuzhe",
                        "middle": [],
                        "last": "Cai",
                        "suffix": ""
                    },
                    {
                        "first": "Yiduo",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Yan",
                        "middle": [],
                        "last": "Xia",
                        "suffix": ""
                    },
                    {
                        "first": "Furu",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Duan",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.48550/ARXIV.2310.08185"
                    ],
                    "arXiv": [
                        "arXiv:2310.08185"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wang You, Wenshan Wu, Yaobo Liang, Shaoguang Mao, Chenfei Wu, Maosong Cao, Yuzhe Cai, Yiduo Guo, Yan Xia, Furu Wei, and Nan Duan. 2023. EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation. CoRR abs/2310.08185 (2023). https://doi.org/10.48550/ARXIV.2310.08185 arXiv:2310.08185",
                "links": null
            },
            "BIBREF199": {
                "ref_id": "b199",
                "title": "Wordcraft: Story Writing With Large Language Models",
                "authors": [
                    {
                        "first": "Ann",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Coenen",
                        "suffix": ""
                    },
                    {
                        "first": "Emily",
                        "middle": [],
                        "last": "Reif",
                        "suffix": ""
                    },
                    {
                        "first": "Daphne",
                        "middle": [],
                        "last": "Ippolito",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "IUI 2022: 27th International Conference on Intelligent User Interfaces",
                "volume": "",
                "issue": "",
                "pages": "841--852",
                "other_ids": {
                    "DOI": [
                        "10.1145/3490099.3511105"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. 2022. Wordcraft: Story Writing With Large Language Models. In IUI 2022: 27th International Conference on Intelligent User Interfaces, Helsinki, Finland, March 22 -25, 2022, Giulio Jacucci, Samuel Kaski, Cristina Conati, Simone Stumpf, Tuukka Ruotsalo, and Krzysztof Gajos (Eds.). ACM, 841-852. https://doi.org/10.1145/3490099.3511105",
                "links": null
            },
            "BIBREF200": {
                "ref_id": "b200",
                "title": "BARTScore: Evaluating Generated Text as Text Generation",
                "authors": [
                    {
                        "first": "Weizhe",
                        "middle": [],
                        "last": "Yuan",
                        "suffix": ""
                    },
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021",
                "volume": "",
                "issue": "",
                "pages": "27263--27277",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating Generated Text as Text Generation. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (Eds.). 27263-27277. https://proceedings.neurips.cc/ paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html",
                "links": null
            },
            "BIBREF201": {
                "ref_id": "b201",
                "title": "Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal Storyteller",
                "authors": [
                    {
                        "first": "Chuanqi",
                        "middle": [],
                        "last": "Zang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiji",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Rongsheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Zeng",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Tangjie",
                        "middle": [],
                        "last": "Lv",
                        "suffix": ""
                    },
                    {
                        "first": "Mingtao",
                        "middle": [],
                        "last": "Pei",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2403.07301"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chuanqi Zang, Jiji Tang, Rongsheng Zhang, Zeng Zhao, Tangjie Lv, Mingtao Pei, and Wei Liang. 2024. Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal Storyteller. (2024). arXiv:2403.07301 https://arxiv.org/abs/2403.07301",
                "links": null
            },
            "BIBREF202": {
                "ref_id": "b202",
                "title": "Mm-narrator: Narrating long-form videos with multimodal in-context learning",
                "authors": [
                    {
                        "first": "Chaoyi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengyuan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Linjie",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Chung-Ching",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Zicheng",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Lijuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2311.17435"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chaoyi Zhang, Kevin Lin, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023. Mm-narrator: Narrating long-form videos with multimodal in-context learning. arXiv:2311.17435 https://arxiv.org/abs/2311.17435",
                "links": null
            },
            "BIBREF203": {
                "ref_id": "b203",
                "title": "Instruction tuning for large language models: A survey",
                "authors": [
                    {
                        "first": "Shengyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Linfeng",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoya",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Sen",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaofei",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Shuhe",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Runyi",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Tianwei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2308.10792"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2024. Instruction tuning for large language models: A survey. arXiv:2308.10792 https://arxiv.org/abs/2308.10792",
                "links": null
            },
            "BIBREF204": {
                "ref_id": "b204",
                "title": "BERTScore: Evaluating Text Generation with BERT",
                "authors": [
                    {
                        "first": "Tianyi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Varsha",
                        "middle": [],
                        "last": "Kishore",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Kilian",
                        "middle": [
                            "Q"
                        ],
                        "last": "Weinberger",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "8th International Conference on Learning Representations, ICLR 2020",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/ forum?id=SkeHuCVFDr",
                "links": null
            },
            "BIBREF205": {
                "ref_id": "b205",
                "title": "Wider and deeper llm networks are fairer llm evaluators",
                "authors": [
                    {
                        "first": "Xinghua",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Haiyang",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Yangyu",
                        "middle": [],
                        "last": "Lv",
                        "suffix": ""
                    },
                    {
                        "first": "Tingwen",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Hongbo",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Yongbin",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2308.01862"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023. Wider and deeper llm networks are fairer llm evaluators. (2023). arXiv:2308.01862 https://arxiv.org/abs/2308.01862",
                "links": null
            },
            "BIBREF206": {
                "ref_id": "b206",
                "title": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance",
                "authors": [
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Maxime",
                        "middle": [],
                        "last": "Peyrard",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [
                            "M"
                        ],
                        "last": "Meyer",
                        "suffix": ""
                    },
                    {
                        "first": "Steffen",
                        "middle": [],
                        "last": "Eger",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",
                "volume": "",
                "issue": "",
                "pages": "563--578",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/D19-1053"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, 563-578. https://doi.org/10.18653/V1/D19-1053",
                "links": null
            },
            "BIBREF207": {
                "ref_id": "b207",
                "title": "Towards a Unified Multi-Dimensional Evaluator for Text Generation",
                "authors": [
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Da",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Yuning",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Yizhu",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Chenguang",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Ji",
                        "middle": [],
                        "last": "Heng",
                        "suffix": ""
                    },
                    {
                        "first": "Jiawei",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
                "volume": "2022",
                "issue": "",
                "pages": "2023--2038",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2022.EMNLP-MAIN.131"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a Unified Multi- Dimensional Evaluator for Text Generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 2023-2038. https://doi.org/10.18653/V1/2022.EMNLP-MAIN.131",
                "links": null
            },
            "BIBREF208": {
                "ref_id": "b208",
                "title": "Deconstructing NLG Evaluation: Evaluation Practices, Assumptions, and Their Implications",
                "authors": [
                    {
                        "first": "Kaitlyn",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Su",
                        "middle": [
                            "Lin"
                        ],
                        "last": "Blodgett",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Trischler",
                        "suffix": ""
                    },
                    {
                        "first": "Hal",
                        "middle": [],
                        "last": "Daum\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Iii",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Kaheer",
                        "middle": [],
                        "last": "Suleman",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandra",
                        "middle": [],
                        "last": "Olteanu",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022",
                "volume": "",
                "issue": "",
                "pages": "314--324",
                "other_ids": {
                    "DOI": [
                        "10.18653/V1/2022.NAACL-MAIN.24"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kaitlyn Zhou, Su Lin Blodgett, Adam Trischler, Hal Daum\u00e9 III, Kaheer Suleman, and Alexandra Olteanu. 2022. Deconstructing NLG Evaluation: Evaluation Practices, Assumptions, and Their Implications. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022. Association for Computational Linguistics, 314-324. https://doi.org/10.18653/V1/2022.NAACL-MAIN.24",
                "links": null
            },
            "BIBREF209": {
                "ref_id": "b209",
                "title": "RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text",
                "authors": [
                    {
                        "first": "Wangchunshu",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Yuchen",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Tiannan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenxin",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Yifan",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Cotterell",
                        "suffix": ""
                    },
                    {
                        "first": "Mrinmaya",
                        "middle": [],
                        "last": "Sachan",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2305.13304"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wangchunshu Zhou, Yuchen Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. 2023. RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text. (2023). arXiv:2305.13304 https://arxiv.org/abs/2305.13304",
                "links": null
            },
            "BIBREF210": {
                "ref_id": "b210",
                "title": "Judgelm: Fine-tuned large language models are scalable judges",
                "authors": [
                    {
                        "first": "Lianghui",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Xinggang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xinlong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2310.17631"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. arXiv:2310.17631 https://arxiv.org/abs/2310.17631",
                "links": null
            },
            "BIBREF211": {
                "ref_id": "b211",
                "title": "Texygen: A Benchmarking Platform for Text Generation Models",
                "authors": [
                    {
                        "first": "Yaoming",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Sidi",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Jiaxian",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Weinan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "1097--1100",
                "other_ids": {
                    "DOI": [
                        "10.1145/3209978.3210080"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018. Texygen: A Benchmarking Platform for Text Generation Models. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018, Kevyn Collins-Thompson, Qiaozhu Mei, Brian D. Davison, Yiqun Liu, and Emine Yilmaz (Eds.). ACM, 1097-1100. https://doi.org/10.1145/3209978.3210080",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "uris": null,
                "type_str": "figure",
                "fig_num": null,
                "text": "Evaluate if one story is good or bad, or whether it meets certain criteria (for example, checking whether it is fluent)."
            },
            "FIGREF1": {
                "num": null,
                "uris": null,
                "type_str": "figure",
                "fig_num": null,
                "text": "incorporates over 150 lexical-based indices (such as sentence overlap and paragraph overlap) and outputs a Likert-type scale as the final score. (3) The Coverage metric[52] measures Commonsense by counting the average number of knowledge triples that appear in each generated story."
            },
            "FIGREF2": {
                "num": null,
                "uris": null,
                "type_str": "figure",
                "fig_num": "4",
                "text": "Fig. 4. The Pearson Correlation between various metrics and human ratings on OpenMEVA (ROC) benchmark dataset."
            },
            "TABREF0": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td/><td>Dataset</td><td>#Stories</td><td>#Tokens per Story</td><td>Annotations</td><td>Tasks</td><td>Domain</td></tr><tr><td/><td>Children's Book [62]</td><td>687,343</td><td>464.7</td><td>Story Context, Query\u2192Infilling Entity</td><td>Story Completion</td><td>Fiction</td></tr><tr><td/><td>CNN [59]</td><td>92,579</td><td>721.9</td><td>Story Context, Query\u2192Infilling Entity</td><td>Story Completion</td><td>News</td></tr><tr><td/><td>Story Cloze Test [127]</td><td>3,744</td><td>48.1</td><td>Story Context\u2192Ending</td><td>Story Completion</td><td>Commonsense</td></tr><tr><td/><td>RocStories [127]</td><td>98,156</td><td>88.0</td><td>Title\u2192Five-Sentence Story</td><td>Story Generation</td><td>Commonsense</td></tr><tr><td/><td>NYTimes [145, 158]</td><td>1,855,658</td><td>-</td><td>Title\u2192Outline[145]\u2192Story</td><td>Story Generation</td><td>News</td></tr><tr><td/><td>WritingPrompts [38]</td><td>303,358</td><td>735.0</td><td>Prompt\u2192Outline[145]\u2192Story</td><td>Story Generation</td><td>Real World</td></tr><tr><td>Text-to-Text</td><td>Mystery [3]</td><td>532</td><td>479.4</td><td>Outline\u2192Story</td><td>Story Generation</td><td>Fiction</td></tr><tr><td/><td>Fairy Tales [3]</td><td>850</td><td>543.4</td><td>Outline\u2192Story</td><td>Story Generation</td><td>Fiction</td></tr><tr><td/><td>Hippocorpus [159]</td><td>6,854</td><td>292.6</td><td>Prompt\u2192Story</td><td>Story Generation</td><td>General</td></tr><tr><td/><td>STORIUM [2]</td><td>5,743</td><td>19,278</td><td>Prompt, Structural Descriptions\u2192Story</td><td>Story Generation</td><td>Fiction</td></tr><tr><td/><td>TVSTORYGEN [17]</td><td>29,013</td><td>1868.7</td><td>Prompt, Character Descriptions\u2192Story</td><td>Story Generation</td><td>TV Show</td></tr><tr><td/><td>LOT [51]</td><td>2,427</td><td>128.0</td><td>Title\u2192Outline\u2192Story</td><td>Story Generation</td><td>Fiction</td></tr><tr><td/><td>GPT-BOOKSUM [185]</td><td>30,047</td><td>5,363</td><td>Hierarchical outline\u2192Story</td><td>Story/Plot Generation</td><td>Fiction</td></tr><tr><td/><td>Image Paragraph [86]</td><td>19,561</td><td>67.5</td><td>Image\u2192Story</td><td>Image Paragraph Captioning</td><td>Real World</td></tr><tr><td/><td>Travel Blogs [133]</td><td>11,863</td><td>222.3</td><td>Image\u2192Story</td><td>Visual Storytelling</td><td>Real World</td></tr><tr><td/><td>VIST [69]</td><td>50,200</td><td>57.6</td><td>Image Sequence\u2192Story</td><td>Visual Storytelling</td><td>Real World</td></tr><tr><td>Visual-to-Text</td><td>AESOP [146]</td><td>7,015</td><td>26.6</td><td>Image Sequence\u2192Story</td><td>Visual Storytelling</td><td>Real World</td></tr><tr><td/><td>Video Storytelling [96]</td><td>105</td><td>162.2</td><td>Video\u2192Story</td><td>Video Storytelling</td><td>Real World</td></tr><tr><td/><td>VWP [63]</td><td>13,213</td><td>83.7</td><td>Image Sequence\u2192Story</td><td>Visual Storytelling</td><td>Movie</td></tr><tr><td/><td>Album Storytelling [130]</td><td>30</td><td>-</td><td>Image Sequence\u2192Story</td><td>Visual Storytelling</td><td>Real World</td></tr><tr><td/><td>MUGEN [57]</td><td>375,368</td><td>52.5</td><td>Story\u2192Video</td><td>Story Visualization</td><td>Game</td></tr><tr><td/><td>PororoSV [98]</td><td>15,336</td><td>69.2</td><td>Story\u2192Image Sequence</td><td>(Continuous) Story Visualization</td><td>Cartoon</td></tr><tr><td/><td>FlintstonesSV [55]</td><td>24,512</td><td>83.1</td><td>Story\u2192Image Sequence</td><td>(Continuous) Story Visualization</td><td>Cartoon</td></tr><tr><td/><td>DiDeMoSV [117]</td><td>17,635</td><td>22.3</td><td>Story\u2192Image Sequence</td><td>(Continuous) Story Visualization</td><td>Real World</td></tr><tr><td>Text-to-Visual</td><td>StorySalon [104]</td><td>10,366</td><td>298.1</td><td>Story\u2192Image Sequence</td><td>(Continuous) Story Visualization</td><td>Animation</td></tr><tr><td/><td>MovieNet-TeViS [50]</td><td>10,000</td><td>21.7</td><td>Story\u2192Image Sequence</td><td>Story Illustration</td><td>Movie</td></tr><tr><td/><td>CMD [7]</td><td>3,606</td><td>136.6</td><td>Story\u2192Video Clip Sequence</td><td>Story Illustration</td><td>Movie</td></tr><tr><td/><td>CVSV [111]</td><td>84,569</td><td>534.7</td><td>Story\u2192Video Clip Sequence</td><td>Story Illustration</td><td>Movie</td></tr><tr><td/><td>StoryBench [12]</td><td>8,900</td><td>37.6</td><td>Story\u2192Video Segments</td><td>(Continuous) Story Visualization</td><td>Real World</td></tr></table>",
                "text": "Detailed statistics of existing story generation datasets."
            },
            "TABREF1": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td>Score</td><td>Boolean /Likert Scale</td><td>Score</td><td colspan=\"2\">Score</td><td colspan=\"2\">Boolean /Likert Scale</td><td/><td>Score</td><td colspan=\"3\">Reasoning Process or Error Analysis</td><td>Evaluation Output (All Formats)</td></tr><tr><td>Regressor</td><td>Classifier</td><td>Matching</td><td colspan=\"2\">Regressor</td><td colspan=\"2\">Classifier</td><td colspan=\"2\">P(Target Story | Inputs)</td><td/><td/><td/></tr><tr><td/><td/><td/><td/><td/><td/><td/><td/><td>Target Story</td><td/><td/><td/></tr><tr><td>Vector_src</td><td>Vector_out</td><td>Vector_ref</td><td/><td colspan=\"2\">Vector</td><td/><td/><td/><td/><td/><td/></tr><tr><td/><td/><td/><td/><td/><td/><td/><td colspan=\"2\">Generative Model</td><td/><td colspan=\"3\">Generative Model</td></tr><tr><td>Encoder</td><td>Encoder</td><td>Encoder</td><td/><td colspan=\"2\">Encoder</td><td/><td/><td/><td/><td/><td/></tr><tr><td>Source</td><td>Target Story</td><td>Reference</td><td>Source</td><td colspan=\"2\">Target Story</td><td>Reference</td><td>Instructions</td><td>Source Reference</td><td>Instructions</td><td>Source</td><td colspan=\"2\">Target Story</td><td>Reference</td></tr><tr><td/><td/><td colspan=\"2\">(a) Embedding-Based</td><td/><td/><td/><td colspan=\"2\">(b) Probability-Based</td><td/><td colspan=\"3\">(c) Generative-Based</td></tr><tr><td>Fig. 3.</td><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/><td/></tr></table>",
                "text": "Table 5 provides a comprehensive overview of metrics evaluating various aspects of textual stories. Illustration of different types of neural models applied for automatic evaluation metrics (all the dashed boxes are optional input or output): (a) Embedding-Based Methods, which evaluate based on separately encoded vectors (left) or a jointly encoded vector (right); (b) Probability-Based Methods, which calculate based on the generation probability of the target story; (c) Generative-Based Methods, which directly generate the evaluation results, with or without the reasoning process. These three types of models can be fine-tuned on evaluation benchmarks, referred to as Trained Metrics."
            },
            "TABREF2": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td>Metric</td><td>Aspects</td><td>Method</td><td>Format</td></tr><tr><td/><td>REL DIV FLU COH COM CLA COMM INF CHA INT EMP SUR</td><td/><td/></tr><tr><td>Distinct-n [94]</td><td>\u2713</td><td>Lexical-based</td><td>Score</td></tr><tr><td>SELF_BLEU [212]</td><td>\u2713</td><td>Lexical-based</td><td>Score</td></tr><tr><td>Backward BLEU [164]</td><td>\u2713</td><td>Lexical-based</td><td>Score</td></tr><tr><td>MS-Jaccard [126]</td><td>\u2713</td><td>Lexical-based</td><td>Score</td></tr><tr><td>Inter-Story Repetition [197]</td><td>\u2713</td><td>Lexical-based</td><td>Score</td></tr><tr><td>Intra-Story Repetition [197]</td><td>\u2713</td><td>Lexical-based</td><td>Score</td></tr><tr><td>TAACO [32]</td><td>\u2713</td><td>Lexical-based</td><td>Likert</td></tr><tr><td>TAACO 2.0 [31]</td><td>\u2713</td><td>Embedding-based</td><td>Likert</td></tr><tr><td>UNION [53]</td><td/><td/><td/></tr></table>",
                "text": "Evaluation Metrics proposed (\u2713) and adopted (*) for evaluating various aspects of generated texts. For each aspect, REL: relevance, DIV: diversity, FLU: fluency, COH: coherence, COM: completeness, CLA: clarity, COMM: commonsense, INF: informativeness, CHA: character development, INT: interestingness, EMP: empathy, SUR: surprise."
            },
            "TABREF4": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table/>",
                "text": "achieved under specific prompt formats. Designing more professional prompts or fine-tuning the model with more diverse instructions can mitigate this problem, but it still remains to be solved. (2) Position bias, a common problem in LLM-based evaluations, which means that LLM exhibits a preference for the first displayed candidate[182]. To address this problem, a simple solution is to combine evaluations using different sample orderings. Specifically for generative-based methods, Wang et al.[182] aggregates results across various orders to determine the final score; for trained methods, Li et al.[95] change the order of the candidates within each training sample to double the training data. (3) Knowledge bias, which means LLM-based evaluators tend to favor the results they have seen or results that are generated by themselves[106,107]. This can be partially mitigated by replacing proper nouns, but it remains a very challenging issue. (4) Likelihood Bias, which is showcased in probability-based methods, because the probability of a"
            }
        }
    }
}